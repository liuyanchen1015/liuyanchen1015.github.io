---
---

@article{Liu2023tabularbiases,
  abbr={Preprint},
  bibtex_show={true},
  title={Investigating the Fairness of Large Language Models for Predictions on Tabular Data},
  abstract={Recent literature has suggested the potential of using large language models (LLMs) to make predictions for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in the society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is imperative to explore the following questions: what sources of information do LLMs draw upon when making predictions for tabular tasks; whether and to what extent are LLM predictions for tabular tasks influenced by social biases and stereotypes; and what are the consequential implications for fairness? Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular prediction tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and fine-tuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pre-training corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.},
  author={Liu, Yanchen and Gautam, Srishti and Ma, Jiaqi and Lakkaraju, Himabindu},
  year={2023},
  journal={Under Review<br>},
  preview={tabular_biases.png},
}

@article{Ma2023MIDDAG,
  abbr={Preprint},
  bibtex_show={true},
  title={MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways},
  abstract={We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level.},
  author={Ma, Mingyu Derek and Taylor, Alexander K. and Wen, Nuan and Liu, Yanchen and Kung,  Po-Nien and Qin, Wenna and Wen, Shicheng and Zhou, Azure and Yang, Diyi and Ma, Xuezhe and Peng, Nanyun and Wang, Wei},
  year={2023},
  html={https://arxiv.org/abs/2310.02529},
  journal={Under Review by AAAI 2024<br>},
  preview={middag.png},
}

@inproceedings{Liu2023DADA,
  abbr={EMNLP},
  bibtex_show={true},
  title={DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules},
  abstract={Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.},
  author={Liu, Yanchen and Held, William and Yang, Diyi},
  year={2023},
  html={https://arxiv.org/abs/2305.13406},
  publisher={EMNLP 2023},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2023</b>)<br>},
  preview={DADA.png},
  dimensions={true},
  selected={true}
}

@inproceedings{Xiao2023HyperLoRA,
  abbr={EMNLP},
  bibtex_show={true},
  title={Task-Agnostic Low-Rank Adapters for Unseen English Dialects},
  abstract={Large Language Models (LLMs) are trained on corpora disproportionally weighted in favor of Standard American English. As a result, speakers of other dialects experience significantly more failures when interacting with these technologies. In practice, these speakers often accommodate their speech to be better understood. Our work shares the belief that language technologies should be designed to accommodate the diversity in English dialects and not the other way around. However, prior work on dialect struggle with generalizing to evolving and emerging dialects in a scalable manner. To fill this gap, our method, HyperLoRA, leverages expert linguistic knowledge to enable resource-efficient adaptation via hypernetworks. By disentangling dialect-specific and cross-dialectal information, HyperLoRA improves generalization to unseen dialects in a task-agnostic fashion. Not only is HyperLoRA more scalable in the number of parameters, but it also achieves the best or most competitive performance across 5 dialects in a zero-shot setting. In this way, our approach facilitates access to language technology for billions of English dialect speakers who are traditionally underrepresented.},
  author={Xiao, Zedian and Held, William and Liu, Yanchen and Yang, Diyi},
  publisher={EMNLP 2023},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (<b>EMNLP 2023</b>)<br>},
  year={2023},
  preview={hyperlora.png},
}

@inproceedings{Liu2022sMixtureAdapters,
  abbr={ACL TrustNLP},
  bibtex_show={true},
  title={SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases},
  author={Liu, Yanchen and Yan, Jing and Chen, Yan and Liu, Jing and Wu, Hua},
  abstract={Recent studies reveal that various biases exist in different NLP tasks, and over-reliance on biases results in models' poor generalization ability and low adversarial robustness. To mitigate datasets biases, previous works propose lots of debiasing techniques to tackle specific biases, which perform well on respective adversarial sets but fail to mitigate other biases. In this paper, we propose a new debiasing method Sparse Mixture-of-Adapters (SMoA), which can mitigate multiple dataset biases effectively and efficiently. Experiments on Natural Language Inference and Paraphrase Identification tasks demonstrate that SMoA outperforms full-finetuning, adapter tuning baselines, and prior strong debiasing methods. Further analysis indicates the interpretability of SMoA that sub-adapter can capture specific pattern from the training data and specialize to handle specific bias.},
  year={2023},
  html={https://arxiv.org/abs/2302.14413},
  publisher={ACL 2023},
  booktitle={ACL 2023 Workshop on Trustworthy Natural Language Processing (<b>TrustNLP 2023 </b>)<br>},
  preview={smoa.png},
  selected={false}
}

@inproceedings{Liu2022SemanticPriming,
  abbr={ACL SustaiNLP},
  bibtex_show={true},
  title={Semantic-Oriented Unlabeled Priming for Large-Scale Language Models},
  abstract={Due to the high costs associated with finetuning large language models, various recent works propose to adapt them to specific tasks without any parameter updates through in-context learning. Unfortunately, for in-context learning there is currently no way to leverage unlabeled data, which is often much easier to obtain in large quantities than labeled examples. In this work, we therefore investigate ways to make use of unlabeled examples to improve the zero-shot performance of pretrained language models without any finetuning: We introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies examples by retrieving semantically similar unlabeled examples, assigning labels to them in a zero-shot fashion, and then using them for in-context learning. We also propose bag-of-contexts priming, a new priming strategy that is more suitable for our setting and enables the usage of more examples than fit into the context window.},
  author={Liu, Yanchen and Schick, Timo and SchÃ¼tze, Hinrich},
  year={2022},
  html={https://arxiv.org/abs/2202.06133},
  publisher={ACL 2023},
  booktitle={ACL 2023 Workshop on Simple and Efficient Natural Language Processing (<b>SustaiNLP 2023 ðŸ¥‡Oral</b>)<br>},
  preview={SOUP.jpg},
  dimensions={true},
  selected={true}
}

@inproceedings{Wu2022CustomSineWaves,
  abbr={ICMA},
  bibtex_show={true},
  title={Custom Sine Waves Are Enough for Imitation Learning of Bipedal Gaits with Different Styles},
  abstract={Not until recently, robust bipedal locomotion has been achieved through reinforcement learning. However, existing implementations rely heavily on insights and efforts from human experts, which is costly for the iterative design of robot systems. Also, styles of the learned motion are strictly limited to that of the reference. In this paper, we propose a new way to learn bipedal locomotion from a simple sine wave as the reference for foot heights. With the naive human insight that the two feet should be lifted up alternatively and periodically, we experimentally demonstrate on the Cassie robot that, a simple reward function is able to make the robot learn to walk end-to-end and efficiently without any explicit knowledge of the model. With custom sine waves, the learned gait pattern can also have customized styles.},
  author={Wu*, Qi and Zhang*, Chong and Liu, Yanchen},
  publisher={IEEE International Conference on Mechatronics and Automation (ICMA) 2022. Finalists of Toshio Fukuda Best Paper Award in Mechatroincs.},
  booktitle={IEEE International Conference on Mechatronics and Automation (<b>ICMA 2022 ðŸ¥‡Finalists of Toshio Fukuda Best Paper Award in Mechatroincs</b>)<br>},
  year={2022},
  html={https://arxiv.org/abs/2204.04157},
  preview={robo.png},
  selected={false}
}









