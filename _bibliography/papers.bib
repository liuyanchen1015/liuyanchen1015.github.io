---
---

@manuscript{Liu2023DADA,
  abbr={Preprint},
  bibtex_show={true},
  title={DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules},
  abstract={Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.},
  author={Liu, Yanchen and Held, William and Yang, Diyi},
  year={2023},
  html={https://arxiv.org/abs/2305.13406},
  preview={DADA.png},
  dimensions={true},
  selected={true}
}


@inproceedings{Liu2022sMixtureAdapters,
  abbr={ACL TrustNLP},
  bibtex_show={true},
  title={SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases},
  author={Liu, Yanchen and Yan, Jing and Chen, Yan and Liu, Jing and Wu, Hua},
  abstract={Recent studies reveal that various biases exist in different NLP tasks, and over-reliance on biases results in models' poor generalization ability and low adversarial robustness. To mitigate datasets biases, previous works propose lots of debiasing techniques to tackle specific biases, which perform well on respective adversarial sets but fail to mitigate other biases. In this paper, we propose a new debiasing method Sparse Mixture-of-Adapters (SMoA), which can mitigate multiple dataset biases effectively and efficiently. Experiments on Natural Language Inference and Paraphrase Identification tasks demonstrate that SMoA outperforms full-finetuning, adapter tuning baselines, and prior strong debiasing methods. Further analysis indicates the interpretability of SMoA that sub-adapter can capture specific pattern from the training data and specialize to handle specific bias.},
  year={2023},
  html={https://arxiv.org/abs/2302.14413},
  publisher={ACL 2023},
  booktitle={ACL 2023 Workshop on Trustworthy Natural Language Processing (TrustNLP)<br>},
  preview={smoa.png},
  selected={false}
}

@inproceedings{Liu2022SemanticPriming,
  abbr={ACL SustaiNLP},
  bibtex_show={true},
  title={Semantic-Oriented Unlabeled Priming for Large-Scale Language Models},
  abstract={Due to the high costs associated with finetuning large language models, various recent works propose to adapt them to specific tasks without any parameter updates through in-context learning. Unfortunately, for in-context learning there is currently no way to leverage unlabeled data, which is often much easier to obtain in large quantities than labeled examples. In this work, we therefore investigate ways to make use of unlabeled examples to improve the zero-shot performance of pretrained language models without any finetuning: We introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies examples by retrieving semantically similar unlabeled examples, assigning labels to them in a zero-shot fashion, and then using them for in-context learning. We also propose bag-of-contexts priming, a new priming strategy that is more suitable for our setting and enables the usage of more examples than fit into the context window.},
  author={Liu, Yanchen and Schick, Timo and Sch√ºtze, Hinrich},
  year={2022},
  html={https://arxiv.org/abs/2202.06133},
  publisher={ACL 2023},
  booktitle={ACL 2023 Workshop on Simple and Efficient Natural Language Processing (SustaiNLP <span style="color: rgb(153, 0, 0);"><b>Oral</b></span>)<br>},
  preview={SOUP.jpg},
  dimensions={true},
  selected={true}
}

@inproceedings{Wu2022CustomSineWaves,
  abbr={ICMA},
  bibtex_show={true},
  title={Custom Sine Waves Are Enough for Imitation Learning of Bipedal Gaits with Different Styles},
  abstract={Not until recently, robust bipedal locomotion has been achieved through reinforcement learning. However, existing implementations rely heavily on insights and efforts from human experts, which is costly for the iterative design of robot systems. Also, styles of the learned motion are strictly limited to that of the reference. In this paper, we propose a new way to learn bipedal locomotion from a simple sine wave as the reference for foot heights. With the naive human insight that the two feet should be lifted up alternatively and periodically, we experimentally demonstrate on the Cassie robot that, a simple reward function is able to make the robot learn to walk end-to-end and efficiently without any explicit knowledge of the model. With custom sine waves, the learned gait pattern can also have customized styles.},
  author={Wu*, Qi and Zhang*, Chong and Liu, Yanchen},
  publisher={IEEE International Conference on Mechatronics and Automation (ICMA) 2022. Finalists of Toshio Fukuda Best Paper Award in Mechatroincs.},
  booktitle={IEEE International Conference on Mechatronics and Automation (ICMA <b>Finalists of Toshio Fukuda Best Paper Award in Mechatroincs</b>)<br>},
  year={2022},
  html={https://arxiv.org/abs/2204.04157},
  preview={robo.png},
  selected={false}
}









