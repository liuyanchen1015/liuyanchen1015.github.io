# Misinformation Susceptibility
@article{imhoff2022conspiracy,
  title={Conspiracy mentality and political orientation across 26 countries},
  author={Imhoff, Roland and Zimmer, Felix and Klein, Olivier and Ant{\'o}nio, Jo{\~a}o HC and Babinska, Maria and Bangerter, Adrian and Bilewicz, Michal and Blanu{\v{s}}a, Neboj{\v{s}}a and Bovan, Kosta and Bu{\v{z}}arovska, Rumena and others},
  journal={Nature human behaviour},
  volume={6},
  number={3},
  pages={392--403},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{roozenbeek2020susceptibility,
  title={Susceptibility to misinformation about COVID-19 around the world},
  author={Roozenbeek, Jon and Schneider, Claudia R and Dryhurst, Sarah and Kerr, John and Freeman, Alexandra LJ and Recchia, Gabriel and Van Der Bles, Anne Marthe and Van Der Linden, Sander},
  journal={Royal Society open science},
  volume={7},
  number={10},
  pages={201199},
  year={2020},
  publisher={The Royal Society}
}

@article{van2022misinformation,
  title={Misinformation: susceptibility, spread, and interventions to immunize the public},
  author={Van Der Linden, Sander},
  journal={Nature Medicine},
  volume={28},
  number={3},
  pages={460--467},
  year={2022},
  publisher={Nature Publishing Group US New York}
}

@article{pennycook2021psychology,
  title={The psychology of fake news},
  author={Pennycook, Gordon and Rand, David G},
  journal={Trends in cognitive sciences},
  volume={25},
  number={5},
  pages={388--402},
  year={2021},
  publisher={Elsevier}
}

@article{bringula2022gullible,
  title={“Who is gullible to political disinformation?”: predicting susceptibility of university students to fake news},
  author={Bringula, Rex P and Catacutan-Bangit, Annaliza E and Garcia, Manuel B and Gonzales, John Paul S and Valderama, Arlene Mae C},
  journal={Journal of Information Technology \& Politics},
  volume={19},
  number={2},
  pages={165--179},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{scherer2021susceptible,
  title={Who is susceptible to online health misinformation? A test of four psychosocial hypotheses.},
  author={Scherer, Laura D and McPhetres, Jon and Pennycook, Gordon and Kempe, Allison and Allen, Larry A and Knoepke, Christopher E and Tate, Channing E and Matlock, Daniel D},
  journal={Health Psychology},
  volume={40},
  number={4},
  pages={274},
  year={2021},
  publisher={American Psychological Association}
}

@article{nan2022people,
  title={Why people believe health misinformation and who are at risk? A systematic review of individual differences in susceptibility to health misinformation},
  author={Nan, Xiaoli and Wang, Yuan and Thier, Kathryn},
  journal={Social Science \& Medicine},
  pages={115398},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{zhang2015ideology,
author = {Zhang, Amy X. and Counts, Scott},
title = {Modeling Ideology and Predicting Policy Change with Social Media: Case of Same-Sex Marriage},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702193},
doi = {10.1145/2702123.2702193},
abstract = {Social media has emerged as a prominent platform where people can express their feelings about social and political issues of our time. We study the many voices discussing an issue within a constituency and how they reflect ideology and may signal the outcome of important policy decisions. Focusing on the issue of same-sex marriage legalization, we examine almost 2 million public Twitter posts related to same-sex marriage in the U.S. states over the course of 4 years starting from 2011. Among other findings, we find evidence of moral culture wars between ideologies and show that constituencies that express higher levels of emotion and have fewer actively engaged participants often precede legalization efforts that fail. From our measures, we build statistical models to predict the outcome of potential policy changes, with our best model achieving 87\% accuracy. We also achieve accuracies of 70\%, comparable to public opinion surveys, many months before a policy decision. We discuss how these analyses can augment traditional political science techniques as well as assist activists and policy analysts in understanding discussions on important issues at a population scale.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {2603–2612},
numpages = {10},
keywords = {public policy, social media, political science, same-sex marriage},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@article{mccright2013influence,
  title={The influence of political ideology on trust in science},
  author={McCright, Aaron M and Dentzman, Katherine and Charters, Meghan and Dietz, Thomas},
  journal={Environmental Research Letters},
  volume={8},
  number={4},
  pages={044029},
  year={2013},
  publisher={IOP Publishing}
}

@article{baptista2021influence,
  title={The influence of political ideology on fake news belief: The Portuguese case},
  author={Baptista, Jo{\~a}o Pedro and Correia, Elisete and Gradim, Anabela and Pi{\~n}eiro-Naval, Valeriano},
  journal={Publications},
  volume={9},
  number={2},
  pages={23},
  year={2021},
  publisher={MDPI}
}

@misc{cui2020coaid,
    title={CoAID: COVID-19 Healthcare Misinformation Dataset},
    author={Limeng Cui and Dongwon Lee},
    year={2020},
    eprint={2006.00885},
    archivePrefix={arXiv},
    primaryClass={cs.SI}
}

@article{hayawi2022anti,
  title={ANTi-Vax: a novel Twitter dataset for COVID-19 vaccine misinformation detection},
  author={Hayawi, Kadhim and Shahriar, Sakib and Serhani, Mohamed Adel and Taleb, Ikbal and Mathew, Sujith Samuel},
  journal={Public health},
  volume={203},
  pages={23--30},
  year={2022},
  publisher={Elsevier}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{chen2009ranking,
  title={Ranking measures and loss functions in learning to rank},
  author={Chen, Wei and Liu, Tie-Yan and Lan, Yanyan and Ma, Zhi-Ming and Li, Hang},
  journal={Advances in Neural Information Processing Systems},
  volume={22},
  year={2009}
}

@inproceedings{hoffer2015deep,
  title={Deep metric learning using triplet network},
  author={Hoffer, Elad and Ailon, Nir},
  booktitle={Similarity-Based Pattern Recognition: Third International Workshop, SIMBAD 2015, Copenhagen, Denmark, October 12-14, 2015. Proceedings 3},
  pages={84--92},
  year={2015},
  organization={Springer}
}

@book{gelman2009red,
  title={Red state, blue state, rich state, poor state: Why Americans vote the way they do-expanded edition},
  author={Gelman, Andrew},
  year={2009},
  publisher={Princeton University Press}
}

@article{singh2022misinformation,
  title={Misinformation, believability, and vaccine acceptance over 40 countries: Takeaways from the initial phase of the COVID-19 infodemic},
  author={Singh, Karandeep and Lima, Gabriel and Cha, Meeyoung and Cha, Chiyoung and Kulshrestha, Juhi and Ahn, Yong-Yeol and Varol, Onur},
  journal={Plos one},
  volume={17},
  number={2},
  pages={e0263381},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}

@book{2000MixedEffectsModelsSPLUS,
  title = {Mixed-{{Effects Models}} in {{S}} and {{S-PLUS}}},
  year = {2000},
  series = {Statistics and {{Computing}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/b98882},
  url = {http://link.springer.com/10.1007/b98882},
  urldate = {2023-10-24},
  isbn = {978-0-387-98957-0},
  langid = {english},
  file = {/Users/ma/Zotero/storage/P9S4HTSD/2000 - Mixed-Effects Models in S and S-PLUS.pdf}
}

@article{2023ProgramThoughtsPrompting,
  title = {Program of {{Thoughts Prompting}}: {{Disentangling Computation}} from {{Reasoning}} for {{Numerical Reasoning Tasks}}},
  shorttitle = {Program of {{Thoughts Prompting}}},
  year = {2023},
  month = jun,
  journal = {Transactions on Machine Learning Research},
  url = {https://openreview.net/forum?id=YfZ4ZPt8zd},
  urldate = {2023-10-26},
  abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question. Here we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer. In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. We evaluate PoT on five math word problem datasets and three financial-QA datasets in both few-shot and zero-shot settings. We find that PoT has an average performance gain over CoT of around 12\% across all datasets. By combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets. All of our data and code will be released.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/2023ProgramThoughtsPrompting_Program of Thoughts Prompting.pdf}
}

@inproceedings{Agrawal2022LargeLanguageModels,
  title = {Large Language Models Are Few-Shot Clinical Information Extractors},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Agrawal, Monica and Hegselmann, Stefan and Lang, Hunter and Kim, Yoon and Sontag, David},
  year = {2022},
  month = dec,
  pages = {1998--2022},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.130},
  urldate = {2023-02-09},
  abstract = {A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Agrawal2022LargeLanguageModels_Large language models are few-shot clinical information extractors.pdf}
}

@misc{Ajay2023ConditionalGenerativeModeling,
  title = {Is {{Conditional Generative Modeling}} All You Need for {{Decision-Making}}?},
  author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua and Jaakkola, Tommi and Agrawal, Pulkit},
  year = {2023},
  month = jul,
  number = {arXiv:2211.15657},
  eprint = {2211.15657},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15657},
  url = {http://arxiv.org/abs/2211.15657},
  urldate = {2023-10-06},
  abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ajay2023ConditionalGenerativeModeling_Is Conditional Generative Modeling all you need for Decision-Making.pdf;/Users/ma/Zotero/storage/NC4EUHFE/2211.html}
}

@article{Altay2022WhyFewPeople,
  title = {Why Do so Few People Share Fake News? {{It}} Hurts Their Reputation},
  shorttitle = {Why Do so Few People Share Fake News?},
  author = {Altay, Sacha and Hacquin, Anne-Sophie and Mercier, Hugo},
  year = {2022},
  month = jun,
  journal = {New Media \& Society},
  volume = {24},
  number = {6},
  pages = {1303--1324},
  publisher = {{SAGE Publications}},
  issn = {1461-4448},
  doi = {10.1177/1461444820969893},
  url = {https://doi.org/10.1177/1461444820969893},
  urldate = {2023-10-14},
  abstract = {In spite of the attractiveness of fake news stories, most people are reluctant to share them. Why? Four pre-registered experiments (N\,=\,3,656) suggest that sharing fake news hurt one's reputation in a way that is difficult to fix, even for politically congruent fake news. The decrease in trust a source (media outlet or individual) suffers when sharing one fake news story against a background of real news is larger than the increase in trust a source enjoys when sharing one real news story against a background of fake news. A comparison with real-world media outlets showed that only sources sharing no fake news at all had similar trust ratings to mainstream media. Finally, we found that the majority of people declare they would have to be paid to share fake news, even when the news is politically congruent, and more so when their reputation is at stake.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Altay2022WhyFewPeople_Why do so few people share fake news.pdf}
}

@article{Altay2023MisinformationMisinformationConceptual,
  title = {Misinformation on {{Misinformation}}: {{Conceptual}} and {{Methodological Challenges}}},
  shorttitle = {Misinformation on {{Misinformation}}},
  author = {Altay, Sacha and Berriche, Manon and Acerbi, Alberto},
  year = {2023},
  month = jan,
  journal = {Social Media + Society},
  volume = {9},
  number = {1},
  pages = {20563051221150412},
  publisher = {{SAGE Publications Ltd}},
  issn = {2056-3051},
  doi = {10.1177/20563051221150412},
  url = {https://doi.org/10.1177/20563051221150412},
  urldate = {2023-10-13},
  abstract = {Alarmist narratives about online misinformation continue to gain traction despite evidence that its prevalence and impact are overstated. Drawing on research examining the use of big data in social science and reception studies, we identify six misconceptions about misinformation and highlight the conceptual and methodological challenges they raise. The first set of misconceptions concerns the prevalence and circulation of misinformation. First, scientists focus on social media because it is methodologically convenient, but misinformation is not just a social media problem. Second, the internet is not rife with misinformation or news, but with memes and entertaining content. Third, falsehoods do not spread faster than the truth; how we define (mis)information influences our results and their practical implications. The second set of misconceptions concerns the impact and the reception of misinformation. Fourth, people do not believe everything they see on the internet: the sheer volume of engagement should not be conflated with belief. Fifth, people are more likely to be uninformed than misinformed; surveys overestimate misperceptions and say little about the causal influence of misinformation. Sixth, the influence of misinformation on people's behavior is overblown as misinformation often ``preaches to the choir.'' To appropriately understand and fight misinformation, future research needs to address these challenges.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Altay2023MisinformationMisinformationConceptual_Misinformation on Misinformation.pdf}
}

@article{An2023KAMPNetMultisourceMedical,
  title = {{{KAMPNet}}: Multi-Source Medical Knowledge Augmented Medication Prediction Network with Multi-Level Graph Contrastive Learning},
  shorttitle = {{{KAMPNet}}},
  author = {An, Yang and Tang, Haocheng and Jin, Bo and Xu, Yi and Wei, Xiaopeng},
  year = {2023},
  month = oct,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {23},
  number = {1},
  pages = {243},
  issn = {1472-6947},
  doi = {10.1186/s12911-023-02325-x},
  url = {https://doi.org/10.1186/s12911-023-02325-x},
  urldate = {2023-10-31},
  abstract = {Predicting medications is a crucial task in intelligent healthcare systems, aiding doctors in making informed decisions based on electronic medical records (EMR). However, medication prediction faces challenges due to complex relations within heterogeneous medical data. Existing studies primarily focus on the supervised mining of hierarchical relations between homogeneous codes in medical ontology graphs, such as diagnosis codes. Few studies consider the valuable relations, including synergistic relations between medications, concurrent relations between diseases, and therapeutic relations between medications and diseases from historical EMR. This limitation restricts prediction performance and application scenarios.},
  keywords = {Electronic medical records,Graph contrastive learning,Intelligent healthcare system,Medication prediction,Multi-source medical knowledge},
  file = {/Users/ma/Drive_ma/Papers_Zotero/An2023KAMPNetMultisourceMedical_KAMPNet.pdf;/Users/ma/Zotero/storage/HEIPMTFN/An et al. - 2023 - KAMPNet multi-source medical knowledge augmented .pdf}
}

@inproceedings{Antypas2022TwitterTopicClassification,
  title = {Twitter {{Topic Classification}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Antypas, Dimosthenis and Ushio, Asahi and {Camacho-Collados}, Jose and Silva, Vitor and Neves, Leonardo and Barbieri, Francesco},
  year = {2022},
  month = oct,
  pages = {3386--3400},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  url = {https://aclanthology.org/2022.coling-1.299},
  urldate = {2023-08-18},
  abstract = {Social media platforms host discussions about a wide variety of topics that arise everyday. Making sense of all the content and organising it into categories is an arduous task. A common way to deal with this issue is relying on topic modeling, but topics discovered using this technique are difficult to interpret and can differ from corpus to corpus. In this paper, we present a new task based on tweet topic classification and release two associated datasets. Given a wide range of topics covering the most important discussion points in social media, we provide training and testing data from recent time periods that can be used to evaluate tweet classification models. Moreover, we perform a quantitative evaluation and analysis of current general- and domain-specific language models on the task, which provide more insights on the challenges and nature of the task.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Antypas2022TwitterTopicClassification_Twitter Topic Classification.pdf}
}

@misc{Arora2022AskMeAnything,
  title = {Ask {{Me Anything}}: {{A}} Simple Strategy for Prompting Language Models},
  shorttitle = {Ask {{Me Anything}}},
  author = {Arora, Simran and Narayan, Avanika and Chen, Mayee F. and Orr, Laurel and Guha, Neel and Bhatia, Kush and Chami, Ines and Sala, Frederic and R{\'e}, Christopher},
  year = {2022},
  month = nov,
  number = {arXiv:2210.02441},
  eprint = {2210.02441},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.02441},
  urldate = {2023-03-06},
  abstract = {Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly "perfect prompt" for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation ("Who went to the park?") tend to outperform those that restrict the model outputs ("John went to the park. Output True or False."). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama\_prompting},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Downloads/5912_ask_me_anything_a_simple_strat.pdf;/Users/ma/Drive_ma/Papers_Zotero/Arora2022AskMeAnything_Ask Me Anything.pdf;/Users/ma/Zotero/storage/ICLJHJKR/2210.html}
}

@misc{Askell2021GeneralLanguageAssistant,
  title = {A {{General Language Assistant}} as a {{Laboratory}} for {{Alignment}}},
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2021},
  month = dec,
  number = {arXiv:2112.00861},
  eprint = {2112.00861},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.00861},
  url = {http://arxiv.org/abs/2112.00861},
  urldate = {2023-03-01},
  abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Askell2021GeneralLanguageAssistant_A General Language Assistant as a Laboratory for Alignment.pdf;/Users/ma/Zotero/storage/ACQ8R3BR/2112.html}
}

@misc{Atske2019ManyAmericansSay,
  title = {Many {{Americans Say Made-Up News Is}} a {{Critical Problem That Needs To Be Fixed}}},
  author = {Atske, Sara},
  year = {2019},
  month = jun,
  journal = {Pew Research Center's Journalism Project},
  url = {https://www.pewresearch.org/journalism/2019/06/05/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-fixed/},
  urldate = {2023-10-14},
  abstract = {Politicians viewed as major creators of it, but journalists seen as the ones who should fix it},
  langid = {american},
  file = {/Users/ma/Zotero/storage/694JDKWQ/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-fixed.html}
}

@misc{Bai2022TrainingHelpfulHarmless,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05862},
  eprint = {2204.05862},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.05862},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2023-03-01},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bai2022TrainingHelpfulHarmless_Training a Helpful and Harmless Assistant with Reinforcement Learning from.pdf;/Users/ma/Zotero/storage/48E72937/2204.html}
}

@misc{Bang2023MultitaskMultilingualMultimodal,
  title = {A {{Multitask}}, {{Multilingual}}, {{Multimodal Evaluation}} of {{ChatGPT}} on {{Reasoning}}, {{Hallucination}}, and {{Interactivity}}},
  author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04023},
  eprint = {2302.04023},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04023},
  url = {http://arxiv.org/abs/2302.04023},
  urldate = {2023-03-10},
  abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bang2023MultitaskMultilingualMultimodal_A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning,.pdf;/Users/ma/Zotero/storage/U64G4H4K/2302.html}
}

@misc{Bang2023MultitaskMultilingualMultimodala,
  title = {A {{Multitask}}, {{Multilingual}}, {{Multimodal Evaluation}} of {{ChatGPT}} on {{Reasoning}}, {{Hallucination}}, and {{Interactivity}}},
  author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04023},
  eprint = {2302.04023},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04023},
  url = {http://arxiv.org/abs/2302.04023},
  urldate = {2023-04-20},
  abstract = {This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41\% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8\% ROUGE-1 on summarization and 2\% ChrF++ on machine translation, in a multi-turn "prompt engineering" fashion. We also release codebase for evaluation set extraction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bang2023MultitaskMultilingualMultimodala_A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning,.pdf;/Users/ma/Zotero/storage/B6EFQLQ7/2302.html}
}

@misc{Bansal2023CleanCLIPMitigatingData,
  title = {{{CleanCLIP}}: {{Mitigating Data Poisoning Attacks}} in {{Multimodal Contrastive Learning}}},
  shorttitle = {{{CleanCLIP}}},
  author = {Bansal, Hritik and Singhi, Nishad and Yang, Yu and Yin, Fan and Grover, Aditya and Chang, Kai-Wei},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03323},
  eprint = {2303.03323},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.03323},
  urldate = {2023-04-27},
  abstract = {Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for individual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor attacks on multimodal contrastive learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bansal2023CleanCLIPMitigatingData_CleanCLIP.pdf;/Users/ma/Zotero/storage/YL73EXP4/2303.html}
}

@misc{Bansal2023CleanCLIPMitigatingDataa,
  title = {{{CleanCLIP}}: {{Mitigating Data Poisoning Attacks}} in {{Multimodal Contrastive Learning}}},
  shorttitle = {{{CleanCLIP}}},
  author = {Bansal, Hritik and Singhi, Nishad and Yang, Yu and Yin, Fan and Grover, Aditya and Chang, Kai-Wei},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03323},
  eprint = {2303.03323},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.03323},
  urldate = {2023-05-12},
  abstract = {Multimodal contrastive pretraining has been used to train multimodal representation models, such as CLIP, on large amounts of paired image-text data. However, previous studies have revealed that such models are vulnerable to backdoor attacks. Specifically, when trained on backdoored examples, CLIP learns spurious correlations between the embedded backdoor trigger and the target label, aligning their representations in the joint embedding space. Injecting even a small number of poisoned examples, such as 75 examples in 3 million pretraining data, can significantly manipulate the model's behavior, making it difficult to detect or unlearn such correlations. To address this issue, we propose CleanCLIP, a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks by independently re-aligning the representations for individual modalities. We demonstrate that unsupervised finetuning using a combination of multimodal contrastive and unimodal self-supervised objectives for individual modalities can significantly reduce the impact of the backdoor attack. Additionally, we show that supervised finetuning on task-specific labeled image data removes the backdoor trigger from the CLIP vision encoder. We show empirically that CleanCLIP maintains model performance on benign examples while erasing a range of backdoor attacks on multimodal contrastive learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bansal2023CleanCLIPMitigatingDataa_CleanCLIP.pdf;/Users/ma/Zotero/storage/533LTTZB/2303.html}
}

@inproceedings{Barbieri2020TweetEvalUnifiedBenchmark,
  title = {{{TweetEval}}: {{Unified Benchmark}} and {{Comparative Evaluation}} for {{Tweet Classification}}},
  shorttitle = {{{TweetEval}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Barbieri, Francesco and {Camacho-Collados}, Jose and Espinosa Anke, Luis and Neves, Leonardo},
  year = {2020},
  month = nov,
  pages = {1644--1650},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.148},
  url = {https://aclanthology.org/2020.findings-emnlp.148},
  urldate = {2023-08-18},
  abstract = {The experimental landscape in natural language processing for social media is too fragmented. Each year, new shared tasks and datasets are proposed, ranging from classics like sentiment analysis to irony detection or emoji prediction. Therefore, it is unclear what the current state of the art is, as there is no standardized evaluation protocol, neither a strong set of baselines trained on such domain-specific data. In this paper, we propose a new evaluation framework (TweetEval) consisting of seven heterogeneous Twitter-specific classification tasks. We also provide a strong set of baselines as starting point, and compare different language modeling pre-training strategies. Our initial experiments show the effectiveness of starting off with existing pre-trained generic language models, and continue training them on Twitter corpora.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Barbieri2020TweetEvalUnifiedBenchmark_TweetEval.pdf}
}

@misc{Bavarian2022EfficientTrainingLanguage,
  title = {Efficient {{Training}} of {{Language Models}} to {{Fill}} in the {{Middle}}},
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  year = {2022},
  month = jul,
  number = {arXiv:2207.14255},
  eprint = {2207.14255},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.14255},
  url = {http://arxiv.org/abs/2207.14255},
  urldate = {2023-04-27},
  abstract = {We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bavarian2022EfficientTrainingLanguage_Efficient Training of Language Models to Fill in the Middle.pdf;/Users/ma/Zotero/storage/CRUCNYHF/2207.html}
}

@misc{Bavarian2022EfficientTrainingLanguagea,
  title = {Efficient {{Training}} of {{Language Models}} to {{Fill}} in the {{Middle}}},
  author = {Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  year = {2022},
  month = jul,
  number = {arXiv:2207.14255},
  eprint = {2207.14255},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.14255},
  urldate = {2023-05-01},
  abstract = {We show that autoregressive language models can learn to infill text after we apply a straightforward transformation to the dataset, which simply moves a span of text from the middle of a document to its end. While this data augmentation has garnered much interest in recent years, we provide extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales. Given the usefulness, simplicity, and efficiency of training models to fill-in-the-middle (FIM), we suggest that future autoregressive language models be trained with FIM by default. To this end, we run a series of ablations on key hyperparameters, such as the data transformation frequency, the structure of the transformation, and the method of selecting the infill span. We use these ablations to prescribe strong default settings and best practices to train FIM models. We have released our best infilling model trained with best practices in our API, and release our infilling benchmarks to aid future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bavarian2022EfficientTrainingLanguagea_Efficient Training of Language Models to Fill in the Middle.pdf;/Users/ma/Zotero/storage/UCWG6ZJL/2207.html}
}

@misc{Berglund2023ReversalCurseLLMs,
  title = {The {{Reversal Curse}}: {{LLMs}} Trained on "{{A}} Is {{B}}" Fail to Learn "{{B}} Is {{A}}"},
  shorttitle = {The {{Reversal Curse}}},
  author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  year = {2023},
  month = sep,
  number = {arXiv:2309.12288},
  eprint = {2309.12288},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.12288},
  url = {http://arxiv.org/abs/2309.12288},
  urldate = {2023-11-01},
  abstract = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79\% of the time, compared to 33\% for the latter. This shows a failure of logical deduction that we hypothesize is caused by the Reversal Curse. Code is available at https://github.com/lukasberglund/reversal\_curse.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Berglund2023ReversalCurseLLMs_The Reversal Curse.pdf;/Users/ma/Zotero/storage/WSQ5T9X4/2309.html}
}

@misc{Bhasuran2023LiteratureBasedDiscovery,
  title = {Literature {{Based Discovery}} ({{LBD}}): {{Towards Hypothesis Generation}} and {{Knowledge Discovery}} in {{Biomedical Text Mining}}},
  shorttitle = {Literature {{Based Discovery}} ({{LBD}})},
  author = {Bhasuran, Balu and Murugesan, Gurusamy and Natarajan, Jeyakumar},
  year = {2023},
  month = oct,
  number = {arXiv:2310.03766},
  eprint = {2310.03766},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2310.03766},
  urldate = {2023-11-02},
  abstract = {Biomedical knowledge is growing in an astounding pace with a majority of this knowledge is represented as scientific publications. Text mining tools and methods represents automatic approaches for extracting hidden patterns and trends from this semi structured and unstructured data. In Biomedical Text mining, Literature Based Discovery (LBD) is the process of automatically discovering novel associations between medical terms otherwise mentioned in disjoint literature sets. LBD approaches proven to be successfully reducing the discovery time of potential associations that are hidden in the vast amount of scientific literature. The process focuses on creating concept profiles for medical terms such as a disease or symptom and connecting it with a drug and treatment based on the statistical significance of the shared profiles. This knowledge discovery approach introduced in 1989 still remains as a core task in text mining. Currently the ABC principle based two approaches namely open discovery and closed discovery are mostly explored in LBD process. This review starts with general introduction about text mining followed by biomedical text mining and introduces various literature resources such as MEDLINE, UMLS, MESH, and SemMedDB. This is followed by brief introduction of the core ABC principle and its associated two approaches open discovery and closed discovery in LBD process. This review also discusses the deep learning applications in LBD by reviewing the role of transformer models and neural networks based LBD models and its future aspects. Finally, reviews the key biomedical discoveries generated through LBD approaches in biomedicine and conclude with the current limitations and future directions of LBD.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bhasuran2023LiteratureBasedDiscovery_Literature Based Discovery (LBD).pdf;/Users/ma/Zotero/storage/LRUEQQSR/2310.html}
}

@misc{Bian2023ChatGPTKnowledgeableInexperienced,
  title = {{{ChatGPT}} Is a {{Knowledgeable}} but {{Inexperienced Solver}}: {{An Investigation}} of {{Commonsense Problem}} in {{Large Language Models}}},
  shorttitle = {{{ChatGPT}} Is a {{Knowledgeable}} but {{Inexperienced Solver}}},
  author = {Bian, Ning and Han, Xianpei and Sun, Le and Lin, Hongyu and Lu, Yaojie and He, Ben},
  year = {2023},
  month = mar,
  number = {arXiv:2303.16421},
  eprint = {2303.16421},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.16421},
  url = {http://arxiv.org/abs/2303.16421},
  urldate = {2023-04-20},
  abstract = {Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cannot precisely identify the needed commonsense knowledge for answering a specific question, i.e., ChatGPT does not precisely know what commonsense knowledge is required to answer a question. The above findings raise the need to investigate better mechanisms for utilizing commonsense knowledge in LLMs, such as instruction following, better commonsense guidance, etc.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bian2023ChatGPTKnowledgeableInexperienced_ChatGPT is a Knowledgeable but Inexperienced Solver.pdf;/Users/ma/Zotero/storage/SPH8VEAM/2303.html}
}

@misc{Bietti2023BirthTransformerMemory,
  title = {Birth of a {{Transformer}}: {{A Memory Viewpoint}}},
  shorttitle = {Birth of a {{Transformer}}},
  author = {Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
  year = {2023},
  month = jun,
  number = {arXiv:2306.00802},
  eprint = {2306.00802},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.00802},
  url = {http://arxiv.org/abs/2306.00802},
  urldate = {2023-06-05},
  abstract = {Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bietti2023BirthTransformerMemory_Birth of a Transformer.pdf;/Users/ma/Zotero/storage/IVBG5B7V/2306.html}
}

@inproceedings{Bisk2020ExperienceGroundsLanguage,
  title = {Experience {{Grounds Language}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Bisk, Yonatan and Holtzman, Ari and Thomason, Jesse and Andreas, Jacob and Bengio, Yoshua and Chai, Joyce and Lapata, Mirella and Lazaridou, Angeliki and May, Jonathan and Nisnevich, Aleksandr and Pinto, Nicolas and Turian, Joseph},
  year = {2020},
  month = nov,
  pages = {8718--8735},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.703},
  url = {https://aclanthology.org/2020.emnlp-main.703},
  urldate = {2023-04-20},
  abstract = {Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bisk2020ExperienceGroundsLanguage_Experience Grounds Language.pdf}
}

@inproceedings{Blodgett2021StereotypingNorwegianSalmon,
  title = {Stereotyping {{Norwegian Salmon}}: {{An Inventory}} of {{Pitfalls}} in {{Fairness Benchmark Datasets}}},
  shorttitle = {Stereotyping {{Norwegian Salmon}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
  year = {2021},
  month = aug,
  pages = {1004--1015},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.81},
  url = {https://aclanthology.org/2021.acl-long.81},
  urldate = {2023-10-10},
  abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens\textemdash originating from the social sciences\textemdash to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Blodgett2021StereotypingNorwegianSalmon_Stereotyping Norwegian Salmon.pdf}
}

@inproceedings{Blodgett2021StereotypingNorwegianSalmona,
  title = {Stereotyping {{Norwegian Salmon}}: {{An Inventory}} of {{Pitfalls}} in {{Fairness Benchmark Datasets}}},
  shorttitle = {Stereotyping {{Norwegian Salmon}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
  year = {2021},
  month = aug,
  pages = {1004--1015},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.81},
  url = {https://aclanthology.org/2021.acl-long.81},
  urldate = {2023-10-11},
  abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens\textemdash originating from the social sciences\textemdash to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Blodgett2021StereotypingNorwegianSalmona_Stereotyping Norwegian Salmon.pdf}
}

@misc{Boiko2023EmergentAutonomousScientific,
  title = {Emergent Autonomous Scientific Research Capabilities of Large Language Models},
  author = {Boiko, Daniil A. and MacKnight, Robert and Gomes, Gabe},
  year = {2023},
  month = apr,
  number = {arXiv:2304.05332},
  eprint = {2304.05332},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.05332},
  url = {http://arxiv.org/abs/2304.05332},
  urldate = {2023-11-01},
  abstract = {Transformer-based large language models are rapidly advancing in the field of machine learning research, with applications spanning natural language, biology, chemistry, and computer programming. Extreme scaling and reinforcement learning from human feedback have significantly improved the quality of generated text, enabling these models to perform various tasks and reason about their choices. In this paper, we present an Intelligent Agent system that combines multiple large language models for autonomous design, planning, and execution of scientific experiments. We showcase the Agent's scientific research capabilities with three distinct examples, with the most complex being the successful performance of catalyzed cross-coupling reactions. Finally, we discuss the safety implications of such systems and propose measures to prevent their misuse.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Physics - Chemical Physics},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Boiko2023EmergentAutonomousScientific_Emergent autonomous scientific research capabilities of large language models.pdf;/Users/ma/Zotero/storage/VHLL7MZH/2304.html}
}

@article{Bollen2002LatentVariablesPsychology,
  title = {Latent {{Variables}} in {{Psychology}} and the {{Social Sciences}}},
  author = {Bollen, Kenneth A.},
  year = {2002},
  month = feb,
  journal = {Annual Review of Psychology},
  volume = {53},
  number = {1},
  pages = {605--634},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.53.100901.135239},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.psych.53.100901.135239},
  urldate = {2023-10-24},
  abstract = {{$\blacksquare$} Abstract\hspace{0.6em} The paper discusses the use of latent variables in psychology and social science research. Local independence, expected value true scores, and nondeterministic functions of observed variables are three types of definitions for latent variables. These definitions are reviewed and an alternative ``sample realizations'' definition is presented. Another section briefly describes identification, latent variable indeterminancy, and other properties common to models with latent variables. The paper then reviews the role of latent variables in multiple regression, probit and logistic regression, factor analysis, latent curve models, item response theory, latent class analysis, and structural equation models. Though these application areas are diverse, the paper highlights the similarities as well as the differences in the manner in which the latent variables are defined and used. It concludes with an evaluation of the different definitions of latent variables and their properties.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/4I6QPCIU/Bollen - 2002 - Latent Variables in Psychology and the Social Scie.pdf}
}

@inproceedings{Borgeaud2022ImprovingLanguageModels,
  title = {Improving {{Language Models}} by {{Retrieving}} from {{Trillions}} of {{Tokens}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George Bm Van Den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego De Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack and Elsen, Erich and Sifre, Laurent},
  year = {2022},
  month = jun,
  pages = {2206--2240},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/borgeaud22a.html},
  urldate = {2023-10-20},
  abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\{\textbackslash texttimes\} fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Borgeaud2022ImprovingLanguageModels_Improving Language Models by Retrieving from Trillions of Tokens.pdf}
}

@article{BorgeaudImprovingLanguageModels,
  title = {Improving Language Models by Retrieving from Trillions of Tokens},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie},
  langid = {english},
  file = {/Users/ma/Zotero/storage/UUA3XPEV/Borgeaud et al. - Improving language models by retrieving from trill.pdf}
}

@article{Borsboom2003TheoreticalStatusLatent,
  title = {The Theoretical Status of Latent Variables},
  author = {Borsboom, Denny and Mellenbergh, Gideon J. and {van Heerden}, Jaap},
  year = {2003},
  journal = {Psychological Review},
  volume = {110},
  number = {2},
  pages = {203--219},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.110.2.203},
  abstract = {This article examines the theoretical status of latent variables as used in modern test theory models. First, it is argued that a consistent interpretation of such models requires a realist ontology for latent variables. Second, the relation between latent variables and their indicators is discussed. It is maintained that this relation can be interpreted as a causal one but that in measurement models for interindividual differences the relation does not apply to the level of the individual person. To substantiate intraindividual causal conclusions, one must explicitly represent individual level processes in the measurement model. Several research strategies that may be useful in this respect are discussed, and a typology of constructs is proposed on the basis of this analysis. The need to link individual processes to latent variable models for interindividual differences is emphasized. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Individual Differences,Item Response Theory,Latent Variables,Measurement,Measurement Models,Models,Psychological Theories,Realism (Philosophy)},
  file = {/Users/ma/Zotero/storage/C2JQ58XS/2003-00307-002.html}
}

@article{Bozarth2023WisdomTwoCrowds,
  title = {Wisdom of {{Two Crowds}}: {{Misinformation Moderation}} on {{Reddit}} and {{How}} to {{Improve}} This {{Process---A Case Study}} of {{COVID-19}}},
  shorttitle = {Wisdom of {{Two Crowds}}},
  author = {Bozarth, Lia and Im, Jane and Quarles, Christopher and Budak, Ceren},
  year = {2023},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {7},
  number = {CSCW1},
  pages = {1--33},
  issn = {2573-0142},
  doi = {10.1145/3579631},
  url = {https://dl.acm.org/doi/10.1145/3579631},
  urldate = {2023-04-27},
  abstract = {Past work has explored various ways for online platforms to leverage crowd wisdom for misinformation detection and moderation. Yet, platforms often relegate governance to their communities, and limited research has been done from the perspective of these communities and their moderators. How is misinformation currently moderated in online communities that are heavily self-governed? What role does the crowd play in this process, and how can this process be improved? In this study, we answer these questions through semi-structured interviews with Reddit moderators. We focus on a case study of COVID-19 misinformation. First, our analysis identifies a general moderation workflow model encompassing various processes participants use for handling COVID-19 misinformation. Further, we show that the moderation workflow revolves around three elements: content facticity, user intent, and perceived harm. Next, our interviews reveal that Reddit moderators rely on two types of crowd wisdom for misinformation detection. Almost all participants are heavily reliant on reports from crowds of ordinary users to identify potential misinformation. A second crowd--participants' own moderation teams and expert moderators of other communities--provide support when participants encounter difficult, ambiguous cases. Finally, we use design probes to better understand how different types of crowd signals---from ordinary users and moderators---readily available on Reddit can assist moderators with identifying misinformation. We observe that nearly half of all participants preferred these cues over labels from expert fact-checkers because these cues can help them discern user intent. Additionally, a quarter of the participants distrust professional fact-checkers, raising important concerns about misinformation moderation.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/4ATWXMCE/Bozarth et al. - 2023 - Wisdom of Two Crowds Misinformation Moderation on.pdf}
}

@article{Brady2020AttentionalCaptureHelps,
  title = {Attentional Capture Helps Explain Why Moral and Emotional Content Go Viral},
  author = {Brady, William J. and Gantman, Ana P. and Van Bavel, Jay J.},
  year = {2020},
  month = apr,
  journal = {Journal of Experimental Psychology. General},
  volume = {149},
  number = {4},
  pages = {746--756},
  issn = {1939-2222},
  doi = {10.1037/xge0000673},
  abstract = {Our social media newsfeeds are filled with a variety of content all battling for our limited attention. Across 3 studies, we investigated whether moral and emotional content captures our attention more than other content and if this may help explain why this content is more likely to go viral online. Using a combination of controlled lab experiments and nearly 50,000 political tweets, we found that moral and emotional content are prioritized in early visual attention more than neutral content, and that such attentional capture is associated with increased retweets during political conversations online. Furthermore, we found that the differences in attentional capture among moral and emotional stimuli could not be fully explained by differences in arousal. These studies suggest that attentional capture is 1 basic psychological process that helps explain the increased diffusion of moral and emotional content during political discourse on social media, and shed light on ways in which political leaders, disinformation profiteers, marketers, and activist organizations can spread moralized content by capitalizing on natural tendencies of our perceptual systems. (PsycINFO Database Record (c) 2020 APA, all rights reserved).},
  langid = {english},
  pmid = {31486666},
  keywords = {Adolescent,Arousal,Attention,Emotions,Female,Humans,Male,Morals,Social Media,Young Adult}
}

@article{Brady2020MADModelMoral,
  title = {The {{MAD Model}} of {{Moral Contagion}}: {{The Role}} of {{Motivation}}, {{Attention}}, and {{Design}} in the {{Spread}} of {{Moralized Content Online}}},
  shorttitle = {The {{MAD Model}} of {{Moral Contagion}}},
  author = {Brady, William J. and Crockett, M. J. and Van Bavel, Jay J.},
  year = {2020},
  month = jul,
  journal = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  volume = {15},
  number = {4},
  pages = {978--1010},
  issn = {1745-6924},
  doi = {10.1177/1745691620917336},
  abstract = {With more than 3 billion users, online social networks represent an important venue for moral and political discourse and have been used to organize political revolutions, influence elections, and raise awareness of social issues. These examples rely on a common process to be effective: the ability to engage users and spread moralized content through online networks. Here, we review evidence that expressions of moral emotion play an important role in the spread of moralized content (a phenomenon we call moral contagion). Next, we propose a psychological model called the motivation, attention, and design (MAD) model to explain moral contagion. The MAD model posits that people have group-identity-based motivations to share moral-emotional content, that such content is especially likely to capture our attention, and that the design of social-media platforms amplifies our natural motivational and cognitive tendencies to spread such content. We review each component of the model (as well as interactions between components) and raise several novel, testable hypotheses that can spark progress on the scientific investigation of civic engagement and activism, political polarization, propaganda and disinformation, and other moralized behaviors in the digital age.},
  langid = {english},
  pmid = {32511060},
  keywords = {Attention,emotion,Emotions,Humans,{Models, Psychological},morality,Morals,Motivation,politics,Politics,social media,Social Media,Social Networking,social networks}
}

@misc{Bran2023ChemCrowAugmentingLargelanguage,
  title = {{{ChemCrow}}: {{Augmenting}} Large-Language Models with Chemistry Tools},
  shorttitle = {{{ChemCrow}}},
  author = {Bran, Andres M. and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D. and Schwaller, Philippe},
  year = {2023},
  month = oct,
  number = {arXiv:2304.05376},
  eprint = {2304.05376},
  primaryclass = {physics, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.05376},
  url = {http://arxiv.org/abs/2304.05376},
  urldate = {2023-11-01},
  abstract = {Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow's effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow's performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry.},
  archiveprefix = {arxiv},
  keywords = {Physics - Chemical Physics,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Bran2023ChemCrowAugmentingLargelanguage_ChemCrow.pdf;/Users/ma/Zotero/storage/S6HE82AS/2304.html}
}

@article{Brashier2020AgingEraFake,
  title = {Aging in an {{Era}} of {{Fake News}}},
  author = {Brashier, Nadia M. and Schacter, Daniel L.},
  year = {2020},
  month = jun,
  journal = {Current Directions in Psychological Science},
  volume = {29},
  number = {3},
  pages = {316--323},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721420915872},
  url = {https://doi.org/10.1177/0963721420915872},
  urldate = {2023-10-13},
  abstract = {Misinformation causes serious harm, from sowing doubt in modern medicine to inciting violence. Older adults are especially susceptible\textemdash they shared the most fake news during the 2016 U.S. election. The most intuitive explanation for this pattern lays the blame on cognitive deficits. Although older adults forget where they learned information, fluency remains intact, and knowledge accumulated across decades helps them evaluate claims. Thus, cognitive declines cannot fully explain older adults' engagement with fake news. Late adulthood also involves social changes, including greater trust, difficulty detecting lies, and less emphasis on accuracy when communicating. In addition, older adults are relative newcomers to social media and may struggle to spot sponsored content or manipulated images. In a post-truth world, interventions should account for older adults' shifting social goals and gaps in their digital literacy.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Brashier2020AgingEraFake_Aging in an Era of Fake News.pdf}
}

@misc{Brown2020LanguageModelsAre,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-04-29},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Brown2020LanguageModelsAre_Language Models are Few-Shot Learners.pdf;/Users/ma/Zotero/storage/LTNLVG2U/2005.html}
}

@misc{Cai2020GroupwiseContrastiveLearning,
  title = {Group-Wise {{Contrastive Learning}} for {{Neural Dialogue Generation}}},
  author = {Cai, Hengyi and Chen, Hongshen and Song, Yonghao and Ding, Zhuoye and Bao, Yongjun and Yan, Weipeng and Zhao, Xiaofang},
  year = {2020},
  month = oct,
  number = {arXiv:2009.07543},
  eprint = {2009.07543},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.07543},
  urldate = {2023-02-17},
  abstract = {Neural dialogue response generation has gained much popularity in recent years. Maximum Likelihood Estimation (MLE) objective is widely adopted in existing dialogue model learning. However, models trained with MLE objective function are plagued by the low-diversity issue when it comes to the open-domain conversational setting. Inspired by the observation that humans not only learn from the positive signals but also benefit from correcting behaviors of undesirable actions, in this work, we introduce contrastive learning into dialogue generation, where the model explicitly perceives the difference between the well-chosen positive and negative utterances. Specifically, we employ a pretrained baseline model as a reference. During contrastive learning, the target dialogue model is trained to give higher conditional probabilities for the positive samples, and lower conditional probabilities for those negative samples, compared to the reference model. To manage the multi-mapping relations prevailed in human conversation, we augment contrastive dialogue learning with group-wise dual sampling. Extensive experimental results show that the proposed group-wise contrastive learning framework is suited for training a wide range of neural dialogue generation models with very favorable performance over the baseline training approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cai2020GroupwiseContrastiveLearning_Group-wise Contrastive Learning for Neural Dialogue Generation.pdf;/Users/ma/Zotero/storage/975AJIGB/2009.html}
}

@misc{Cai2022SimpleEffectiveBaseline,
  title = {A Simple yet Effective Baseline for Non-Attributed Graph Classification},
  author = {Cai, Chen and Wang, Yusu},
  year = {2022},
  month = may,
  number = {arXiv:1811.03508},
  eprint = {1811.03508},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1811.03508},
  urldate = {2023-10-17},
  abstract = {Graphs are complex objects that do not lend themselves easily to typical learning tasks. Recently, a range of approaches based on graph kernels or graph neural networks have been developed for graph classification and for representation learning on graphs in general. As the developed methodologies become more sophisticated, it is important to understand which components of the increasingly complex methods are necessary or most effective. As a first step, we develop a simple yet meaningful graph representation, and explore its effectiveness in graph classification. We test our baseline representation for the graph classification task on a range of graph datasets. Interestingly, this simple representation achieves similar performance as the state-of-the-art graph kernels and graph neural networks for non-attributed graph classification. Its performance on classifying attributed graphs is slightly weaker as it does not incorporate attributes. However, given its simplicity and efficiency, we believe that it still serves as an effective baseline for attributed graph classification. Our graph representation is efficient (linear-time) to compute. We also provide a simple connection with the graph neural networks. Note that these observations are only for the task of graph classification while existing methods are often designed for a broader scope including node embedding and link prediction. The results are also likely biased due to the limited amount of benchmark datasets available. Nevertheless, the good performance of our simple baseline calls for the development of new, more comprehensive benchmark datasets so as to better evaluate and analyze different graph learning methods. Furthermore, given the computational efficiency of our graph summary, we believe that it is a good candidate as a baseline method for future graph classification (or even other graph learning) studies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cai2022SimpleEffectiveBaseline_A simple yet effective baseline for non-attributed graph classification.pdf;/Users/ma/Zotero/storage/REG8MQ9R/1811.html}
}

@article{Cao2020InferringUnobservablePopulation,
  title = {Inferring an Unobservable Population Size from Observable Samples},
  author = {Cao, Jack and Banaji, Mahzarin R.},
  year = {2020},
  month = apr,
  journal = {Memory \& Cognition},
  volume = {48},
  number = {3},
  pages = {348--360},
  issn = {1532-5946},
  doi = {10.3758/s13421-019-00974-w},
  url = {https://doi.org/10.3758/s13421-019-00974-w},
  urldate = {2023-10-24},
  abstract = {Success in the physical and social worlds often requires knowledge of population size. However, many populations cannot be observed in their entirety, making direct assessment of their size difficult, if not impossible. Nevertheless, an unobservable population size can be inferred from observable samples. We measured people's ability to make such inferences and their confidence in these inferences. Contrary to past work suggesting insensitivity to sample size and failures in statistical reasoning, inferences of populations size were accurate\textemdash but only when observable samples indicated a large underlying population. When observable samples indicated a small underlying population, inferences were systematically biased. This error, which cannot be attributed to a heuristics account, was compounded by a metacognitive failure: Confidence was highest when accuracy was at its worst. This dissociation between accuracy and confidence was confirmed by a manipulation that shifted the magnitude and variability of people's inferences without impacting their confidence. Together, these results (a) highlight the mental acuity and limits of a fundamental human judgment and (b) demonstrate an inverse relationship between cognition and metacognition.},
  langid = {english},
  keywords = {Accuracy,Confidence,Numerical cognition,Population estimates,Sampling processes},
  file = {/Users/ma/Zotero/storage/AFY8UIMI/Cao and Banaji - 2020 - Inferring an unobservable population size from obs.pdf}
}

@inproceedings{Cao2023AutoregressiveEntityRetrieval,
  title = {Autoregressive {{Entity Retrieval}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Cao, Nicola De and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
  year = {2023},
  month = jan,
  url = {https://openreview.net/forum?id=5k8F6UU39V},
  urldate = {2023-05-01},
  abstract = {Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cao2023AutoregressiveEntityRetrieval_Autoregressive Entity Retrieval.pdf}
}

@article{Carvalho2023KnowledgeGraphEmbeddings,
  title = {Knowledge {{Graph Embeddings}} for {{ICU}} Readmission Prediction},
  author = {Carvalho, Ricardo M. S. and Oliveira, Daniela and Pesquita, Catia},
  year = {2023},
  month = jan,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {23},
  number = {1},
  pages = {12},
  issn = {1472-6947},
  doi = {10.1186/s12911-022-02070-7},
  url = {https://doi.org/10.1186/s12911-022-02070-7},
  urldate = {2023-09-04},
  abstract = {Intensive Care Unit (ICU) readmissions represent both a health risk for patients,with increased mortality rates and overall health deterioration, and a financial burden for healthcare facilities. As healthcare became more data-driven with the introduction of Electronic Health Records (EHR), machine learning methods have been applied to predict ICU readmission risk. However, these methods disregard the meaning and relationships of data objects and work blindly over clinical data without taking into account scientific knowledge and context. Ontologies and Knowledge Graphs can help bridge this gap between data and scientific context, as they are computational artefacts that represent the entities of a domain and their relationships to each other in a formalized way.},
  keywords = {ICU readmission prediction,Knowledge Graph embeddings,Machine learning,Ontologies,Semantic annotations},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Carvalho2023KnowledgeGraphEmbeddings_Knowledge Graph Embeddings for ICU readmission prediction.pdf;/Users/ma/Zotero/storage/P9YT969N/s12911-022-02070-7.html}
}

@inproceedings{Chang2023DataCurationAlone,
  title = {Data {{Curation Alone Can Stabilize In-context Learning}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chang, Ting-Yun and Jia, Robin},
  year = {2023},
  month = jul,
  pages = {8123--8144},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.452},
  url = {https://aclanthology.org/2023.acl-long.452},
  urldate = {2023-10-23},
  abstract = {In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets\textemdash both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy over sampling from the entire training set by 7.7\% and 6.3\%, respectively. Surprisingly, the stable subset examples are not especially diverse in content or low in perplexity, in contrast with other work suggesting that diversity and perplexity are important when prompting LLMs.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chang2023DataCurationAlone_Data Curation Alone Can Stabilize In-context Learning.pdf}
}

@article{Chen2020TrackingSocialMedia,
  title = {Tracking {{Social Media Discourse About}} the {{COVID-19 Pandemic}}: {{Development}} of a {{Public Coronavirus Twitter Data Set}}},
  shorttitle = {Tracking {{Social Media Discourse About}} the {{COVID-19 Pandemic}}},
  author = {Chen, Emily and Lerman, Kristina and Ferrara, Emilio},
  year = {2020},
  month = may,
  journal = {JMIR Public Health and Surveillance},
  volume = {6},
  number = {2},
  pages = {e19273},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/19273},
  url = {https://publichealth.jmir.org/2020/2/e19273},
  urldate = {2023-10-15},
  abstract = {Background: At the time of this writing, the coronavirus disease (COVID-19) pandemic outbreak has already put tremendous strain on many countries' citizens, resources, and economies around the world. Social distancing measures, travel bans, self-quarantines, and business closures are changing the very fabric of societies worldwide. With people forced out of public spaces, much of the conversation about these phenomena now occurs online on social media platforms like Twitter. Objective: In this paper, we describe a multilingual COVID-19 Twitter data set that we are making available to the research community via our COVID-19-TweetIDs GitHub repository. Methods: We started this ongoing data collection on January 28, 2020, leveraging Twitter's streaming application programming interface (API) and Tweepy to follow certain keywords and accounts that were trending at the time data collection began. We used Twitter's search API to query for past tweets, resulting in the earliest tweets in our collection dating back to January 21, 2020. Results: Since the inception of our collection, we have actively maintained and updated our GitHub repository on a weekly basis. We have published over 123 million tweets, with over 60\% of the tweets in English. This paper also presents basic statistics that show that Twitter activity responds and reacts to COVID-19-related events. Conclusions: It is our hope that our contribution will enable the study of online conversation dynamics in the context of a planetary-scale epidemic outbreak of unprecedented proportions and implications. This data set could also help track COVID-19-related misinformation and unverified rumors or enable the understanding of fear and panic\textemdash and undoubtedly more.},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2020TrackingSocialMedia_Tracking Social Media Discourse About the COVID-19 Pandemic.pdf;/Users/ma/Zotero/storage/MBCF6YUH/e19273.html}
}

@article{Chen2022DISCODistillingPhrasal,
  title = {{{DISCO}}: {{Distilling Phrasal Counterfactuals}} with {{Large Language Models}}},
  shorttitle = {{{DISCO}}},
  author = {Chen, Zeming and Gao, Qiyue and Richardson, Kyle and Bosselut, Antoine and Sabharwal, Ashish},
  year = {2022},
  month = dec,
  doi = {10.48550/arXiv.2212.10534},
  url = {https://arxiv.org/abs/2212.10534v1},
  urldate = {2023-02-24},
  abstract = {Recent methods demonstrate that data augmentation using counterfactual knowledge can teach models the causal structure of a task, leading to robust and generalizable models. However, such counterfactual data often has a limited scale and diversity if crowdsourced and is computationally expensive to extend to new perturbation types if generated using supervised methods. To address this, we introduce a new framework called DISCO for automatically generating high-quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters the generation to distill high-quality counterfactual data. We show that learning with this counterfactual data yields a comparatively small student model that is 6\% (absolute) more robust and generalizes 5\% better across distributions than baselines on various challenging evaluations. This model is also 15\% more sensitive in differentiating original and counterfactual examples, on three evaluation sets written by human workers and via human-AI collaboration.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2022DISCODistillingPhrasal_DISCO.pdf}
}

@misc{Chen2022ProgramThoughtsPrompting,
  title = {Program of {{Thoughts Prompting}}: {{Disentangling Computation}} from {{Reasoning}} for {{Numerical Reasoning Tasks}}},
  shorttitle = {Program of {{Thoughts Prompting}}},
  author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
  year = {2022},
  month = nov,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2211.12588v3},
  urldate = {2023-10-23},
  abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\textbackslash\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github\textbackslash footnote\{\textbackslash url\{https://github.com/wenhuchen/Program-of-Thoughts\}\}.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2022ProgramThoughtsPrompting_Program of Thoughts Prompting.pdf}
}

@misc{Chen2023AlpaGasusTrainingBetter,
  title = {{{AlpaGasus}}: {{Training A Better Alpaca}} with {{Fewer Data}}},
  shorttitle = {{{AlpaGasus}}},
  author = {Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and Jin, Hongxia},
  year = {2023},
  month = jul,
  number = {arXiv:2307.08701},
  eprint = {2307.08701},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.08701},
  url = {http://arxiv.org/abs/2307.08701},
  urldate = {2023-08-03},
  abstract = {Large language models\textasciitilde (LLMs) obtain instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and removes low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and its 13B variant matches \${$>$}90\textbackslash\%\$ performance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes \textbackslash footnote\{We apply IFT for the same number of epochs as Alpaca(7B) but on fewer data, using 4\$\textbackslash times\$NVIDIA A100 (80GB) GPUs and following the original Alpaca setting and hyperparameters.\}. Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models. Our project page is available at: \textbackslash url\{https://lichang-chen.github.io/AlpaGasus/\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2023AlpaGasusTrainingBetter_AlpaGasus.pdf;/Users/ma/Zotero/storage/Y6HVCGHW/2307.html}
}

@misc{Chen2023FireActLanguageAgent,
  title = {{{FireAct}}: {{Toward Language Agent Fine-tuning}}},
  shorttitle = {{{FireAct}}},
  author = {Chen, Baian and Shu, Chang and Shareghi, Ehsan and Collier, Nigel and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05915},
  eprint = {2310.05915},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.05915},
  url = {http://arxiv.org/abs/2310.05915},
  urldate = {2023-10-26},
  abstract = {Recent efforts have augmented language models (LMs) with external tools or environments, leading to the development of language agents that can reason and act. However, most of these agents rely on few-shot prompting techniques with off-the-shelf LMs. In this paper, we investigate and argue for the overlooked direction of fine-tuning LMs to obtain language agents. Using a setup of question answering (QA) with a Google search API, we explore a variety of base LMs, prompting methods, fine-tuning data, and QA tasks, and find language agents are consistently improved after fine-tuning their backbone LMs. For example, fine-tuning Llama2-7B with 500 agent trajectories generated by GPT-4 leads to a 77\% HotpotQA performance increase. Furthermore, we propose FireAct, a novel approach to fine-tuning LMs with trajectories from multiple tasks and prompting methods, and show having more diverse fine-tuning data can further improve agents. Along with other findings regarding scaling effects, robustness, generalization, efficiency and cost, our work establishes comprehensive benefits of fine-tuning LMs for agents, and provides an initial set of experimental designs, insights, as well as open questions toward language agent fine-tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2023FireActLanguageAgent_FireAct.pdf;/Users/ma/Zotero/storage/RUKRN525/2310.html}
}

@misc{Chen2023MixtureSoftPrompts,
  title = {Mixture of {{Soft Prompts}} for {{Controllable Data Generation}}},
  author = {Chen, Derek and Lee, Celine and Lu, Yunan and Rosati, Domenic and Yu, Zhou},
  year = {2023},
  month = mar,
  number = {arXiv:2303.01580},
  eprint = {2303.01580},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.01580},
  urldate = {2023-03-06},
  abstract = {Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2023MixtureSoftPrompts_Mixture of Soft Prompts for Controllable Data Generation.pdf;/Users/ma/Zotero/storage/J7R687XZ/2303.html}
}

@misc{Chen2023MixtureSoftPromptsa,
  title = {Mixture of {{Soft Prompts}} for {{Controllable Data Generation}}},
  author = {Chen, Derek and Lee, Celine and Lu, Yunan and Rosati, Domenic and Yu, Zhou},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2303.01580v2},
  urldate = {2023-10-23},
  abstract = {Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chen2023MixtureSoftPromptsa_Mixture of Soft Prompts for Controllable Data Generation.pdf}
}

@misc{Cheng2023BindingLanguageModels,
  title = {Binding {{Language Models}} in {{Symbolic Languages}}},
  author = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  year = {2023},
  month = feb,
  number = {arXiv:2210.02875},
  eprint = {2210.02875},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.02875},
  url = {http://arxiv.org/abs/2210.02875},
  urldate = {2023-03-16},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cheng2023BindingLanguageModels_Binding Language Models in Symbolic Languages.pdf;/Users/ma/Zotero/storage/UPY97NW3/2210.html}
}

@article{Cheong2023AdaptiveIntegrationCategorical,
  title = {Adaptive {{Integration}} of {{Categorical}} and {{Multi-relational Ontologies}} with {{EHR Data}} for {{Medical Concept Embedding}}},
  author = {Cheong, Chin Wang and Yin, Kejing and Cheung, William K. and Fung, Benjamin C. M. and Poon, Jonathan},
  year = {2023},
  month = sep,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  issn = {2157-6904},
  doi = {10.1145/3625224},
  url = {https://dl.acm.org/doi/10.1145/3625224},
  urldate = {2023-10-31},
  abstract = {Representation learning has been applied to Electronic Health Records (EHR) for medical concept embedding and the downstream predictive analytics tasks with promising results. Medical ontologies can also be integrated to guide the learning so that the embedding space can better align with existing medical knowledge. Yet, properly carrying out the integration is non-trivial. Medical concepts which are similar according to a medical ontology may not be necessarily close in the embedding space learned from the EHR data, as medical ontologies organize medical concepts for their own specific objectives. Any integration methodology without considering the underlying inconsistency will result in sub-optimal medical concept embedding, and in turn degrade the performance of the downstream tasks. In this paper, we propose a novel representation learning framework called ADORE(ADaptive Ontological REpresentations) which allows the medical ontologies to adapt their structures for more robust integrating with the EHR data. ADORE first learns multiple embeddings for each category in the ontology via an attention mechanism. At the same time, it supports an adaptive integration of categorical and multi-relational ontologies in the embedding space using a category-aware graph attention network. We evaluate the performance of ADORE on a number of predictive analytics tasks using two EHR datasets. Our experimental results show that the medical concept embeddings obtained by ADORE can outperform the state-of-the-art methods for all the tasks. More importantly, it can result in clinically meaningful sub-categorization of the existing ontological categories and yield attention values which can further enhance the model interpretability.},
  keywords = {data mining with ontologies,Electronic health record,predictive data analytics,representation learning},
  annotation = {Just Accepted},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cheong2023AdaptiveIntegrationCategorical_Adaptive Integration of Categorical and Multi-relational Ontologies with EHR.pdf}
}

@inproceedings{Chia2022RelationPromptLeveragingPrompts,
  title = {{{RelationPrompt}}: {{Leveraging Prompts}} to {{Generate Synthetic Data}} for {{Zero-Shot Relation Triplet Extraction}}},
  shorttitle = {{{RelationPrompt}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Chia, Yew Ken and Bing, Lidong and Poria, Soujanya and Si, Luo},
  year = {2022},
  month = may,
  pages = {45--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.5},
  url = {https://aclanthology.org/2022.findings-acl.5},
  urldate = {2023-05-01},
  abstract = {Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chia2022RelationPromptLeveragingPrompts_RelationPrompt.pdf}
}

@inproceedings{Chia2022RelationPromptLeveragingPromptsa,
  title = {{{RelationPrompt}}: {{Leveraging Prompts}} to {{Generate Synthetic Data}} for {{Zero-Shot Relation Triplet Extraction}}},
  shorttitle = {{{RelationPrompt}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Chia, Yew Ken and Bing, Lidong and Poria, Soujanya and Si, Luo},
  year = {2022},
  month = may,
  pages = {45--57},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.5},
  url = {https://aclanthology.org/2022.findings-acl.5},
  urldate = {2023-06-24},
  abstract = {Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chia2022RelationPromptLeveragingPromptsa_RelationPrompt.pdf}
}

@article{Choi2016DoctorAIPredicting,
  title = {Doctor {{AI}}: {{Predicting Clinical Events}} via {{Recurrent Neural Networks}}},
  shorttitle = {Doctor {{AI}}},
  author = {Choi, Edward and Bahadori, Mohammad Taha and Schuetz, Andy and Stewart, Walter F. and Sun, Jimeng},
  year = {2016},
  month = aug,
  journal = {JMLR workshop and conference proceedings},
  volume = {56},
  pages = {301--318},
  issn = {1938-7288},
  abstract = {Leveraging large historical data in electronic health record (EHR), we developed Doctor AI, a generic predictive model that covers observed medical conditions and medication uses. Doctor AI is a temporal model using recurrent neural networks (RNN) and was developed and applied to longitudinal time stamped EHR data from 260K patients over 8 years. Encounter records (e.g. diagnosis codes, medication codes or procedure codes) were input to RNN to predict (all) the diagnosis and medication categories for a subsequent visit. Doctor AI assesses the history of patients to make multilabel predictions (one label for each diagnosis or medication category). Based on separate blind test set evaluation, Doctor AI can perform differential diagnosis with up to 79\% recall@30, significantly higher than several baselines. Moreover, we demonstrate great generalizability of Doctor AI by adapting the resulting models from one institution to another without losing substantial accuracy.},
  langid = {english},
  pmcid = {PMC5341604},
  pmid = {28286600}
}

@article{Choi2016LearningLowDimensionalRepresentations,
  title = {Learning {{Low-Dimensional Representations}} of {{Medical Concepts}}},
  author = {Choi, Youngduck and Chiu, Chill Yi-I. and Sontag, David},
  year = {2016},
  journal = {AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science},
  volume = {2016},
  pages = {41--50},
  issn = {2153-4063},
  abstract = {We show how to learn low-dimensional representations (embeddings) of a wide range of concepts in medicine, including diseases (e.g., ICD9 codes), medications, procedures, and laboratory tests. We expect that these embeddings will be useful across medical informatics for tasks such as cohort selection and patient summarization. These embeddings are learned using a technique called neural language modeling from the natural language processing community. However, rather than learning the embeddings solely from text, we show how to learn the embeddings from claims data, which is widely available both to providers and to payers. We also show that with a simple algorithmic adjustment, it is possible to learn medical concept embeddings in a privacy preserving manner from co-occurrence counts derived from clinical narratives. Finally, we establish a methodological framework, arising from standard medical ontologies such as UMLS, NDF-RT, and CCS, to further investigate the embeddings and precisely characterize their quantitative properties.},
  langid = {english},
  pmcid = {PMC5001761},
  pmid = {27570647}
}

@misc{Chung2022ScalingInstructionFinetunedLanguage,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and {Castro-Ros}, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = dec,
  number = {arXiv:2210.11416},
  eprint = {2210.11416},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.11416},
  urldate = {2023-04-29},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Chung2022ScalingInstructionFinetunedLanguage_Scaling Instruction-Finetuned Language Models.pdf;/Users/ma/Zotero/storage/T8GAS46S/2210.html}
}

@misc{Cohen2023CrawlingInternalKnowledgeBase,
  title = {Crawling the {{Internal Knowledge-Base}} of {{Language Models}}},
  author = {Cohen, Roi and Geva, Mor and Berant, Jonathan and Globerson, Amir},
  year = {2023},
  month = jan,
  number = {arXiv:2301.12810},
  eprint = {2301.12810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12810},
  url = {http://arxiv.org/abs/2301.12810},
  urldate = {2023-10-26},
  abstract = {Language models are trained on large volumes of text, and as a result their parameters might contain a significant body of factual knowledge. Any downstream task performed by these models implicitly builds on these facts, and thus it is highly desirable to have means for representing this body of knowledge in an interpretable way. However, there is currently no mechanism for such a representation. Here, we propose to address this goal by extracting a knowledge-graph of facts from a given language model. We describe a procedure for ``crawling'' the internal knowledge-base of a language model. Specifically, given a seed entity, we expand a knowledge-graph around it. The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision (i.e., that no wrong facts are generated) and recall (i.e., the number of facts generated). We evaluate our approach on graphs crawled starting from dozens of seed entities, and show it yields high precision graphs (82-92\%), while emitting a reasonable number of facts per entity.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cohen2023CrawlingInternalKnowledgeBase_Crawling the Internal Knowledge-Base of Language Models.pdf;/Users/ma/Zotero/storage/UP46F3G6/2301.html}
}

@inproceedings{Cui2022DocumentLevelEventExtraction,
  title = {Document-{{Level Event Extraction}} via {{Human-Like Reading Process}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cui, Shiyao and Cong, Xin and Yu, Bowen and Liu, Tingwen and Wang, Yucheng and Shi, Jinqiao},
  year = {2022},
  month = may,
  pages = {6337--6341},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9747721},
  abstract = {Document-level Event Extraction (DEE) is particularly tricky due to the two challenges it poses: scattering-arguments and multi-events. The first challenge means that arguments of one event record could reside in different sentences in the document, while the second one reflects that one document may simultaneously contain multiple such event records. Motivated by humans' reading cognitive to extract information of interests, in this paper, we propose a method called HRE (Human Reading inspired Extractor for Document Events), where DEE is decomposed into these two iterative stages, rough reading and elaborate reading. Specifically, the first stage browses the document to detect the occurrence of events, and the second stage serves to extract specific event arguments. For each concrete event role, elaborate reading hierarchically works from sentences to characters to locate arguments across sentences, thus the scattering-arguments problem is tackled. Meanwhile, rough reading is explored in a multi-round manner to discover undetected events, thus the multi-events problem is handled. Experiment results show the superiority of HRE over prior competitive methods.},
  keywords = {Acoustics,Cognitive processes,Conferences,Data mining,dataset\_Doc2EDAG,document-level event extraction,Document-level Event Extraction,Event Extraction,Indexes,Information Extraction,Iterative methods,Natural Language Processing,Signal processing},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Cui2022DocumentLevelEventExtraction_Document-Level Event Extraction via Human-Like Reading Process.pdf;/Users/ma/Zotero/storage/7CXCAXGN/9747721.html}
}

@article{Dai2019JointExtractionEntities,
  title = {Joint {{Extraction}} of {{Entities}} and {{Overlapping Relations Using Position-Attentive Sequence Labeling}}},
  author = {Dai, Dai and Xiao, Xinyan and Lyu, Yajuan and Dou, Shan and She, Qiaoqiao and Wang, Haifeng},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {6300--6308},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33016300},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4591},
  urldate = {2023-05-01},
  abstract = {Joint entity and relation extraction is to detect entity and relation using a single model. In this paper, we present a novel unified joint extraction model which directly tags entity and relation labels according to a query word position p, i.e., detecting entity at p, and identifying entities at other positions that have relationship with the former. To this end, we first design a tagging scheme to generate n tag sequences for an n-word sentence. Then a position-attention mechanism is introduced to produce different sentence representations for every query position to model these n tag sequences. In this way, our method can simultaneously extract all entities and their type, as well as all overlapping relations. Experiment results show that our framework performances significantly better on extracting overlapping relations as well as detecting long-range relation, and thus we achieve state-of-the-art performance on two public datasets.},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Dai2019JointExtractionEntities_Joint Extraction of Entities and Overlapping Relations Using Position-Attentive.pdf}
}

@inproceedings{Dai2022BiDirectionalIterativePromptTuning,
  title = {Bi-{{Directional Iterative Prompt-Tuning}} for {{Event Argument Extraction}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Dai, Lu and Wang, Bang and Xiang, Wei and Mo, Yijun},
  year = {2022},
  month = dec,
  pages = {6251--6263},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.419},
  urldate = {2023-02-09},
  abstract = {Recently, prompt-tuning has attracted growing interests in event argument extraction (EAE). However, the existing prompt-tuning methods have not achieved satisfactory performance due to the lack of consideration of entity information. In this paper, we propose a bi-directional iterative prompt-tuning method for EAE, where the EAE task is treated as a cloze-style task to take full advantage of entity information and pre-trained language models (PLMs). Furthermore, our method explores event argument interactions by introducing the argument roles of contextual entities into prompt construction. Since template and verbalizer are two crucial components in a cloze-style prompt, we propose to utilize the role label semantic knowledge to construct a semantic verbalizer and design three kind of templates for the EAE task. Experiments on the ACE 2005 English dataset with standard and low-resource settings show that the proposed method significantly outperforms the peer state-of-the-art methods.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Dai2022BiDirectionalIterativePromptTuning_Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction.pdf}
}

@article{Davani2023HateSpeechClassifiers,
  title = {Hate {{Speech Classifiers Learn Normative Social Stereotypes}}},
  author = {Davani, Aida Mostafazadeh and Atari, Mohammad and Kennedy, Brendan and Dehghani, Morteza},
  year = {2023},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {300--319},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00550},
  url = {https://doi.org/10.1162/tacl\_a\_00550},
  urldate = {2023-04-27},
  abstract = {Social stereotypes negatively impact individuals' judgments about different groups and may have a critical role in understanding language directed toward marginalized groups. Here, we assess the role of social stereotypes in the automated detection of hate speech in the English language by examining the impact of social stereotypes on annotation behaviors, annotated datasets, and hate speech classifiers. Specifically, we first investigate the impact of novice annotators' stereotypes on their hate-speech-annotation behavior. Then, we examine the effect of normative stereotypes in language on the aggregated annotators' judgments in a large annotated corpus. Finally, we demonstrate how normative stereotypes embedded in language resources are associated with systematic prediction errors in a hate-speech classifier. The results demonstrate that hate-speech classifiers reflect social stereotypes against marginalized groups, which can perpetuate social inequalities when propagated at scale. This framework, combining social-psychological and computational-linguistic methods, provides insights into sources of bias in hate-speech moderation, informing ongoing debates regarding machine learning fairness.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Davani2023HateSpeechClassifiers_Hate Speech Classifiers Learn Normative Social Stereotypes.pdf}
}

@inproceedings{DeChoudhury2014CharacterizingPredictingPostpartum,
  title = {Characterizing and Predicting Postpartum Depression from Shared Facebook Data},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Computer}} Supported Cooperative Work \& Social Computing},
  author = {De Choudhury, Munmun and Counts, Scott and Horvitz, Eric J. and Hoff, Aaron},
  year = {2014},
  month = feb,
  pages = {626--638},
  publisher = {{ACM}},
  address = {{Baltimore Maryland USA}},
  doi = {10.1145/2531602.2531675},
  url = {https://dl.acm.org/doi/10.1145/2531602.2531675},
  urldate = {2023-05-22},
  abstract = {The birth of a child is a major milestone in the life of parents. We leverage Facebook data shared voluntarily by 165 new mothers as streams of evidence for characterizing their postnatal experiences. We consider multiple measures including activity, social capital, emotion, and linguistic style in participants' Facebook data in pre- and postnatal periods. Our study includes detecting and predicting onset of post-partum depression (PPD). The work complements recent work on detecting and predicting significant postpartum changes in behavior, language, and affect from Twitter data. In contrast to prior studies, we gain access to ground truth on postpartum experiences via self-reports and a common psychometric instrument used to evaluate PPD. We develop a series of statistical models to predict, from data available before childbirth, a mother's likelihood of PPD. We corroborate our quantitative findings through interviews with mothers experiencing PPD. We find that increased social isolation and lowered availability of social capital on Facebook, are the best predictors of PPD in mothers.},
  isbn = {978-1-4503-2540-0},
  langid = {english},
  file = {/Users/ma/Zotero/storage/JX2CRCWN/De Choudhury et al. - 2014 - Characterizing and predicting postpartum depressio.pdf}
}

@inproceedings{DeChoudhury2014CharacterizingPredictingPostpartuma,
  title = {Characterizing and Predicting Postpartum Depression from Shared Facebook Data},
  booktitle = {Proceedings of the 17th {{ACM}} Conference on {{Computer}} Supported Cooperative Work \& Social Computing},
  author = {De Choudhury, Munmun and Counts, Scott and Horvitz, Eric J. and Hoff, Aaron},
  year = {2014},
  month = feb,
  pages = {626--638},
  publisher = {{ACM}},
  address = {{Baltimore Maryland USA}},
  doi = {10.1145/2531602.2531675},
  url = {https://dl.acm.org/doi/10.1145/2531602.2531675},
  urldate = {2023-10-17},
  isbn = {978-1-4503-2540-0},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/DeChoudhury2014CharacterizingPredictingPostpartuma_Characterizing and predicting postpartum depression from shared facebook data.pdf}
}

@article{Dias2020EmphasizingPublishersDoes,
  title = {Emphasizing Publishers Does Not Effectively Reduce Susceptibility to Misinformation on Social Media},
  author = {Dias, Nicholas and Pennycook, Gordon and Rand, David G.},
  year = {2020},
  month = jan,
  journal = {Harvard Kennedy School Misinformation Review},
  volume = {1},
  number = {1},
  doi = {10.37016/mr-2020-001},
  url = {https://misinforeview.hks.harvard.edu/article/emphasizing-publishers-does-not-reduce-misinformation/},
  urldate = {2023-10-12},
  abstract = {Survey experiments with nearly 7,000 Americans suggest that increasing the visibility of publishers is an ineffective, and perhaps even counterproductive, way to address misinformation on social media. Our findings underscore the importance of social media platforms and civil society organizations evaluating interventions experimentally rather than implementing them based on intuitive appeal. Research Question Platforms are making},
  langid = {american},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Dias2020EmphasizingPublishersDoes_Emphasizing publishers does not effectively reduce susceptibility to.pdf}
}

@inproceedings{Ding2022ExplicitRoleInteraction,
  title = {Explicit {{Role Interaction Network}} for {{Event Argument Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Ding, Nan and Hu, Chunming and Sun, Kai and Mensah, Samuel and Zhang, Richong},
  year = {2022},
  month = dec,
  pages = {3475--3485},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.254},
  urldate = {2023-02-09},
  abstract = {Event argument extraction is a challenging subtask of event extraction, aiming to identify and assign roles to arguments under a certain event. Existing methods extract arguments of each role independently, ignoring the relationship between different roles. Such an approach hinders the model from learning explicit interactions between different roles to improve the performance of individual argument extraction. As a solution, we design a neural model that we refer to as the Explicit Role Interaction Network (ERIN) which allows for dynamically capturing the correlations between different argument roles within an event. Extensive experiments on the benchmark dataset ACE2005 demonstrate the superiority of our proposed model to existing approaches.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ding2022ExplicitRoleInteraction_Explicit Role Interaction Network for Event Argument Extraction.pdf}
}

@inproceedings{Ding2023GPT3GoodData,
  title = {Is {{GPT-3}} a {{Good Data Annotator}}?},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ding, Bosheng and Qin, Chengwei and Liu, Linlin and Chia, Yew Ken and Li, Boyang and Joty, Shafiq and Bing, Lidong},
  year = {2023},
  month = jul,
  pages = {11173--11195},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.626},
  url = {https://aclanthology.org/2023.acl-long.626},
  urldate = {2023-08-15},
  abstract = {Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ding2023GPT3GoodData_Is GPT-3 a Good Data Annotator.pdf}
}

@misc{Ding2023KnowledgeCrosswordsGeometric,
  title = {Knowledge {{Crosswords}}: {{Geometric Reasoning}} over {{Structured Knowledge}} with {{Large Language Models}}},
  shorttitle = {Knowledge {{Crosswords}}},
  author = {Ding, Wenxuan and Feng, Shangbin and Liu, Yuhan and Tan, Zhaoxuan and Balachandran, Vidhisha and He, Tianxing and Tsvetkov, Yulia},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01290},
  eprint = {2310.01290},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.01290},
  url = {http://arxiv.org/abs/2310.01290},
  urldate = {2023-10-25},
  abstract = {Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We propose Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. Knowledge Crosswords contains 2,101 individual problems, covering various knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the Knowledge Crosswords benchmark. We additionally propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with hard ones, our proposed Verify-All outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/9AZIJ6WP/Ding et al. - 2023 - Knowledge Crosswords Geometric Reasoning over Str.pdf;/Users/ma/Zotero/storage/D4XMK8S3/2310.html}
}

@inproceedings{Du2020DocumentLevelEventRole,
  title = {Document-{{Level Event Role Filler Extraction}} Using {{Multi-Granularity Contextualized Encoding}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Du, Xinya and Cardie, Claire},
  year = {2020},
  month = jul,
  pages = {8010--8020},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.714},
  url = {https://aclanthology.org/2020.acl-main.714},
  urldate = {2023-02-09},
  abstract = {Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions. This is problematic when the information needed to recognize an event argument is spread across multiple sentences. We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers. We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models' performance. To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader. We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work. We also report findings on the relationship between context length and neural model performance on the task.},
  keywords = {dataset\_MUC4,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Du2020DocumentLevelEventRole_Document-Level Event Role Filler Extraction using Multi-Granularity.pdf}
}

@inproceedings{Du2020EventExtractionAnswering,
  title = {Event {{Extraction}} by {{Answering}} ({{Almost}}) {{Natural Questions}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Du, Xinya and Cardie, Claire},
  year = {2020},
  month = nov,
  pages = {671--683},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.49},
  url = {https://aclanthology.org/2020.emnlp-main.49},
  urldate = {2023-02-21},
  abstract = {The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Du2020EventExtractionAnswering_Event Extraction by Answering (Almost) Natural Questions.pdf}
}

@inproceedings{Du2021GRITGenerativeRolefiller,
  title = {{{GRIT}}: {{Generative Role-filler Transformers}} for {{Document-level Event Entity Extraction}}},
  shorttitle = {{{GRIT}}},
  booktitle = {Proceedings of the 16th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Main Volume}}},
  author = {Du, Xinya and Rush, Alexander and Cardie, Claire},
  year = {2021},
  month = apr,
  pages = {634--644},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.eacl-main.52},
  url = {https://aclanthology.org/2021.eacl-main.52},
  urldate = {2023-02-09},
  abstract = {We revisit the classic problem of document-level role-filler entity extraction (REE) for template filling. We argue that sentence-level approaches are ill-suited to the task and introduce a generative transformer-based encoder-decoder framework (GRIT) that is designed to model context at the document level: it can make extraction decisions across sentence boundaries; is implicitly aware of noun phrase coreference structure, and has the capacity to respect cross-role dependencies in the template structure. We evaluate our approach on the MUC-4 dataset, and show that our model performs substantially better than prior work. We also show that our modeling choices contribute to model performance, e.g., by implicitly capturing linguistic knowledge such as recognizing coreferent entity mentions.},
  keywords = {dataset\_MUC4,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Du2021GRITGenerativeRolefiller_GRIT.pdf}
}

@inproceedings{Du2021TemplateFillingGenerative,
  title = {Template {{Filling}} with {{Generative Transformers}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Du, Xinya and Rush, Alexander and Cardie, Claire},
  year = {2021},
  month = jun,
  pages = {909--914},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.70},
  url = {https://aclanthology.org/2021.naacl-main.70},
  urldate = {2023-02-09},
  abstract = {Template filling is generally tackled by a pipeline of two separate supervised systems \textendash{} one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Du2021TemplateFillingGenerative_Template Filling with Generative Transformers.pdf}
}

@inproceedings{Du2022DynamicGlobalMemory,
  title = {Dynamic {{Global Memory}} for {{Document-level Argument Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Xinya and Li, Sha and Ji, Heng},
  year = {2022},
  month = may,
  pages = {5264--5275},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.361},
  url = {https://aclanthology.org/2022.acl-long.361},
  urldate = {2023-02-09},
  abstract = {Extracting informative arguments of events from news articles is a challenging problem in information extraction, which requires a global contextual understanding of each document. While recent work on document-level extraction has gone beyond single-sentence and increased the cross-sentence inference capability of end-to-end models, they are still restricted by certain input sequence length constraints and usually ignore the global context between events. To tackle this issue, we introduce a new global neural generation-based framework for document-level event argument extraction by constructing a document memory store to record the contextual event information and leveraging it to implicitly and explicitly help with decoding of arguments for later events. Empirical results show that our framework outperforms prior methods substantially and it is more robust to adversarially annotated examples with our constrained decoding design.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Du2022DynamicGlobalMemory_Dynamic Global Memory for Document-level Argument Extraction.pdf}
}

@inproceedings{Du2022RetrievalAugmentedGenerativeQuestion,
  title = {Retrieval-{{Augmented Generative Question Answering}} for {{Event Argument Extraction}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Du, Xinya and Ji, Heng},
  year = {2022},
  month = dec,
  pages = {4649--4666},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.307},
  urldate = {2023-02-09},
  abstract = {Event argument extraction has long been studied as a sequential prediction problem with extractive-based methods, tackling each argument in isolation. Although recent work proposes generation-based methods to capture cross-argument dependency, they require generating and post-processing a complicated target sequence (template). Motivated by these observations and recent pretrained language models' capabilities of learning from demonstrations. We propose a retrieval-augmented generative QA model (R-GQA) for event argument extraction. It retrieves the most similar QA pair and augments it as prompt to the current example's context, then decodes the arguments as answers. Our approach outperforms substantially prior methods across various settings (i.e. fully supervised, domain transfer, and fewshot learning). Finally, we propose a clustering-based sampling strategy (JointEnc) and conduct a thorough analysis of how different strategies influence the few-shot learning performances.},
  keywords = {sentence-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Du2022RetrievalAugmentedGenerativeQuestion_Retrieval-Augmented Generative Question Answering for Event Argument Extraction.pdf}
}

@misc{Dunn2022StructuredInformationExtraction,
  title = {Structured Information Extraction from Complex Scientific Text with Fine-Tuned Large Language Models},
  author = {Dunn, Alexander and Dagdelen, John and Walker, Nicholas and Lee, Sanghoon and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin and Jain, Anubhav},
  year = {2022},
  month = dec,
  number = {arXiv:2212.05238},
  eprint = {2212.05238},
  primaryclass = {cond-mat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.05238},
  url = {http://arxiv.org/abs/2212.05238},
  urldate = {2023-03-02},
  abstract = {Intelligently extracting and linking complex scientific information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientific text. The approach leverages a pre-trained large language model (LLM), GPT-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of JSON objects. We demonstrate that LLMs trained in this way are capable of accurately extracting useful records of complex scientific knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-flexible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Condensed Matter - Materials Science,I.7.m},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Dunn2022StructuredInformationExtraction_Structured information extraction from complex scientific text with fine-tuned.pdf;/Users/ma/Zotero/storage/JTHZ9H2L/2212.html}
}

@misc{Ebert2023ComparingTrajectoryVision,
  title = {Comparing {{Trajectory}} and {{Vision Modalities}} for {{Verb Representation}}},
  author = {Ebert, Dylan and Sun, Chen and Pavlick, Ellie},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12737},
  eprint = {2303.12737},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.12737},
  urldate = {2023-04-27},
  abstract = {Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation learning for language.},
  archiveprefix = {arxiv},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ebert2023ComparingTrajectoryVision_Comparing Trajectory and Vision Modalities for Verb Representation.pdf;/Users/ma/Zotero/storage/IH8AIKST/2303.html}
}

@misc{Ebert2023ComparingTrajectoryVisiona,
  title = {Comparing {{Trajectory}} and {{Vision Modalities}} for {{Verb Representation}}},
  author = {Ebert, Dylan and Sun, Chen and Pavlick, Ellie},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12737},
  eprint = {2303.12737},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.12737},
  urldate = {2023-05-12},
  abstract = {Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation learning for language.},
  archiveprefix = {arxiv},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ebert2023ComparingTrajectoryVisiona_Comparing Trajectory and Vision Modalities for Verb Representation.pdf;/Users/ma/Zotero/storage/99YEGI6D/2303.html}
}

@inproceedings{Ebner2020MultiSentenceArgumentLinking,
  title = {Multi-{{Sentence Argument Linking}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ebner, Seth and Xia, Patrick and Culkin, Ryan and Rawlins, Kyle and Van Durme, Benjamin},
  year = {2020},
  month = jul,
  pages = {8057--8077},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.718},
  url = {https://aclanthology.org/2020.acl-main.718},
  urldate = {2023-02-09},
  abstract = {We present a novel document-level model for finding argument spans that fill an event's roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution. Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types. We demonstrate strong performance of our model on RAMS and other event-related datasets.},
  keywords = {dataset proposal,dataset\_RAMS,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ebner2020MultiSentenceArgumentLinking_Multi-Sentence Argument Linking.pdf}
}

@article{Ecker2022PsychologicalDriversMisinformation,
  title = {The Psychological Drivers of Misinformation Belief and Its Resistance to Correction},
  author = {Ecker, Ullrich K. H. and Lewandowsky, Stephan and Cook, John and Schmid, Philipp and Fazio, Lisa K. and Brashier, Nadia and Kendeou, Panayiota and Vraga, Emily K. and Amazeen, Michelle A.},
  year = {2022},
  month = jan,
  journal = {Nature Reviews Psychology},
  volume = {1},
  number = {1},
  pages = {13--29},
  publisher = {{Nature Publishing Group}},
  issn = {2731-0574},
  doi = {10.1038/s44159-021-00006-y},
  url = {https://www.nature.com/articles/s44159-021-00006-y},
  urldate = {2023-10-13},
  abstract = {Misinformation has been identified as a major contributor to various contentious contemporary events ranging from elections and referenda to the response to the COVID-19 pandemic. Not only can belief in misinformation lead to poor judgements and decision-making, it also exerts a lingering influence on people's reasoning after it has been corrected \textemdash{} an effect known as the continued influence effect. In this Review, we describe the cognitive, social and affective factors that lead people to form or endorse misinformed views, and the psychological barriers to knowledge revision after misinformation has been corrected, including theories of continued influence. We discuss the effectiveness of both pre-emptive (`prebunking') and reactive (`debunking') interventions to reduce the effects of misinformation, as well as implications for information consumers and practitioners in various areas including journalism, public health, policymaking and education.},
  copyright = {2022 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Communication,Policy,Psychology,Social behaviour,Technology},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ecker2022PsychologicalDriversMisinformation_The psychological drivers of misinformation belief and its resistance to.pdf}
}

@inproceedings{Edwards2021Text2MolCrossModalMolecule,
  title = {{{Text2Mol}}: {{Cross-Modal Molecule Retrieval}} with {{Natural Language Queries}}},
  shorttitle = {{{Text2Mol}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Edwards, Carl and Zhai, ChengXiang and Ji, Heng},
  year = {2021},
  month = nov,
  pages = {595--607},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.47},
  url = {https://aclanthology.org/2021.emnlp-main.47},
  urldate = {2023-10-26},
  abstract = {We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Edwards2021Text2MolCrossModalMolecule_Text2Mol.pdf}
}

@inproceedings{Edwards2022TranslationMoleculesNatural,
  title = {Translation between {{Molecules}} and {{Natural Language}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Edwards, Carl and Lai, Tuan and Ros, Kevin and Honke, Garrett and Cho, Kyunghyun and Ji, Heng},
  year = {2022},
  month = dec,
  pages = {375--413},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  doi = {10.18653/v1/2022.emnlp-main.26},
  url = {https://aclanthology.org/2022.emnlp-main.26},
  urldate = {2023-10-26},
  abstract = {We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Edwards2022TranslationMoleculesNatural_Translation between Molecules and Natural Language.pdf}
}

@misc{Egami2023UsingLargeLanguage,
  title = {Using {{Large Language Model Annotations}} for {{Valid Downstream Statistical Inference}} in {{Social Science}}: {{Design-Based Semi-Supervised Learning}}},
  shorttitle = {Using {{Large Language Model Annotations}} for {{Valid Downstream Statistical Inference}} in {{Social Science}}},
  author = {Egami, Naoki and {Jacobs-Harukawa}, Musashi and Stewart, Brandon M. and Wei, Hanying},
  year = {2023},
  month = jun,
  number = {arXiv:2306.04746},
  eprint = {2306.04746},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.04746},
  url = {http://arxiv.org/abs/2306.04746},
  urldate = {2023-10-25},
  abstract = {In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. The recent advancements in large language models (LLMs) can lower costs for CSS research by annotating documents cheaply at scale, but such surrogate labels are often imperfect and biased. We present a new algorithm for using outputs from LLMs for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of LLM-predicted surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80--90\textbackslash\%. To address this, we build on debiased machine learning to propose the design-based semi-supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased, without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without statistical guarantees.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Egami2023UsingLargeLanguage_Using Large Language Model Annotations for Valid Downstream Statistical.pdf;/Users/ma/Zotero/storage/XCTJPTSV/2306.html}
}

@misc{Eirew2022CrossdocumentEventCoreference,
  title = {Cross-Document {{Event Coreference Search}}: {{Task}}, {{Dataset}} and {{Modeling}}},
  shorttitle = {Cross-Document {{Event Coreference Search}}},
  author = {Eirew, Alon and Caciularu, Avi and Dagan, Ido},
  year = {2022},
  month = oct,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2210.12654},
  url = {https://arxiv.org/abs/2210.12654v1},
  urldate = {2023-02-03},
  abstract = {The task of Cross-document Coreference Resolution has been traditionally formulated as requiring to identify all coreference links across a given set of documents. We propose an appealing, and often more applicable, complementary set up for the task - Cross-document Coreference Search, focusing in this paper on event coreference. Concretely, given a mention in context of an event of interest, considered as a query, the task is to find all coreferring mentions for the query event in a large document collection. To support research on this task, we create a corresponding dataset, which is derived from Wikipedia while leveraging annotations in the available Wikipedia Event Coreference dataset (WEC-Eng). Observing that the coreference search setup is largely analogous to the setting of Open Domain Question Answering, we adapt the prominent Deep Passage Retrieval (DPR) model to our setting, as an appealing baseline. Finally, we present a novel model that integrates a powerful coreference scoring scheme into the DPR architecture, yielding improved performance.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Eirew2022CrossdocumentEventCoreference_Cross-document Event Coreference Search.pdf}
}

@article{Escola-Gascon2021CriticalThinkingPredicts,
  title = {Critical Thinking Predicts Reductions in {{Spanish}} Physicians' Stress Levels and Promotes Fake News Detection},
  author = {{Escol{\`a}-Gasc{\'o}n}, {\'A}lex and Dagnall, Neil and Gallifa, Josep},
  year = {2021},
  month = dec,
  journal = {Thinking Skills and Creativity},
  volume = {42},
  pages = {100934},
  issn = {1871-1871},
  doi = {10.1016/j.tsc.2021.100934},
  url = {https://www.sciencedirect.com/science/article/pii/S1871187121001498},
  urldate = {2023-10-13},
  abstract = {The prevalence of pseudoscientific beliefs and fake news increased during the coronavirus crisis. Misinformation streams such as these potentially pose risks to people's health. Thus, knowing how these pseudoscientific beliefs and fake news impact the community of internists may be useful for improving primary care services. In this research, analyses of stress levels, effectiveness in detecting fake news, use of critical thinking (CP), and attitudes toward pseudosciences in internists during the COVID-19 crisis were performed. A total of 1129 internists participated. Several multiple regression models were applied using the forward stepwise method to determine the weight of CP and physicians' attitudes toward pseudosciences in predicting reductions in stress levels and facilitating the detection of fake news. The use of critical thinking predicted 46.9\% of the reduction in stress levels. Similarly, skeptical attitudes and critical thinking predicted 56.1\% of the hits on fake news detection tests. The stress levels of physicians during the coronavirus pandemic were clinically significant. The efficacy of fake news detection increases by 30.7\% if the individual was a physician. Study outcomes indicate that the use of critical thinking and skeptical attitudes reduce stress levels and allow better detection of fake news. The importance of how to promote critical and skeptical attitudes in the field of medicine is discussed.},
  keywords = {Coronavirus,Critical thinking,Fake news,Internal Medicine,Pseudosciences,Stress levels},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Escola-Gascon2021CriticalThinkingPredicts_Critical thinking predicts reductions in Spanish physicians' stress levels and.pdf}
}

@misc{Fernandes2023BridgingGapSurvey,
  title = {Bridging the {{Gap}}: {{A Survey}} on {{Integrating}} ({{Human}}) {{Feedback}} for {{Natural Language Generation}}},
  shorttitle = {Bridging the {{Gap}}},
  author = {Fernandes, Patrick and Madaan, Aman and Liu, Emmy and Farinhas, Ant{\'o}nio and Martins, Pedro Henrique and Bertsch, Amanda and {de Souza}, Jos{\'e} G. C. and Zhou, Shuyan and Wu, Tongshuang and Neubig, Graham and Martins, Andr{\'e} F. T.},
  year = {2023},
  month = may,
  number = {arXiv:2305.00955},
  eprint = {2305.00955},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.00955},
  url = {http://arxiv.org/abs/2305.00955},
  urldate = {2023-05-30},
  abstract = {Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Fernandes2023BridgingGapSurvey_Bridging the Gap.pdf;/Users/ma/Zotero/storage/RSIKVX59/2305.html}
}

@misc{Fleming2023MedAlignClinicianGeneratedDataset,
  title = {{{MedAlign}}: {{A Clinician-Generated Dataset}} for {{Instruction Following}} with {{Electronic Medical Records}}},
  shorttitle = {{{MedAlign}}},
  author = {Fleming, Scott L. and Lozano, Alejandro and Haberkorn, William J. and Jindal, Jenelle A. and Reis, Eduardo P. and Thapa, Rahul and Blankemeier, Louis and Genkins, Julian Z. and Steinberg, Ethan and Nayak, Ashwin and Patel, Birju S. and Chiang, Chia-Chun and Callahan, Alison and Huo, Zepeng and Gatidis, Sergios and Adams, Scott J. and Fayanju, Oluseyi and Shah, Shreya J. and Savage, Thomas and Goh, Ethan and Chaudhari, Akshay S. and Aghaeepour, Nima and Sharp, Christopher and Pfeffer, Michael A. and Liang, Percy and Chen, Jonathan H. and Morse, Keith E. and Brunskill, Emma P. and Fries, Jason A. and Shah, Nigam H.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14089},
  eprint = {2308.14089},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.14089},
  url = {http://arxiv.org/abs/2308.14089},
  urldate = {2023-10-25},
  abstract = {The ability of large language models (LLMs) to follow natural language instructions with human-level fluency suggests many opportunities in healthcare to reduce administrative burden and improve quality of care. However, evaluating LLMs on realistic text generation tasks for healthcare remains challenging. Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians. To address these challenges, we introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for 303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality of each LLM response. We found high error rates, ranging from 35\% (GPT-4) to 68\% (MPT-7B-Instruct), and an 8.3\% drop in accuracy moving from 32k to 2k context lengths for GPT-4. Finally, we report correlations between clinician rankings and automated natural language generation metrics as a way to rank LLMs without human review. We make MedAlign available under a research data use agreement to enable LLM evaluations on tasks aligned with clinician needs and preferences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Fleming2023MedAlignClinicianGeneratedDataset_MedAlign.pdf;/Users/ma/Zotero/storage/FNFMNH4J/2308.html}
}

@article{Foster2012RepetitionNotNumber,
  title = {Repetition, Not Number of Sources, Increases Both Susceptibility to Misinformation and Confidence in the Accuracy of Eyewitnesses},
  author = {Foster, Jeffrey L. and Huthwaite, Thomas and Yesberg, Julia A. and Garry, Maryanne and Loftus, Elizabeth F.},
  year = {2012},
  month = feb,
  journal = {Acta Psychologica},
  volume = {139},
  number = {2},
  pages = {320--326},
  issn = {0001-6918},
  doi = {10.1016/j.actpsy.2011.12.004},
  url = {https://www.sciencedirect.com/science/article/pii/S000169181100223X},
  urldate = {2023-10-12},
  abstract = {Are claims more credible when made by multiple sources, or is it the repetition of claims that matters? Some research suggests that claims have more credibility when independent sources make them. Yet, other research suggests that simply repeating information makes it more accessible and encourages reliance on automatic processes\textemdash factors known to change people's judgments. In Experiment 1, people took part in a ``misinformation'' study: people first watched a video of a crime and later read eyewitness reports attributed to one or three different eyewitnesses who made misleading claims in either one report or repeated the same misleading claims across all three reports. In Experiment 2, people who had not seen any videos read those same reports and indicated how confident they were that each claim happened in the original event. People were more misled by\textemdash and more confident about\textemdash claims that were repeated, regardless of how many eyewitnesses made them. We hypothesize that people interpreted the familiarity of repeated claims as markers of accuracy. These findings fit with research showing that repeating information makes it seem more true, and highlight the power of a single repeated voice.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Foster2012RepetitionNotNumber_Repetition, not number of sources, increases both susceptibility to.pdf}
}

@article{Frazer2021DiseaseVariantPrediction,
  title = {Disease Variant Prediction with Deep Generative Models of Evolutionary Data},
  author = {Frazer, Jonathan and Notin, Pascal and Dias, Mafalda and Gomez, Aidan and Min, Joseph K. and Brock, Kelly and Gal, Yarin and Marks, Debora S.},
  year = {2021},
  month = nov,
  journal = {Nature},
  volume = {599},
  number = {7883},
  pages = {91--95},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04043-8},
  url = {https://www.nature.com/articles/s41586-021-04043-8},
  urldate = {2023-11-02},
  abstract = {Quantifying the pathogenicity of protein variants in human disease-related genes would have a marked effect on clinical decisions, yet the overwhelming majority (over 98\%) of these variants still have unknown consequences1\textendash 3. In principle, computational methods could support the large-scale interpretation of genetic variants. However, state-of-the-art methods4\textendash 10 have relied on training machine learning models on known disease labels. As these labels are sparse, biased and of variable quality, the resulting models have been considered insufficiently reliable11. Here we propose an approach that leverages deep generative models to predict variant pathogenicity without relying on labels. By modelling the distribution of sequence variation across organisms, we implicitly capture constraints on the protein sequences that maintain fitness. Our model EVE (evolutionary model of variant effect) not only outperforms computational approaches that rely on labelled data but also performs on par with, if not better than, predictions from high-throughput experiments, which are increasingly used as evidence for variant classification12\textendash 16. We predict the pathogenicity of more than 36~million variants across 3,219 disease genes and provide evidence for the classification of more than 256,000 variants of unknown~significance. Our work suggests that models of evolutionary information can provide valuable independent evidence for variant interpretation that will be widely useful in research and clinical settings.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational models,Disease genetics,Genetic variation,Genetics research,Machine learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Frazer2021DiseaseVariantPrediction_Disease variant prediction with deep generative models of evolutionary data.pdf}
}

@misc{Fung2022NormSAGEMultiLingualMultiCultural,
  title = {{{NormSAGE}}: {{Multi-Lingual Multi-Cultural Norm Discovery}} from {{Conversations On-the-Fly}}},
  shorttitle = {{{NormSAGE}}},
  author = {Fung, Yi R. and Chakraborty, Tuhin and Guo, Hao and Rambow, Owen and Muresan, Smaranda and Ji, Heng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08604},
  eprint = {2210.08604},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.08604},
  urldate = {2023-10-26},
  abstract = {Norm discovery is important for understanding and reasoning about the acceptable behaviors and potential violations in human communication and interactions. We introduce NormSage, a framework for addressing the novel task of conversation-grounded multi-lingual, multi-cultural norm discovery, based on language model prompting and self-verification. NormSAGE leverages the expressiveness and implicit knowledge of the pretrained GPT-3 language model backbone, to elicit knowledge about norms through directed questions representing the norm discovery task and conversation context. It further addresses the risk of language model hallucination with a self-verification mechanism ensuring that the norms discovered are correct and are substantially grounded to their source conversations. Evaluation results show that our approach discovers significantly more relevant and insightful norms for conversations on-the-fly compared to baselines ({$>$}10+\% in Likert scale rating). The norms discovered from Chinese conversation are also comparable to the norms discovered from English conversation in terms of insightfulness and correctness ({$<$}3\% difference). In addition, the culture-specific norms are promising quality, allowing for 80\% accuracy in culture pair human identification. Finally, our grounding process in norm discovery self-verification can be extended for instantiating the adherence and violation of any norm for a given conversation on-the-fly, with explainability and transparency. NormSAGE achieves an AUC of 95.4\% in grounding, with natural language explanation matching human-written quality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Fung2022NormSAGEMultiLingualMultiCultural_NormSAGE.pdf;/Users/ma/Zotero/storage/SXFT3QT5/2210.html}
}

@misc{Gabriel2022MisinfoReactionFrames,
  title = {Misinfo {{Reaction Frames}}: {{Reasoning}} about {{Readers}}' {{Reactions}} to {{News Headlines}}},
  shorttitle = {Misinfo {{Reaction Frames}}},
  author = {Gabriel, Saadia and Hallinan, Skyler and Sap, Maarten and Nguyen, Pemi and Roesner, Franziska and Choi, Eunsol and Choi, Yejin},
  year = {2022},
  month = mar,
  number = {arXiv:2104.08790},
  eprint = {2104.08790},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08790},
  url = {http://arxiv.org/abs/2104.08790},
  urldate = {2023-04-13},
  abstract = {Even to a simple and short news headline, readers react in a multitude of ways: cognitively (e.g. inferring the writer's intent), emotionally (e.g. feeling distrust), and behaviorally (e.g. sharing the news with their friends). Such reactions are instantaneous and yet complex, as they rely on factors that go beyond interpreting factual content of news. We propose Misinfo Reaction Frames (MRF), a pragmatic formalism for modeling how readers might react to a news headline. In contrast to categorical schema, our free-text dimensions provide a more nuanced way of understanding intent beyond being benign or malicious. We also introduce a Misinfo Reaction Frames corpus, a crowdsourced dataset of reactions to over 25k news headlines focusing on global crises: the Covid-19 pandemic, climate change, and cancer. Empirical results confirm that it is indeed possible for neural models to predict the prominent patterns of readers' reactions to previously unseen news headlines. Additionally, our user study shows that displaying machine-generated MRF implications alongside news headlines to readers can increase their trust in real news while decreasing their trust in misinformation. Our work demonstrates the feasibility and importance of pragmatic inferences on news headlines to help enhance AI-guided misinformation detection and mitigation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gabriel2022MisinfoReactionFrames_Misinfo Reaction Frames.pdf;/Users/ma/Zotero/storage/4F8SVEXD/2104.html}
}

@misc{Ganguli2022RedTeamingLanguage,
  title = {Red {{Teaming Language Models}} to {{Reduce Harms}}: {{Methods}}, {{Scaling Behaviors}}, and {{Lessons Learned}}},
  shorttitle = {Red {{Teaming Language Models}} to {{Reduce Harms}}},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and {El-Showk}, Sheer and Fort, Stanislav and {Hatfield-Dodds}, Zac and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and {Tran-Johnson}, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
  year = {2022},
  month = nov,
  number = {arXiv:2209.07858},
  eprint = {2209.07858},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.07858},
  urldate = {2023-05-16},
  abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ganguli2022RedTeamingLanguage_Red Teaming Language Models to Reduce Harms.pdf;/Users/ma/Zotero/storage/ZEF53YLU/2209.html}
}

@article{GanguliRedTeamingLanguage,
  title = {Red {{Teaming Language Models}} to {{Reduce Harms}}: {{Methods}}, {{Scaling Behaviors}}, and {{Lessons Learned}}},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and {El-Showk}, Sheer and Fort, Stanislav and {Hatfield-Dodds}, Zac and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and {Tran-Johnson}, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
  abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. Warning: this paper contains examples that may be offensive or upsetting.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/SZTLJGVW/Ganguli et al. - Red Teaming Language Models to Reduce Harms Metho.pdf}
}

@article{Gao2020DrAgentClinical,
  title = {Dr. {{Agent}}: {{Clinical}} Predictive Model via Mimicked Second Opinions},
  shorttitle = {Dr. {{Agent}}},
  author = {Gao, Junyi and Xiao, Cao and Glass, Lucas M and Sun, Jimeng},
  year = {2020},
  month = jun,
  journal = {Journal of the American Medical Informatics Association : JAMIA},
  volume = {27},
  number = {7},
  pages = {1084--1091},
  issn = {1067-5027},
  doi = {10.1093/jamia/ocaa074},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7647368/},
  urldate = {2023-11-02},
  abstract = {Objective Prediction of disease phenotypes and their outcomes is a difficult task. In practice, patients routinely seek second opinions from multiple clinical experts for complex disease diagnosis. Our objective is to mimic such a practice of seeking second opinions by training 2 agents with different focuses: the primary agent studies the most recent visit of the patient to learn the current health status, and then the second-opinion agent considers the entire patient history to obtain a more global view. Materials and Methods Our approach Dr. Agent augments recurrent neural networks with 2 policy gradient agents. Moreover, Dr. Agent is customized with various patient demographics information and learns a dynamic skip connection to focus on the relevant information over time. We trained Dr. Agent to perform 4 clinical prediction tasks on the publicly available MIMIC-III (Medical Information Mart for Intensive Care) database: (1) in-hospital mortality prediction, (2) acute care phenotype classification, (3) physiologic decompensation prediction, and (4) forecasting length of stay. We compared the performance of Dr. Agent against 4 baseline clinical predictive models. Results Dr. Agent outperforms baseline clinical prediction models across all 4 tasks in terms of all metrics. Compared with the best baseline model, Dr. Agent achieves up to 15\% higher area under the precision-recall curve on different tasks. Conclusions Dr. Agent can comprehensively model the long-term dependencies of patients' health status while considering patients' demographics using 2 agents, and therefore achieves better prediction performance on different clinical prediction tasks.},
  pmcid = {PMC7647368},
  pmid = {32548622},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gao2020DrAgentClinical_Dr.pdf}
}

@inproceedings{Gao2021MakingPretrainedLanguage,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  year = {2021},
  month = aug,
  pages = {3816--3830},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.295},
  url = {https://aclanthology.org/2021.acl-long.295},
  urldate = {2023-03-21},
  abstract = {The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\textemdash better few-shot fine-tuning of language models\textemdash a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30\% absolute improvement, and 11\% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gao2021MakingPretrainedLanguage_Making Pre-trained Language Models Better Few-shot Learners.pdf}
}

@inproceedings{Gao2022MaskthenFillFlexibleEffective,
  title = {Mask-Then-{{Fill}}: {{A Flexible}} and {{Effective Data Augmentation Framework}} for {{Event Extraction}}},
  shorttitle = {Mask-Then-{{Fill}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Gao, Jun and Yu, Changlong and Wang, Wei and Zhao, Huan and Xu, Ruifeng},
  year = {2022},
  month = dec,
  pages = {4537--4544},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.332},
  urldate = {2023-02-09},
  abstract = {We present Mask-then-Fill, a flexible and effective data augmentation framework for event extraction. Our approach allows for more flexible manipulation of text and thus can generate more diverse data while keeping the original event structure unchanged as much as possible. Specifically, it first randomly masks out an adjunct sentence fragment and then infills a variable-length text span with a fine-tuned infilling model. The main advantage lies in that it can replace a fragment of arbitrary length in the text with another fragment of variable length, compared to the existing methods which can only replace a single word or a fixed-length fragment. On trigger and argument extraction tasks, the proposed framework is more effective than baseline methods and it demonstrates particularly strong results in the low-resource setting. Our further analysis shows that it achieves a good balance between diversity and distributional similarity.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gao2022MaskthenFillFlexibleEffective_Mask-then-Fill.pdf}
}

@misc{Gao2023EnablingLargeLanguage,
  title = {Enabling {{Large Language Models}} to {{Generate Text}} with {{Citations}}},
  author = {Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi},
  year = {2023},
  month = may,
  number = {arXiv:2305.14627},
  eprint = {2305.14627},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14627},
  url = {http://arxiv.org/abs/2305.14627},
  urldate = {2023-10-26},
  abstract = {Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, we aim to enable LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare with different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We build automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvements -- for example, on the ELI5 dataset, even the best model has 49\% of its generations lacking complete citation support. Our extensive analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gao2023EnablingLargeLanguage_Enabling Large Language Models to Generate Text with Citations.pdf;/Users/ma/Zotero/storage/IC32I5TB/2305.html}
}

@misc{Gao2023ExploringFeasibilityChatGPT,
  title = {Exploring the {{Feasibility}} of {{ChatGPT}} for {{Event Extraction}}},
  author = {Gao, Jun and Zhao, Huan and Yu, Changlong and Xu, Ruifeng},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03836},
  eprint = {2303.03836},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.03836},
  urldate = {2023-03-08},
  abstract = {Event extraction is a fundamental task in natural language processing that involves identifying and extracting information about events mentioned in text. However, it is a challenging task due to the lack of annotated data, which is expensive and time-consuming to obtain. The emergence of large language models (LLMs) such as ChatGPT provides an opportunity to solve language tasks with simple prompts without the need for task-specific datasets and fine-tuning. While ChatGPT has demonstrated impressive results in tasks like machine translation, text summarization, and question answering, it presents challenges when used for complex tasks like event extraction. Unlike other tasks, event extraction requires the model to be provided with a complex set of instructions defining all event types and their schemas. To explore the feasibility of ChatGPT for event extraction and the challenges it poses, we conducted a series of experiments. Our results show that ChatGPT has, on average, only 51.04\% of the performance of a task-specific model such as EEQA in long-tail and complex scenarios. Our usability testing experiments indicate that ChatGPT is not robust enough, and continuous refinement of the prompt does not lead to stable performance improvements, which can result in a poor user experience. Besides, ChatGPT is highly sensitive to different prompt styles.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Event Extraction,LLM IE},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gao2023ExploringFeasibilityChatGPT_Exploring the Feasibility of ChatGPT for Event Extraction.pdf;/Users/ma/Zotero/storage/8WT2A2VY/2303.html}
}

@misc{Gao2023SelfGuidedNoiseFreeData,
  title = {Self-{{Guided Noise-Free Data Generation}} for {{Efficient Zero-Shot Learning}}},
  author = {Gao, Jiahui and Pi, Renjie and Lin, Yong and Xu, Hang and Ye, Jiacheng and Wu, Zhiyong and Zhang, Weizhong and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng},
  year = {2023},
  month = feb,
  number = {arXiv:2205.12679},
  eprint = {2205.12679},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.12679},
  urldate = {2023-03-21},
  abstract = {There is a rising interest in further exploring the zero-shot learning potential of large pre-trained language models (PLMs). A new paradigm called data-generation-based zero-shot learning has achieved impressive success. In this paradigm, the synthesized data from the PLM acts as the carrier of knowledge, which is used to train a task-specific model with orders of magnitude fewer parameters than the PLM, achieving both higher performance and efficiency than prompt-based zero-shot learning methods on PLMs. The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples. Fitting on such data will greatly hamper the performance of the task-specific model, making it unreliable for deployment. Previous methods remedy this issue mainly by filtering synthetic data using heuristic metrics(e.g., output confidence), or refining the data with the help of a human expert, which comes with excessive manual tuning or expensive costs. In this paper, we propose a novel noise-robust re-weighting framework SunGen to automatically construct high-quality data for zero-shot classification problems. Our framework features the ability to learn the sample weights indicating data quality without requiring any human annotation. We theoretically and empirically verify the ability of our method to help construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8\% relative improvement than the baseline on average accuracy across eight different established text classification tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gao2023SelfGuidedNoiseFreeData_Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning.pdf;/Users/ma/Zotero/storage/Z8GGJI87/2205.html}
}

@inproceedings{Giorgi2022SequencetosequenceApproachDocumentlevel,
  title = {A Sequence-to-Sequence Approach for Document-Level Relation Extraction},
  booktitle = {Proceedings of the 21st {{Workshop}} on {{Biomedical Language Processing}}},
  author = {Giorgi, John and Bader, Gary and Wang, Bo},
  year = {2022},
  month = may,
  pages = {10--25},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.bionlp-1.2},
  url = {https://aclanthology.org/2022.bionlp-1.2},
  urldate = {2023-05-01},
  abstract = {Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at https://github.com/johngiorgi/seq2rel. An online demo is available at https://share.streamlit.io/johngiorgi/seq2rel/main/demo.py.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Giorgi2022SequencetosequenceApproachDocumentlevel_A sequence-to-sequence approach for document-level relation extraction.pdf}
}

@inproceedings{Glandt2021StanceDetectionCOVID19,
  title = {Stance {{Detection}} in {{COVID-19 Tweets}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Glandt, Kyle and Khanal, Sarthak and Li, Yingjie and Caragea, Doina and Caragea, Cornelia},
  year = {2021},
  month = aug,
  pages = {1596--1611},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.127},
  url = {https://aclanthology.org/2021.acl-long.127},
  urldate = {2023-04-13},
  abstract = {The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g. stay at home mandates and wearing face masks when out in public. We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic. We annotate a new stance detection dataset, called COVID-19-Stance. Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task. To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets. The dataset, code, and other resources are available on GitHub.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Glandt2021StanceDetectionCOVID19_Stance Detection in COVID-19 Tweets.pdf}
}

@inproceedings{Gong2017PredictingClinicalOutcomes,
  title = {Predicting {{Clinical Outcomes Across Changing Electronic Health Record Systems}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Gong, Jen J. and Naumann, Tristan and Szolovits, Peter and Guttag, John V.},
  year = {2017},
  month = aug,
  pages = {1497--1505},
  publisher = {{ACM}},
  address = {{Halifax NS Canada}},
  doi = {10.1145/3097983.3098064},
  url = {https://dl.acm.org/doi/10.1145/3097983.3098064},
  urldate = {2023-11-02},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gong2017PredictingClinicalOutcomes_Predicting Clinical Outcomes Across Changing Electronic Health Record Systems.pdf}
}

@misc{Gong2022DiffuSeqSequenceSequence,
  title = {{{DiffuSeq}}: {{Sequence}} to {{Sequence Text Generation}} with {{Diffusion Models}}},
  shorttitle = {{{DiffuSeq}}},
  author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08933},
  eprint = {2210.08933},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.08933},
  url = {http://arxiv.org/abs/2210.08933},
  urldate = {2023-02-09},
  abstract = {Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is difficult due to the discrete nature of text. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gong2022DiffuSeqSequenceSequence_DiffuSeq.pdf;/Users/ma/Zotero/storage/UW288BZH/2210.html}
}

@misc{Goyal2023ThinkYouSpeak,
  title = {Think before You Speak: {{Training Language Models With Pause Tokens}}},
  shorttitle = {Think before You Speak},
  author = {Goyal, Sachin and Ji, Ziwei and Rawat, Ankit Singh and Menon, Aditya Krishna and Kumar, Sanjiv and Nagarajan, Vaishnavh},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02226},
  eprint = {2310.02226},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.02226},
  url = {http://arxiv.org/abs/2310.02226},
  urldate = {2023-10-05},
  abstract = {Language models generate responses by producing a series of tokens in immediate succession: the \$(K+1)\^\{th\}\$ token is an outcome of manipulating \$K\$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, \$K+10\$ hidden vectors, before it outputs the \$(K+1)\^\{th\}\$ token? We operationalize this idea by performing training and inference on language models with a (learnable) \$\textbackslash textit\{pause\}\$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate \$\textbackslash textit\{pause-training\}\$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of \$18\textbackslash\%\$ EM score on the QA task of SQuAD, \$8\textbackslash\%\$ on CommonSenseQA and \$1\textbackslash\%\$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Goyal2023ThinkYouSpeak_Think before you speak.pdf;/Users/ma/Zotero/storage/S4RZAQGE/2310.html}
}

@article{Greene2020IndividualDifferencesSusceptibility,
  title = {Individual Differences in Susceptibility to False Memories for {{COVID-19}} Fake News},
  author = {Greene, Ciara M. and Murphy, Gillian},
  year = {2020},
  month = dec,
  journal = {Cognitive Research: Principles and Implications},
  volume = {5},
  number = {1},
  pages = {63},
  issn = {2365-7464},
  doi = {10.1186/s41235-020-00262-1},
  abstract = {Exposure to 'fake news' can result in false memories, with possible consequences for downstream behaviour. Given the sharp rise in online misinformation during the coronavirus pandemic, it is important to understand the factors that influence the development of false memories. The present study measured susceptibility to false memories following exposure to fabricated news stories about the pandemic in a sample of 3746 participants. We investigated the effect of individual differences in (1) knowledge about COVID-19, (2) engagement with media or discussion about the coronavirus, (3) anxiety about COVID-19 and (4) analytical reasoning. Notably, objectively and subjectively assessed knowledge about COVID-19 were not significantly correlated. Objectively assessed knowledge was associated with fewer false memories but more true memories, suggesting a true discrimination between true and fake news. In contrast, participants who merely believed themselves to be very knowledgeable were more likely to report a memory for true stories, but showed no reduction in false memories. Similarly, individuals who reported high levels of media engagement or anxiety about COVID-19 reported an increase in true (but not false) memories. Finally, higher levels of analytical reasoning were associated with fewer memories for both true and fabricated stories, suggesting a stricter threshold for reporting a memory for any story. These data indicate that false memories can form in response to fake COVID-19 news and that susceptibility to this misinformation is affected by the individual's knowledge about and interaction with COVID-19 information, as well as their tendency to think critically.},
  langid = {english},
  pmcid = {PMC7716111},
  pmid = {33275199},
  keywords = {Adolescent,Adult,Aged,{Aged, 80 and over},Communication,Communications Media,COVID-19,Female,Humans,Individuality,Ireland,Knowledge,Male,Mass Media,Middle Aged,Pandemics,{Repression, Psychology},Social Media,Young Adult},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Greene2020IndividualDifferencesSusceptibility_Individual differences in susceptibility to false memories for COVID-19 fake.pdf}
}

@article{Griffiths2004FindingScientificTopics,
  title = {Finding Scientific Topics},
  author = {Griffiths, Thomas L. and Steyvers, Mark},
  year = {2004},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {101},
  number = {suppl\_1},
  pages = {5228--5235},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.0307752101},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.0307752101},
  urldate = {2023-10-26},
  abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Griffiths2004FindingScientificTopics_Finding scientific topics.pdf}
}

@misc{Grosse2023StudyingLargeLanguage,
  title = {Studying {{Large Language Model Generalization}} with {{Influence Functions}}},
  author = {Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez, Ethan and Hubinger, Evan and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared and Bowman, Samuel R.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03296},
  eprint = {2308.03296},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2308.03296},
  urldate = {2023-10-05},
  abstract = {When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Grosse2023StudyingLargeLanguage_Studying Large Language Model Generalization with Influence Functions.pdf;/Users/ma/Zotero/storage/HBUV8KIY/2308.html}
}

@article{Grossmann2023AITransformationSocial,
  title = {{{AI}} and the Transformation of Social Science Research},
  author = {Grossmann, Igor and Feinberg, Matthew and Parker, Dawn C. and Christakis, Nicholas A. and Tetlock, Philip E. and Cunningham, William A.},
  year = {2023},
  month = jun,
  journal = {Science},
  volume = {380},
  number = {6650},
  pages = {1108--1109},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.adi1778},
  url = {https://www.science.org/doi/10.1126/science.adi1778},
  urldate = {2023-07-26}
}

@article{Grossmann2023AITransformationSociala,
  title = {{{AI}} and the Transformation of Social Science Research},
  author = {Grossmann, Igor and Feinberg, Matthew and Parker, Dawn C. and Christakis, Nicholas A. and Tetlock, Philip E. and Cunningham, William A.},
  year = {2023},
  month = jun,
  journal = {Science},
  volume = {380},
  number = {6650},
  pages = {1108--1109},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.adi1778},
  url = {https://www.science.org/doi/10.1126/science.adi1778},
  urldate = {2023-10-20}
}

@article{Gu2021DomainSpecificLanguageModel,
  title = {Domain-{{Specific Language Model Pretraining}} for {{Biomedical Natural Language Processing}}},
  author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  year = {2021},
  month = oct,
  journal = {ACM Transactions on Computing for Healthcare},
  volume = {3},
  number = {1},
  pages = {2:1--2:23},
  issn = {2691-1957},
  doi = {10.1145/3458754},
  url = {https://doi.org/10.1145/3458754},
  urldate = {2023-03-01},
  abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
  keywords = {Biomedical,domain-specific pretraining,NLP},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gu2021DomainSpecificLanguageModel_Domain-Specific Language Model Pretraining for Biomedical Natural Language.pdf}
}

@inproceedings{Gu2023EvaluationNeuralSelective,
  title = {On the {{Evaluation}} of {{Neural Selective Prediction Methods}} for {{Natural Language Processing}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gu, Zhengyao and Hopkins, Mark},
  year = {2023},
  month = jul,
  pages = {7888--7899},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  url = {https://aclanthology.org/2023.acl-long.437},
  urldate = {2023-07-18},
  abstract = {We provide a survey and empirical comparison of the state-of-the-art in neural selective classification for NLP tasks. We also provide a methodological blueprint, including a novel metric called refinement that provides a calibrated evaluation of confidence functions for selective prediction. Finally, we supply documented, open-source code to support the future development of selective prediction techniques.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gu2023EvaluationNeuralSelective_On the Evaluation of Neural Selective Prediction Methods for Natural Language.pdf}
}

@inproceedings{Gu2023EvaluationNeuralSelectivea,
  title = {On the {{Evaluation}} of {{Neural Selective Prediction Methods}} for {{Natural Language Processing}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gu, Zhengyao and Hopkins, Mark},
  year = {2023},
  month = jul,
  pages = {7888--7899},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  url = {https://aclanthology.org/2023.acl-long.437},
  urldate = {2023-07-25},
  abstract = {We provide a survey and empirical comparison of the state-of-the-art in neural selective classification for NLP tasks. We also provide a methodological blueprint, including a novel metric called refinement that provides a calibrated evaluation of confidence functions for selective prediction. Finally, we supply documented, open-source code to support the future development of selective prediction techniques.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gu2023EvaluationNeuralSelectivea_On the Evaluation of Neural Selective Prediction Methods for Natural Language.pdf}
}

@misc{Gu2023WatermarkingPretrainedLanguage,
  title = {Watermarking {{Pre-trained Language Models}} with {{Backdooring}}},
  author = {Gu, Chenxi and Huang, Chengsong and Zheng, Xiaoqing and Chang, Kai-Wei and Hsieh, Cho-Jui},
  year = {2023},
  month = feb,
  number = {arXiv:2210.07543},
  eprint = {2210.07543},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.07543},
  urldate = {2023-06-06},
  abstract = {Large pre-trained language models (PLMs) have proven to be a crucial component of modern natural language processing systems. PLMs typically need to be fine-tuned on task-specific downstream datasets, which makes it hard to claim the ownership of PLMs and protect the developer's intellectual property due to the catastrophic forgetting phenomenon. We show that PLMs can be watermarked with a multi-task learning framework by embedding backdoors triggered by specific inputs defined by the owners, and those watermarks are hard to remove even though the watermarked PLMs are fine-tuned on multiple downstream tasks. In addition to using some rare words as triggers, we also show that the combination of common words can be used as backdoor triggers to avoid them being easily detected. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted with a high success rate and less influenced by the follow-up fine-tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gu2023WatermarkingPretrainedLanguage_Watermarking Pre-trained Language Models with Backdooring.pdf;/Users/ma/Zotero/storage/JFI2E5EI/2210.html}
}

@misc{Gu2023WatermarkingPretrainedLanguagea,
  title = {Watermarking {{Pre-trained Language Models}} with {{Backdooring}}},
  author = {Gu, Chenxi and Huang, Chengsong and Zheng, Xiaoqing and Chang, Kai-Wei and Hsieh, Cho-Jui},
  year = {2023},
  month = feb,
  number = {arXiv:2210.07543},
  eprint = {2210.07543},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.07543},
  url = {http://arxiv.org/abs/2210.07543},
  urldate = {2023-09-15},
  abstract = {Large pre-trained language models (PLMs) have proven to be a crucial component of modern natural language processing systems. PLMs typically need to be fine-tuned on task-specific downstream datasets, which makes it hard to claim the ownership of PLMs and protect the developer's intellectual property due to the catastrophic forgetting phenomenon. We show that PLMs can be watermarked with a multi-task learning framework by embedding backdoors triggered by specific inputs defined by the owners, and those watermarks are hard to remove even though the watermarked PLMs are fine-tuned on multiple downstream tasks. In addition to using some rare words as triggers, we also show that the combination of common words can be used as backdoor triggers to avoid them being easily detected. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted with a high success rate and less influenced by the follow-up fine-tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gu2023WatermarkingPretrainedLanguagea_Watermarking Pre-trained Language Models with Backdooring.pdf;/Users/ma/Zotero/storage/IDMBEVLG/2210.html}
}

@misc{Gueta2023KnowledgeRegionWeight,
  title = {Knowledge Is a {{Region}} in {{Weight Space}} for {{Fine-tuned Language Models}}},
  author = {Gueta, Almog and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04863},
  eprint = {2302.04863},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04863},
  url = {http://arxiv.org/abs/2302.04863},
  urldate = {2023-04-21},
  abstract = {Research on neural networks has largely focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, especially those trained or tested on different datasets. We address this by studying how the weight space and underlying loss landscape of different models are interconnected. Specifically, we demonstrate that fine-tuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also has high performance. Specifically, we show that language models that have been fine-tuned on the same dataset form a tight cluster in the weight space and that models fine-tuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models reaches new models that perform comparably or even better than models found via fine-tuning, even on tasks that the original models were not fine-tuned on. Our findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both. We leverage this finding and design a method to pick a better model for efficient fine-tuning. Specifically, we show that starting from the center of the region is as good or better than the pre-trained model in 11 of 12 datasets and improves accuracy by 3.06 on average.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gueta2023KnowledgeRegionWeight_Knowledge is a Region in Weight Space for Fine-tuned Language Models.pdf;/Users/ma/Zotero/storage/W9LEWWPA/2302.html}
}

@misc{Gupta2023CoveragebasedExampleSelection,
  title = {Coverage-Based {{Example Selection}} for {{In-Context Learning}}},
  author = {Gupta, Shivanshu and Gardner, Matt and Singh, Sameer},
  year = {2023},
  month = may,
  number = {arXiv:2305.14907},
  eprint = {2305.14907},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14907},
  url = {http://arxiv.org/abs/2305.14907},
  urldate = {2023-05-30},
  abstract = {In-context learning (ICL), the ability of large language models to perform novel tasks by conditioning on a prompt with a few task examples, requires demonstrations that are informative about the test instance. The standard approach of independently selecting the most similar examples selects redundant demonstrations while overlooking important information. This work proposes a framework for assessing the informativeness of demonstrations based on their coverage of salient aspects (e.g., reasoning patterns) of the test input. Using this framework, we show that contextual token embeddings effectively capture these salient aspects, and their recall measured using BERTScore-Recall (BSR) yields a reliable measure of informativeness. Further, we extend recall metrics like BSR to propose their set versions to find maximally informative sets of demonstrations. On 6 complex compositional generation tasks and 7 diverse LLMs, we show that Set-BSR outperforms the standard similarity-based approach by up to 16\% on average and, despite being learning-free, often surpasses methods that leverage task or LLM-specific training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Gupta2023CoveragebasedExampleSelection_Coverage-based Example Selection for In-Context Learning.pdf;/Users/ma/Zotero/storage/PRE8BEJ4/2305.html}
}

@article{Gupta2023PolarisedSocialMedia,
  title = {Polarised Social Media Discourse during {{COVID-19}} Pandemic: Evidence from {{YouTube}}},
  shorttitle = {Polarised Social Media Discourse during {{COVID-19}} Pandemic},
  author = {Gupta, Samrat and Jain, Gaurav and Tiwari, Amit Anand},
  year = {2023},
  month = jan,
  journal = {Behaviour \& Information Technology},
  volume = {42},
  number = {2},
  pages = {227--248},
  publisher = {{Taylor \& Francis}},
  issn = {0144-929X},
  doi = {10.1080/0144929X.2022.2059397},
  url = {https://doi.org/10.1080/0144929X.2022.2059397},
  urldate = {2023-10-15},
  abstract = {The onset of the COVID-19 pandemic has attracted significant attention on social media platforms as these platforms provide users unparalleled access to `information' from around the globe. In spite of demographic differences, people have been expressing and shaping their opinions using social media on topics ranging from the plight of migrant workers to vaccine development. However, the social media induced polarisation owing to selective online exposure to information during the COVID-19 pandemic has been a major cause of concern for countries across the world. In this paper, we analyse the temporal dynamics of polarisation in online discourse related to the COVID-19. We use random network theory-based simulation to investigate the evolution of opinion formation in comments posted on different COVID-19-related YouTube videos. Our findings reveal that as the pandemic unfolded, the extent of polarisation in the online discourse increased with time. We validate our experimental model using real-world complex networks and compare consensus formation on these networks with equivalent random networks. This study has several implications as polarisation around socio-cultural issues in crises such as pandemic can exacerbate the social divide. The framework proposed in this study can aid regulatory agencies to take required actions and mitigate social media-induced polarisation.},
  keywords = {consensus formation,COVID-19 pandemic,online opinion formation,Polarisation,social media}
}

@misc{Guu2020REALMRetrievalAugmentedLanguage,
  title = {{{REALM}}: {{Retrieval-Augmented Language Model Pre-Training}}},
  shorttitle = {{{REALM}}},
  author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  year = {2020},
  month = feb,
  number = {arXiv:2002.08909},
  eprint = {2002.08909},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.08909},
  url = {http://arxiv.org/abs/2002.08909},
  urldate = {2023-10-30},
  abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Guu2020REALMRetrievalAugmentedLanguage_REALM.pdf;/Users/ma/Zotero/storage/KTH8SFN2/2002.html}
}

@misc{Guu2023SimfluenceModelingInfluence,
  title = {Simfluence: {{Modeling}} the {{Influence}} of {{Individual Training Examples}} by {{Simulating Training Runs}}},
  shorttitle = {Simfluence},
  author = {Guu, Kelvin and Webson, Albert and Pavlick, Ellie and Dixon, Lucas and Tenney, Ian and Bolukbasi, Tolga},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08114},
  eprint = {2303.08114},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.08114},
  urldate = {2023-04-27},
  abstract = {Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects. To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example \$z\_1\$, then \$z\_2\$, ..., then \$z\_n\$, how would it behave on \$z\_\{test\}\$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on \$z\_\{test\}\$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur. We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75\%) across several tasks, models, and training methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Guu2023SimfluenceModelingInfluence_Simfluence.pdf;/Users/ma/Zotero/storage/YXSCMLR7/2303.html}
}

@misc{Guu2023SimfluenceModelingInfluencea,
  title = {Simfluence: {{Modeling}} the {{Influence}} of {{Individual Training Examples}} by {{Simulating Training Runs}}},
  shorttitle = {Simfluence},
  author = {Guu, Kelvin and Webson, Albert and Pavlick, Ellie and Dixon, Lucas and Tenney, Ian and Bolukbasi, Tolga},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08114},
  eprint = {2303.08114},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.08114},
  urldate = {2023-05-12},
  abstract = {Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects. To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example \$z\_1\$, then \$z\_2\$, ..., then \$z\_n\$, how would it behave on \$z\_\{test\}\$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on \$z\_\{test\}\$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur. We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75\%) across several tasks, models, and training methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Guu2023SimfluenceModelingInfluencea_Simfluence.pdf;/Users/ma/Zotero/storage/SK7Q6JU8/2303.html}
}

@inproceedings{Han2019JointEventTemporal,
  title = {Joint {{Event}} and {{Temporal Relation Extraction}} with {{Shared Representations}} and {{Structured Prediction}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Han, Rujun and Ning, Qiang and Peng, Nanyun},
  year = {2019},
  month = nov,
  pages = {434--444},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1041},
  url = {https://aclanthology.org/D19-1041},
  urldate = {2023-05-01},
  abstract = {We propose a joint event and temporal relation extraction model with shared representation learning and structured prediction. The proposed method has two advantages over existing work. First, it improves event representation by allowing the event and relation modules to share the same contextualized embeddings and neural representation learner. Second, it avoids error propagation in the conventional pipeline systems by leveraging structured inference and learning methods to assign both the event labels and the temporal relation labels jointly. Experiments show that the proposed method can improve both event extraction and temporal relation extraction over state-of-the-art systems, with the end-to-end F1 improved by 10\% and 6.8\% on two benchmark datasets respectively.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Han2019JointEventTemporal_Joint Event and Temporal Relation Extraction with Shared Representations and.pdf}
}

@article{Han2020AngerContributesSpread,
  title = {Anger Contributes to the Spread of {{COVID-19}} Misinformation},
  author = {Han, Jiyoung and Cha, Meeyoung and Lee, Wonjae},
  year = {2020},
  month = sep,
  journal = {Harvard Kennedy School Misinformation Review},
  volume = {1},
  number = {3},
  doi = {10.37016/mr-2020-39},
  url = {https://misinforeview.hks.harvard.edu/article/anger-contributes-to-the-spread-of-covid-19-misinformation/},
  urldate = {2023-10-14},
  abstract = {A survey conducted over South Korean adults (N=513) reveals that emotions, specifically anger, contribute to the broader spread of misinformation on COVID-19 by leading angry individuals to consider false claims to be ``scientifically credible.'' This pattern is more evident among conservatives than liberals. Our finding sheds light on new measures and journalistic interventions that could},
  langid = {american},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Han2020AngerContributesSpread_Anger contributes to the spread of COVID-19 misinformation.pdf}
}

@inproceedings{Han2020DomainKnowledgeEmpowered,
  title = {Domain {{Knowledge Empowered Structured Neural Net}} for {{End-to-End Event Temporal Relation Extraction}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Han, Rujun and Zhou, Yichao and Peng, Nanyun},
  year = {2020},
  month = nov,
  pages = {5717--5729},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.461},
  url = {https://aclanthology.org/2020.emnlp-main.461},
  urldate = {2023-05-01},
  abstract = {Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Han2020DomainKnowledgeEmpowered_Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal.pdf}
}

@inproceedings{Han2021ECONETEffectiveContinual,
  title = {{{ECONET}}: {{Effective Continual Pretraining}} of {{Language Models}} for {{Event Temporal Reasoning}}},
  shorttitle = {{{ECONET}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Han, Rujun and Ren, Xiang and Peng, Nanyun},
  year = {2021},
  month = nov,
  pages = {5367--5380},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.436},
  url = {https://aclanthology.org/2021.emnlp-main.436},
  urldate = {2023-05-01},
  abstract = {While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This **E**ffective **CON**tinual pre-training framework for **E**vent **T**emporal reasoning (ECONET) improves the PTLMs' fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Han2021ECONETEffectiveContinual_ECONET.pdf}
}

@inproceedings{Han2022GenerativePromptTuning,
  title = {Generative {{Prompt Tuning}} for {{Relation Classification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Han, Jiale and Zhao, Shuai and Cheng, Bo and Ma, Shengkun and Lu, Wei},
  year = {2022},
  month = dec,
  pages = {3170--3185},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.231},
  urldate = {2023-06-20},
  abstract = {Using prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to verbalizations with fixed length, which has proven effective for tasks with simple label spaces. However, when applied to relation classification exhibiting complex label spaces, vanilla prompt tuning methods may struggle with label verbalizations with arbitrary lengths due to rigid prompt restrictions. Inspired by the text infilling task for pre-training generative models that can flexibly predict missing spans, we propose a novel generative prompt tuning method to reformulate relation classification as an infilling problem, which frees our approach from limitations of current prompt based approaches and thus fully exploits rich semantics of entity and relation types. In addition, we design entity-guided decoding and discriminative relation scoring to generate and align relations effectively and efficiently during inference. Extensive experiments under fully supervised settings and low-resource settings demonstrate the effectiveness of our approach.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Han2022GenerativePromptTuning_Generative Prompt Tuning for Relation Classification.pdf}
}

@misc{Han2023InformationExtractionSolved,
  title = {Is {{Information Extraction Solved}} by {{ChatGPT}}? {{An Analysis}} of {{Performance}}, {{Evaluation Criteria}}, {{Robustness}} and {{Errors}}},
  shorttitle = {Is {{Information Extraction Solved}} by {{ChatGPT}}?},
  author = {Han, Ridong and Peng, Tao and Yang, Chaohao and Wang, Benyou and Liu, Lu and Wan, Xiang},
  year = {2023},
  month = may,
  number = {arXiv:2305.14450},
  eprint = {2305.14450},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.14450},
  urldate = {2023-06-21},
  abstract = {ChatGPT has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of ChatGPT from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate ChatGPT's performance on 17 datasets with 14 IE sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between ChatGPT and SOTA results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect ChatGPT's performance. Then, we analyze the robustness of ChatGPT on 14 IE sub-tasks, and find that: 1) ChatGPT rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect ChatGPT's performance; 3) ChatGPT cannot understand well the subject-object relationships in RE task. Finally, we analyze the errors of ChatGPT, and find that "unannotated spans" is the most dominant error type. This raises concerns about the quality of annotated data, and indicates the possibility of annotating data with ChatGPT. The data and code are released at Github site.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Han2023InformationExtractionSolved_Is Information Extraction Solved by ChatGPT.pdf;/Users/ma/Zotero/storage/DDU4HJD3/2305.html}
}

@inproceedings{Hao2023MultisourceInductiveKnowledge,
  title = {Multi-Source {{Inductive Knowledge Graph Transfer}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Hao, Junheng and Tang, Lu-An and Sun, Yizhou and Chen, Zhengzhang and Chen, Haifeng and Rhee, Junghwan and Li, Zhichuan and Wang, Wei},
  editor = {Amini, Massih-Reza and Canu, St{\'e}phane and Fischer, Asja and Guns, Tias and Kralj Novak, Petra and Tsoumakas, Grigorios},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {155--171},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-26390-3_10},
  abstract = {Large-scale information systems, such as knowledge graphs (KGs), enterprise system networks, often exhibit dynamic and complex activities. Recent research has shown that formalizing these information systems as graphs can effectively characterize the entities (nodes) and their relationships (edges). Transferring knowledge from existing well-curated source graphs can help construct the target graph of newly-deployed systems faster and better which no doubt will benefit downstream tasks such as link prediction and anomaly detection for new systems. However, current graph transferring methods are either based on a single source, which does not sufficiently consider multiple available sources, or not selectively learns from these sources. In this paper, we propose MSGT-GNN, a graph knowledge transfer model for efficient graph link prediction from multiple source graphs. MSGT-GNN consists of two components: the Intra-Graph Encoder, which embeds latent graph features of system entities into vectors; and the graph transferor, which utilizes graph attention mechanism to learn and optimize the embeddings of corresponding entities from multiple source graphs, in both node level and graph level. Experimental results on multiple real-world datasets from various domains show that MSGT-GNN outperforms other baseline approaches in the link prediction and demonstrate the merit of attentive graph knowledge transfer and the effectiveness of MSGT-GNN.},
  isbn = {978-3-031-26390-3},
  langid = {english},
  keywords = {Graph neural network,Knowledge graphs,Transfer learning}
}

@article{HaoReasoningLanguageModel,
  title = {Reasoning with {{Language Model}} Is {{Planning}} with {{World Model}}},
  author = {Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-ofThought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal world model to predict the world state (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, Reasoning via Planning (RAP). RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration vs. exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLaMA33B surpasses CoT on GPT-4 with 33\% relative improvement in a plan generation setting.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/A5VWNDNP/Hao et al. - Reasoning with Language Model is Planning with Wor.pdf}
}

@article{Harutyunyan2019MultitaskLearningBenchmarking,
  title = {Multitask Learning and Benchmarking with Clinical Time Series Data},
  author = {Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},
  year = {2019},
  month = jun,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {96},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0103-9},
  url = {https://www.nature.com/articles/s41597-019-0103-9},
  urldate = {2023-10-31},
  abstract = {Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been difficult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classification. We propose strong linear and neural baselines for all four tasks and evaluate the effect of deep supervision, multitask training and data-specific architectural modifications on the performance of neural models.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Databases,Disease-free survival,Machine learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Harutyunyan2019MultitaskLearningBenchmarking_Multitask learning and benchmarking with clinical time series data.pdf}
}

@article{HAYAWI202223,
  title = {{{ANTi-Vax}}: A Novel {{Twitter}} Dataset for {{COVID-19}} Vaccine Misinformation Detection},
  author = {Hayawi, K. and Shahriar, S. and Serhani, M.A. and Taleb, I. and Mathew, S.S.},
  year = {2022},
  journal = {Public Health},
  volume = {203},
  pages = {23--30},
  issn = {0033-3506},
  doi = {10.1016/j.puhe.2021.11.022},
  url = {https://www.sciencedirect.com/science/article/pii/S0033350621004534},
  abstract = {Objectives COVID-19 (SARS-CoV-2) pandemic has infected hundreds of millions and inflicted millions of deaths around the globe. Fortunately, the introduction of COVID-19 vaccines provided a glimmer of hope and a pathway to recovery. However, owing to misinformation being spread on social media and other platforms, there has been a rise in vaccine hesitancy which can lead to a negative impact on vaccine uptake in the population. The goal of this research is to introduce a novel machine learning\textendash based COVID-19 vaccine misinformation detection framework. Study design We collected and annotated COVID-19 vaccine tweets and trained machine learning algorithms to classify vaccine misinformation. Methods More than 15,000 tweets were annotated as misinformation or general vaccine tweets using reliable sources and validated by medical experts. The classification models explored were XGBoost, LSTM, and BERT transformer model. Results The best classification performance was obtained using BERT, resulting in 0.98 F1-score on the test set. The precision and recall scores were 0.97 and 0.98, respectively. Conclusion Machine learning\textendash based models are effective in detecting misinformation regarding COVID-19 vaccines on social media platforms.},
  keywords = {COVID-19,Deep learning,Misinformation detection,Natural language processing,Text classification,Vaccines}
}

@inproceedings{He2021ForeseeingBenefitsIncidental,
  title = {Foreseeing the {{Benefits}} of {{Incidental Supervision}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {He, Hangfeng and Zhang, Mingyuan and Ning, Qiang and Roth, Dan},
  year = {2021},
  month = nov,
  pages = {1782--1800},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.134},
  url = {https://aclanthology.org/2021.emnlp-main.134},
  urldate = {2023-10-27},
  abstract = {Real-world applications often require improved models by leveraging *a range of cheap incidental supervision signals*. These could include partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotations \textendash{} all having statistical associations with gold annotations but not exactly the same. However, we currently lack a principled way to measure the benefits of these signals to a given target task, and the common practice of evaluating these benefits is through exhaustive experiments with various models and hyperparameters. This paper studies whether we can, *in a single framework, quantify the benefits of various types of incidental signals for a given target task without going through combinatorial experiments*. We propose a unified PAC-Bayesian motivated informativeness measure, PABI, that characterizes the uncertainty reduction provided by incidental supervision signals. We demonstrate PABI's effectiveness by quantifying the value added by various types of incidental signals to sequence tagging tasks. Experiments on named entity recognition (NER) and question answering (QA) show that PABI's predictions correlate well with learning performance, providing a promising way to determine, ahead of learning, which supervision signals would be beneficial.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/He2021ForeseeingBenefitsIncidental_Foreseeing the Benefits of Incidental Supervision.pdf}
}

@article{He2021GenerateAnnotateLearn,
  title = {Generate, {{Annotate}}, and {{Learn}}: {{Generative Models Advance Self-Training}} and {{Knowledge Distillation}}},
  shorttitle = {Generate, {{Annotate}}, and {{Learn}}},
  author = {He, Xuanli and Nassar, Islam and Kiros, Jamie Ryan and Haffari, Gholamreza and Norouzi, Mohammad},
  year = {2021},
  month = oct,
  url = {https://openreview.net/forum?id=oC12z8lkbrU},
  urldate = {2023-06-24},
  abstract = {Semi-Supervised Learning (SSL) has seen success in many application domains, but this success often relies on the availability of task-specific unlabeled data. Knowledge distillation (KD) has enabled compressing deep networks, achieving the best results when distilling knowledge on fresh task-specific unlabeled examples. However, task-specific unlabeled data can be challenging to find, especially for NLP problems. We present a simple framework called "generate, annotate, and learn (GAL)" that uses unconditional language models to synthesize in-domain unlabeled data, helping advance SSL and KD on NLP and tabular tasks. To obtain strong task-specific generative models, we either fine-tune a large language model (LLM) on inputs from specific tasks, or prompt a LLM with a few input examples to generate more unlabeled examples. Then, we use existing classifiers to annotate generated unlabeled examples with pseudo labels, which are used as additional training data or as additional prompts. GAL improves prompt-based few-shot learning on several NLP tasks. It also yields a new state-of-the-art for 6-layer transformers on the GLUE leaderboard. Finally, self-training with GAL offers large gains on four tabular tasks from the UCI repository.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/He2021GenerateAnnotateLearn_Generate, Annotate, and Learn.pdf}
}

@misc{He2022RethinkingRetrievalFaithful,
  title = {Rethinking with {{Retrieval}}: {{Faithful Large Language Model Inference}}},
  shorttitle = {Rethinking with {{Retrieval}}},
  author = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
  year = {2022},
  month = dec,
  number = {arXiv:2301.00303},
  eprint = {2301.00303},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2301.00303},
  urldate = {2023-02-09},
  abstract = {Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/He2022RethinkingRetrievalFaithful_Rethinking with Retrieval.pdf;/Users/ma/Zotero/storage/LJKVQ5K3/2301.html}
}

@misc{He2022RethinkingRetrievalFaithfula,
  title = {Rethinking with {{Retrieval}}: {{Faithful Large Language Model Inference}}},
  shorttitle = {Rethinking with {{Retrieval}}},
  author = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
  year = {2022},
  month = dec,
  number = {arXiv:2301.00303},
  eprint = {2301.00303},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.00303},
  url = {http://arxiv.org/abs/2301.00303},
  urldate = {2023-10-26},
  abstract = {Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/He2022RethinkingRetrievalFaithfula_Rethinking with Retrieval.pdf;/Users/ma/Zotero/storage/GNM5BJER/2301.html}
}

@misc{He2023SurveyLargeLanguage,
  title = {A {{Survey}} of {{Large Language Models}} for {{Healthcare}}: From {{Data}}, {{Technology}}, and {{Applications}} to {{Accountability}} and {{Ethics}}},
  shorttitle = {A {{Survey}} of {{Large Language Models}} for {{Healthcare}}},
  author = {He, Kai and Mao, Rui and Lin, Qika and Ruan, Yucheng and Lan, Xiang and Feng, Mengling and Cambria, Erik},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05694},
  eprint = {2310.05694},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2310.05694},
  urldate = {2023-10-31},
  abstract = {The utilization of large language models (LLMs) in the Healthcare domain has generated both excitement and concern due to their ability to effectively respond to freetext queries with certain professional knowledge. This survey outlines the capabilities of the currently developed LLMs for Healthcare and explicates their development process, with the aim of providing an overview of the development roadmap from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, as well as comparing various LLMs with each other. Then we summarize related Healthcare training data, training methods, optimization strategies, and usage. Finally, the unique concerns associated with deploying LLMs in Healthcare settings are investigated, particularly regarding fairness, accountability, transparency and ethics. Our survey provide a comprehensive investigation from perspectives of both computer science and Healthcare specialty. Besides the discussion about Healthcare concerns, we supports the computer science community by compiling a collection of open source resources, such as accessible datasets, the latest methodologies, code implementations, and evaluation benchmarks in the Github. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/He2023SurveyLargeLanguage_A Survey of Large Language Models for Healthcare.pdf;/Users/ma/Zotero/storage/K7BSCYFM/2310.html}
}

@misc{Hendrycks2021MeasuringMassiveMultitask,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.03300},
  urldate = {2023-03-08},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hendrycks2021MeasuringMassiveMultitask_Measuring Massive Multitask Language Understanding.pdf;/Users/ma/Zotero/storage/HLCFKKVY/2009.html}
}

@misc{Hendrycks2021MeasuringMassiveMultitaska,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2009.03300},
  urldate = {2023-03-22},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hendrycks2021MeasuringMassiveMultitaska_Measuring Massive Multitask Language Understanding.pdf;/Users/ma/Zotero/storage/HR2VN35P/2009.html}
}

@article{Himelein-Wachowiak2021BotsMisinformationSpread,
  title = {Bots and {{Misinformation Spread}} on {{Social Media}}: {{Implications}} for {{COVID-19}}},
  shorttitle = {Bots and {{Misinformation Spread}} on {{Social Media}}},
  author = {{Himelein-Wachowiak}, McKenzie and Giorgi, Salvatore and Devoto, Amanda and Rahman, Muhammad and Ungar, Lyle and Schwartz, H. Andrew and Epstein, David H. and Leggio, Lorenzo and Curtis, Brenda},
  year = {2021},
  month = may,
  journal = {Journal of Medical Internet Research},
  volume = {23},
  number = {5},
  pages = {e26933},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/26933},
  url = {https://www.jmir.org/2021/5/e26933},
  urldate = {2023-10-12},
  abstract = {As of March 2021, the SARS-CoV-2 virus has been responsible for over 115 million cases of COVID-19 worldwide, resulting in over 2.5 million deaths. As the virus spread exponentially, so did its media coverage, resulting in a proliferation of conflicting information on social media platforms\textemdash a so-called ``infodemic.'' In this viewpoint, we survey past literature investigating the role of automated accounts, or ``bots,'' in spreading such misinformation, drawing connections to the COVID-19 pandemic. We also review strategies used by bots to spread (mis)information and examine the potential origins of bots. We conclude by conducting and presenting a secondary analysis of data sets of known bots in which we find that up to 66\% of bots are discussing COVID-19. The proliferation of COVID-19 (mis)information by bots, coupled with human susceptibility to believing and sharing misinformation, may well impact the course of the pandemic.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Himelein-Wachowiak2021BotsMisinformationSpread_Bots and Misinformation Spread on Social Media.pdf;/Users/ma/Zotero/storage/H8FNXLI7/e26933.html}
}

@misc{Hollmann2023LargeLanguageModels,
  title = {Large {{Language Models}} for {{Automated Data Science}}: {{Introducing CAAFE}} for {{Context-Aware Automated Feature Engineering}}},
  shorttitle = {Large {{Language Models}} for {{Automated Data Science}}},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Hutter, Frank},
  year = {2023},
  month = sep,
  number = {arXiv:2305.03403},
  eprint = {2305.03403},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.03403},
  url = {http://arxiv.org/abs/2305.03403},
  urldate = {2023-10-25},
  abstract = {As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets -- boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature. CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our \$\textbackslash href\{https://github.com/automl/CAAFE\}\{code\}\$, a simple \$\textbackslash href\{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB\_alZvyARTMjhl6RZf0a\}\{demo\}\$ and a \$\textbackslash href\{https://pypi.org/project/caafe/\}\{python\textbackslash{} package\}\$.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hollmann2023LargeLanguageModels_Large Language Models for Automated Data Science.pdf;/Users/ma/Zotero/storage/IYWGR2HY/2305.html}
}

@misc{Honovich2022UnnaturalInstructionsTuning,
  title = {Unnatural {{Instructions}}: {{Tuning Language Models}} with ({{Almost}}) {{No Human Labor}}},
  shorttitle = {Unnatural {{Instructions}}},
  author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09689},
  eprint = {2212.09689},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2212.09689},
  urldate = {2023-03-08},
  abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Honovich2022UnnaturalInstructionsTuning_Unnatural Instructions.pdf;/Users/ma/Zotero/storage/LUP86DX6/2212.html}
}

@misc{Honovich2022UnnaturalInstructionsTuninga,
  title = {Unnatural {{Instructions}}: {{Tuning Language Models}} with ({{Almost}}) {{No Human Labor}}},
  shorttitle = {Unnatural {{Instructions}}},
  author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09689},
  eprint = {2212.09689},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09689},
  url = {http://arxiv.org/abs/2212.09689},
  urldate = {2023-05-04},
  abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Honovich2022UnnaturalInstructionsTuninga_Unnatural Instructions.pdf;/Users/ma/Zotero/storage/GGSAIHZD/2212.html}
}

@inproceedings{Honovich2023InstructionInductionFew,
  title = {Instruction {{Induction}}: {{From Few Examples}} to {{Natural Language Task Descriptions}}},
  shorttitle = {Instruction {{Induction}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Honovich, Or and Shaham, Uri and Bowman, Samuel R. and Levy, Omer},
  year = {2023},
  month = jul,
  pages = {1935--1952},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.108},
  url = {https://aclanthology.org/2023.acl-long.108},
  urldate = {2023-10-23},
  abstract = {Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7\% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8\% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Honovich2023InstructionInductionFew_Instruction Induction.pdf}
}

@misc{Hope2023ComputationalInflectionScientific,
  title = {A {{Computational Inflection}} for {{Scientific Discovery}}},
  author = {Hope, Tom and Downey, Doug and Etzioni, Oren and Weld, Daniel S. and Horvitz, Eric},
  year = {2023},
  month = may,
  number = {arXiv:2205.02007},
  eprint = {2205.02007},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.02007},
  url = {http://arxiv.org/abs/2205.02007},
  urldate = {2023-10-26},
  abstract = {We stand at the foot of a significant inflection in the trajectory of scientific discovery. As society continues on its fast-paced digital transformation, so does humankind's collective scientific knowledge and discourse. We now read and write papers in digitized form, and a great deal of the formal and informal processes of science are captured digitally -- including papers, preprints and books, code and datasets, conference presentations, and interactions in social networks and collaboration and communication platforms. The transition has led to the creation and growth of a tremendous amount of information -- much of which is available for public access -- opening exciting opportunities for computational models and systems that analyze and harness it. In parallel, exponential growth in data processing power has fueled remarkable advances in artificial intelligence, including large neural language models capable of learning powerful representations from unstructured text. Dramatic changes in scientific communication -- such as the advent of the first scientific journal in the 17th century -- have historically catalyzed revolutions in scientific thought. The confluence of societal and computational trends suggests that computer science is poised to ignite a revolution in the scientific process itself.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hope2023ComputationalInflectionScientific_A Computational Inflection for Scientific Discovery.pdf;/Users/ma/Zotero/storage/4HKNV2XZ/2205.html}
}

@inproceedings{Hsu2022DEGREEDataEfficientGenerationBased,
  title = {{{DEGREE}}: {{A Data-Efficient Generation-Based Event Extraction Model}}},
  shorttitle = {{{DEGREE}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Hsu, I-Hung and Huang, Kuan-Hao and Boschee, Elizabeth and Miller, Scott and Natarajan, Prem and Chang, Kai-Wei and Peng, Nanyun},
  year = {2022},
  month = jul,
  pages = {1890--1908},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.138},
  url = {https://aclanthology.org/2022.naacl-main.138},
  urldate = {2023-05-24},
  abstract = {Event extraction requires high-quality expert human annotations, which are usually expensive. Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge. In this paper, we focus on low-resource end-to-end event extraction and propose DEGREE, a data-efficient model that formulates event extraction as a conditional generation problem. Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern. The final event predictions are then extracted from the generated sentence with a deterministic algorithm. DEGREE has three advantages to learn well with less training data. First, our designed prompts provide semantic guidance for DEGREE to leverage DEGREE and thus better capture the event arguments. Moreover, DEGREE is capable of using additional weakly-supervised information, such as the description of events encoded in the prompts. Finally, DEGREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them. Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hsu2022DEGREEDataEfficientGenerationBased_DEGREE.pdf}
}

@misc{Hu2021LoRALowRankAdaptation,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2023-10-27},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,i1},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hu2021LoRALowRankAdaptation_LoRA.pdf;/Users/ma/Zotero/storage/6K8GW7Q3/2106.html}
}

@misc{Hu2023GDAGenerativeData,
  title = {{{GDA}}: {{Generative Data Augmentation Techniques}} for {{Relation Extraction Tasks}}},
  shorttitle = {{{GDA}}},
  author = {Hu, Xuming and Liu, Aiwei and Tan, Zeqi and Zhang, Xin and Zhang, Chenwei and King, Irwin and Yu, Philip S.},
  year = {2023},
  month = may,
  number = {arXiv:2305.16663},
  eprint = {2305.16663},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.16663},
  urldate = {2023-06-01},
  abstract = {Relation extraction (RE) tasks show promising performance in extracting relations from two entities mentioned in sentences, given sufficient annotations available during training. Such annotations would be labor-intensive to obtain in practice. Existing work adopts data augmentation techniques to generate pseudo-annotated sentences beyond limited annotations. These techniques neither preserve the semantic consistency of the original sentences when rule-based augmentations are adopted, nor preserve the syntax structure of sentences when expressing relations using seq2seq models, resulting in less diverse augmentations. In this work, we propose a dedicated augmentation technique for relational texts, named GDA, which uses two complementary modules to preserve both semantic consistency and syntax structures. We adopt a generative formulation and design a multi-tasking solution to achieve synergies. Furthermore, GDA adopts entity hints as the prior knowledge of the generative model to augment diverse sentences. Experimental results in three datasets under a low-resource setting showed that GDA could bring \{\textbackslash em 2.0\textbackslash\%\} F1 improvements compared with no augmentation technique. Source code and data are available.},
  archiveprefix = {arxiv},
  keywords = {68T01,Computer Science - Computation and Language,I.2.7},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Hu2023GDAGenerativeData_GDA.pdf;/Users/ma/Zotero/storage/GE2WJQWK/2305.html}
}

@inproceedings{Huang2018ZeroShotTransferLearning,
  title = {Zero-{{Shot Transfer Learning}} for {{Event Extraction}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Lifu and Ji, Heng and Cho, Kyunghyun and Dagan, Ido and Riedel, Sebastian and Voss, Clare},
  year = {2018},
  month = jul,
  pages = {2160--2170},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1201},
  url = {https://aclanthology.org/P18-1201},
  urldate = {2023-05-23},
  abstract = {Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2018ZeroShotTransferLearning_Zero-Shot Transfer Learning for Event Extraction.pdf}
}

@inproceedings{Huang2021DocumentlevelEntitybasedExtraction,
  title = {Document-Level {{Entity-based Extraction}} as {{Template Generation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Huang, Kung-Hsiang and Tang, Sam and Peng, Nanyun},
  year = {2021},
  month = nov,
  pages = {5257--5269},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.426},
  url = {https://aclanthology.org/2021.emnlp-main.426},
  urldate = {2023-02-20},
  abstract = {Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TopK Copy, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26\%), binary RE (+4.8\%), and 4-ary RE (+2.7\%) in F1 score.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2021DocumentlevelEntitybasedExtraction_Document-level Entity-based Extraction as Template Generation.pdf}
}

@inproceedings{Huang2021DocumentlevelEntitybasedExtractiona,
  title = {Document-Level {{Entity-based Extraction}} as {{Template Generation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Huang, Kung-Hsiang and Tang, Sam and Peng, Nanyun},
  year = {2021},
  month = nov,
  pages = {5257--5269},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.426},
  url = {https://aclanthology.org/2021.emnlp-main.426},
  urldate = {2023-05-01},
  abstract = {Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture cross-entity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TopK Copy, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26\%), binary RE (+4.8\%), and 4-ary RE (+2.7\%) in F1 score.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2021DocumentlevelEntitybasedExtractiona_Document-level Entity-based Extraction as Template Generation.pdf}
}

@inproceedings{Huang2021DocumentlevelEventExtraction,
  title = {Document-Level {{Event Extraction}} with {{Efficient End-to-end Learning}} of {{Cross-event Dependencies}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Narrative Understanding}}},
  author = {Huang, Kung-Hsiang and Peng, Nanyun},
  year = {2021},
  month = jun,
  pages = {36--47},
  publisher = {{Association for Computational Linguistics}},
  address = {{Virtual}},
  doi = {10.18653/v1/2021.nuse-1.4},
  url = {https://aclanthology.org/2021.nuse-1.4},
  urldate = {2023-02-20},
  abstract = {Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency.},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2021DocumentlevelEventExtraction_Document-level Event Extraction with Efficient End-to-end Learning of.pdf}
}

@inproceedings{Huang2021ExploringSentenceCommunity,
  title = {Exploring {{Sentence Community}} for {{Document-Level Event Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Huang, Yusheng and Jia, Weijia},
  year = {2021},
  month = nov,
  pages = {340--351},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.32},
  url = {https://aclanthology.org/2021.findings-emnlp.32},
  urldate = {2023-02-20},
  abstract = {Document-level event extraction is critical to various natural language processing tasks for providing structured information. Existing approaches by sequential modeling neglect the complex logic structures for long texts. In this paper, we leverage the entity interactions and sentence interactions within long documents and transform each document into an undirected unweighted graph by exploiting the relationship between sentences. We introduce the Sentence Community to represent each event as a subgraph. Furthermore, our framework SCDEE maintains the ability to extract multiple events by sentence community detection using graph attention networks and alleviate the role overlapping issue by predicting arguments in terms of roles. Experiments demonstrate that our framework achieves competitive results over state-of-the-art methods on the large-scale document-level event extraction dataset.},
  keywords = {dataset\_Doc2EDAG,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2021ExploringSentenceCommunity_Exploring Sentence Community for Document-Level Event Extraction.pdf}
}

@inproceedings{Huang2022MultilingualGenerativeLanguage,
  title = {Multilingual {{Generative Language Models}} for {{Zero-Shot Cross-Lingual Event Argument Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Huang, Kuan-Hao and Hsu, I-Hung and Natarajan, Prem and Chang, Kai-Wei and Peng, Nanyun},
  year = {2022},
  month = may,
  pages = {4633--4646},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.317},
  url = {https://aclanthology.org/2022.acl-long.317},
  urldate = {2023-05-01},
  abstract = {We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE). By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments. We design language-agnostic templates to represent the event argument structures, which are compatible with any language, hence facilitating the cross-lingual transfer. Our proposed model finetunes multilingual pre-trained generative language models to generate sentences that fill in the language-agnostic template with arguments extracted from the input passage. The model is trained on source languages and is then directly applied to target languages for event argument extraction. Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE. Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2022MultilingualGenerativeLanguage_Multilingual Generative Language Models for Zero-Shot Cross-Lingual Event.pdf}
}

@misc{Huang2023ManiTweetNewBenchmark,
  title = {{{ManiTweet}}: {{A New Benchmark}} for {{Identifying Manipulation}} of {{News}} on {{Social Media}}},
  shorttitle = {{{ManiTweet}}},
  author = {Huang, Kung-Hsiang and Chan, Hou Pong and McKeown, Kathleen and Ji, Heng},
  year = {2023},
  month = may,
  number = {arXiv:2305.14225},
  eprint = {2305.14225},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14225},
  url = {http://arxiv.org/abs/2305.14225},
  urldate = {2023-05-24},
  abstract = {Considerable advancements have been made to tackle the misrepresentation of information derived from reference articles in the domains of fact-checking and faithful summarization. However, an unaddressed aspect remains - the identification of social media posts that manipulate information within associated news articles. This task presents a significant challenge, primarily due to the prevalence of personal opinions in such posts. We present a novel task, identifying manipulation of news on social media, which aims to detect manipulation in social media posts and identify manipulated or inserted information. To study this task, we have proposed a data collection schema and curated a dataset called ManiTweet, consisting of 3.6K pairs of tweets and corresponding articles. Our analysis demonstrates that this task is highly challenging, with large language models (LLMs) yielding unsatisfactory performance. Additionally, we have developed a simple yet effective basic model that outperforms LLMs significantly on the ManiTweet dataset. Finally, we have conducted an exploratory analysis of human-written tweets, unveiling intriguing connections between manipulation and the domain and factuality of news articles, as well as revealing that manipulated sentences are more likely to encapsulate the main story or consequences of a news outlet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Huang2023ManiTweetNewBenchmark_ManiTweet.pdf;/Users/ma/Zotero/storage/L2ZHVPGA/2305.html}
}

@inproceedings{HuguetCabot2021REBELRelationExtraction,
  title = {{{REBEL}}: {{Relation Extraction By End-to-end Language}} Generation},
  shorttitle = {{{REBEL}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Huguet Cabot, Pere-Llu{\'i}s and Navigli, Roberto},
  year = {2021},
  month = nov,
  pages = {2370--2381},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.204},
  url = {https://aclanthology.org/2021.findings-emnlp.204},
  urldate = {2023-05-01},
  abstract = {Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model's flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/HuguetCabot2021REBELRelationExtraction_REBEL.pdf}
}

@misc{Ignat2023PhDStudentPerspective,
  title = {A {{PhD Student}}'s {{Perspective}} on {{Research}} in {{NLP}} in the {{Era}} of {{Very Large Language Models}}},
  author = {Ignat, Oana and Jin, Zhijing and Abzaliev, Artem and Biester, Laura and Castro, Santiago and Deng, Naihao and Gao, Xinyi and Gunal, Aylin and He, Jacky and Kazemi, Ashkan and Khalifa, Muhammad and Koh, Namho and Lee, Andrew and Liu, Siyang and Min, Do June and Mori, Shinka and Nwatu, Joan and {Perez-Rosas}, Veronica and Shen, Siqi and Wang, Zekun and Wu, Winston and Mihalcea, Rada},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2305.12544v1},
  urldate = {2023-05-30},
  abstract = {Recent progress in large language models has enabled the deployment of many generative NLP applications. At the same time, it has also led to a misleading public discourse that ``it's all been solved.'' Not surprisingly, this has in turn made many NLP researchers -- especially those at the beginning of their career -- wonder about what NLP research area they should focus on. This document is a compilation of NLP research directions that are rich for exploration, reflecting the views of a diverse group of PhD students in an academic research lab. While we identify many research areas, many others exist; we do not cover those areas that are currently addressed by LLMs but where LLMs lag behind in performance, or those focused on LLM development. We welcome suggestions for other research directions to include: https://bit.ly/nlp-era-llm},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ignat2023PhDStudentPerspective_A PhD Student's Perspective on Research in NLP in the Era of Very Large.pdf}
}

@inproceedings{Ilyas2022DatamodelsUnderstandingPredictions,
  title = {Datamodels: {{Understanding Predictions}} with {{Data}} and {{Data}} with {{Predictions}}},
  shorttitle = {Datamodels},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Ilyas, Andrew and Park, Sung Min and Engstrom, Logan and Leclerc, Guillaume and Madry, Aleksander},
  year = {2022},
  month = jun,
  pages = {9525--9587},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/ilyas22a.html},
  urldate = {2023-10-26},
  abstract = {We present a conceptual framework, datamodeling, for analyzing the behavior of a model class in terms of the training data. For any fixed ``target'' example {$\mathsl{x}$}xx, training set {$\mathsl{S}$}SS, and learning algorithm, a datamodel is a parameterized function 2{$\mathsl{S}\rightarrow\mathbb{R}$}2S\textrightarrow R2\^S \textbackslash to \textbackslash mathbb\{R\} that for any subset of {$\mathsl{S}{'}\subset\mathsl{S}$}S{${'}\subset$}SS' \textbackslash subset S\textemdash using only information about which examples of {$\mathsl{S}$}SS are contained in {$\mathsl{S}{'}$}S{${'}$}S'\textemdash predicts the outcome of training a model on {$\mathsl{S}{'}$}S{${'}$}S' and evaluating on {$\mathsl{x}$}xx. Despite the complexity of the underlying process being approximated (e.g. end-to-end training and evaluation of deep neural networks), we show that even simple linear datamodels successfully predict model outputs. We then demonstrate that datamodels give rise to a variety of applications, such as: accurately predicting the effect of dataset counterfactuals; identifying brittle predictions; finding semantically similar examples; quantifying train-test leakage; and embedding data into a well-behaved and feature-rich representation space.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ilyas2022DatamodelsUnderstandingPredictions_Datamodels.pdf}
}

@inproceedings{Im2023LessNotMore,
  title = {Less Is {{Not More}}: {{Improving Findability}} and {{Actionability}} of {{Privacy Controls}} for {{Online Behavioral Advertising}}},
  shorttitle = {Less Is {{Not More}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Im, Jane and Wang, Ruiyi and Lyu, Weikun and Cook, Nick and Habib, Hana and Cranor, Lorrie Faith and Banovic, Nikola and Schaub, Florian},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--33},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3544548.3580773},
  url = {https://dl.acm.org/doi/10.1145/3544548.3580773},
  urldate = {2023-04-27},
  abstract = {Tech companies that rely on ads for business argue that users have control over their data via ad privacy settings. However, these ad settings are often hidden. This work aims to inform the design of findable ad controls and study their impact on users' behavior and sentiment. We iteratively designed ad control interfaces that varied in the setting's (1) entry point (within ads, at the feed's top) and (2) level of actionability, with high actionability directly surfacing links to specific advertisement settings, and low actionability pointing to general settings pages (which is reminiscent of companies' current approach to ad controls). We built a Chrome extension that augments Facebook with our experimental ad control interfaces and conducted a between-subjects online experiment with 110 participants. Results showed that entry points within ads or at the feed's top, and high actionability interfaces, both increased Facebook ad settings' findability and discoverability, as well as participants' perceived usability of them. High actionability also reduced users' effort in finding ad settings. Participants perceived high and low actionability as equally usable, which shows it is possible to design more actionable ad controls without overwhelming users. We conclude by emphasizing the importance of regulation to provide specific and research-informed requirements to companies on how to design usable ad controls.},
  isbn = {978-1-4503-9421-5},
  keywords = {ad settings,advertising,consent.,Privacy,social media,social platforms,usability,user interface},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Im2023LessNotMore_Less is Not More.pdf}
}

@article{Islam2020MisinformationSharingSocial,
  title = {Misinformation Sharing and Social Media Fatigue during {{COVID-19}}: {{An}} Affordance and Cognitive Load Perspective},
  shorttitle = {Misinformation Sharing and Social Media Fatigue during {{COVID-19}}},
  author = {Islam, A. K. M. Najmul and Laato, Samuli and Talukder, Shamim and Sutinen, Erkki},
  year = {2020},
  month = oct,
  journal = {Technological Forecasting and Social Change},
  volume = {159},
  pages = {120201},
  issn = {0040-1625},
  doi = {10.1016/j.techfore.2020.120201},
  abstract = {Social media plays a significant role during pandemics such as COVID-19, as it enables people to share news as well as personal experiences and viewpoints with one another in real-time, globally. Building off the affordance lens and cognitive load theory, we investigate how motivational factors and personal attributes influence social media fatigue and the sharing of unverified information during the COVID-19 pandemic. Accordingly, we develop a model which we analyse using the structural equation modelling and neural network techniques with data collected from young adults in Bangladesh (N~=~433). The results show that people, who are driven by self-promotion and entertainment, and those suffering from deficient self-regulation, are more likely to share unverified information. Exploration and religiosity correlated negatively with the sharing of unverified information. However, exploration also increased social media fatigue. Our findings indicate that the different use purposes of social media introduce problematic consequences, in particular, increased misinformation sharing.},
  langid = {english},
  pmcid = {PMC7354273},
  pmid = {32834137},
  keywords = {COVID-19,Fake news,Fatigue,Misinformation,Pandemic,Social media},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Islam2020MisinformationSharingSocial_Misinformation sharing and social media fatigue during COVID-19.pdf}
}

@misc{Jain2023BaselineDefensesAdversarial,
  title = {Baseline {{Defenses}} for {{Adversarial Attacks Against Aligned Language Models}}},
  author = {Jain, Neel and Schwarzschild, Avi and Wen, Yuxin and Somepalli, Gowthami and Kirchenbauer, John and Chiang, Ping-yeh and Goldblum, Micah and Saha, Aniruddha and Geiping, Jonas and Goldstein, Tom},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00614},
  eprint = {2309.00614},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.00614},
  url = {http://arxiv.org/abs/2309.00614},
  urldate = {2023-09-15},
  abstract = {As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Jain2023BaselineDefensesAdversarial_Baseline Defenses for Adversarial Attacks Against Aligned Language Models.pdf;/Users/ma/Zotero/storage/WFM3XU7Y/2309.html}
}

@misc{Jiang2022GlobalLocalHierarchyaware,
  title = {Global and {{Local Hierarchy-aware Contrastive Framework}} for {{Implicit Discourse Relation Recognition}}},
  author = {Jiang, Yuxin and Zhang, Linhan and Wang, Wei},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13873},
  eprint = {2211.13873},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.13873},
  urldate = {2023-02-01},
  abstract = {Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all senses (defined as global hierarchy), and ignore the hierarchical sense label sequence corresponding to each instance (defined as local hierarchy). For the purpose of sufficiently exploiting global and local hierarchies of senses to learn better discourse relation representations, we propose a novel GLobal and LOcal Hierarchy-aware Contrastive Framework (GLOF), to model two kinds of hierarchies with the aid of contrastive learning. Experimental results on the PDTB dataset demonstrate that our method remarkably outperforms the current state-of-the-art model at all hierarchical levels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,contrastive learning,task: dialogue discourse relation,taxonomy},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Jiang2022GlobalLocalHierarchyaware_Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse.pdf;/Users/ma/Zotero/storage/VLBEYPEE/2211.html}
}

@misc{Jiang2023StructGPTGeneralFramework,
  title = {{{StructGPT}}: {{A General Framework}} for {{Large Language Model}} to {{Reason}} over {{Structured Data}}},
  shorttitle = {{{StructGPT}}},
  author = {Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2023},
  month = may,
  number = {arXiv:2305.09645},
  eprint = {2305.09645},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.09645},
  url = {http://arxiv.org/abs/2305.09645},
  urldate = {2023-05-30},
  abstract = {In this paper, we study how to improve the zero-shot reasoning ability of large language models\textasciitilde (LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \textbackslash emph\{Iterative Reading-then-Reasoning\textasciitilde (IRR)\} approach for solving question answering tasks based on structured data, called \textbackslash textbf\{StructGPT\}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\textbackslash ie \textbackslash emph\{reading\}), and let LLMs concentrate the reasoning task based on the collected information (\textbackslash ie \textbackslash emph\{reasoning\}). Specially, we propose an \textbackslash emph\{invoking-linearization-generation\} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at\textasciitilde\textbackslash url\{https://github.com/RUCAIBox/StructGPT\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Jiang2023StructGPTGeneralFramework_StructGPT.pdf;/Users/ma/Zotero/storage/DUKZZVZF/2305.html}
}

@misc{Jiang2023StructGPTGeneralFrameworka,
  title = {{{StructGPT}}: {{A General Framework}} for {{Large Language Model}} to {{Reason}} over {{Structured Data}}},
  shorttitle = {{{StructGPT}}},
  author = {Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2023},
  month = oct,
  number = {arXiv:2305.09645},
  eprint = {2305.09645},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.09645},
  url = {http://arxiv.org/abs/2305.09645},
  urldate = {2023-10-25},
  abstract = {In this paper, we study how to improve the zero-shot reasoning ability of large language models\textasciitilde (LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \textbackslash emph\{Iterative Reading-then-Reasoning\textasciitilde (IRR)\} approach for solving question answering tasks based on structured data, called \textbackslash textbf\{StructGPT\}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\textbackslash ie \textbackslash emph\{reading\}), and let LLMs concentrate the reasoning task based on the collected information (\textbackslash ie \textbackslash emph\{reasoning\}). Specially, we propose an \textbackslash emph\{invoking-linearization-generation\} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at\textasciitilde\textbackslash url\{https://github.com/RUCAIBox/StructGPT\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/KYQV2N8K/Jiang et al. - 2023 - StructGPT A General Framework for Large Language .pdf;/Users/ma/Zotero/storage/6EAEXDXF/2305.html}
}

@inproceedings{Jiao2022OpenVocabularyArgumentRole,
  title = {Open-{{Vocabulary Argument Role Prediction For Event Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Jiao, Yizhu and Li, Sha and Xie, Yiqing and Zhong, Ming and Ji, Heng and Han, Jiawei},
  year = {2022},
  month = dec,
  pages = {5404--5418},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.395},
  urldate = {2023-02-09},
  abstract = {The argument role in event extraction refers to the relation between an event and an argument participating in it. Despite the great progress in event extraction, existing studies still depend on roles pre-defined by domain experts. These studies expose obvious weakness when extending to emerging event types or new domains without available roles. Therefore, more attention and effort needs to be devoted to automatically customizing argument roles. In this paper, we define this essential but under-explored task: open-vocabulary argument role prediction. The goal of this task is to infer a set of argument roles for a given event type. We propose a novel unsupervised framework, RolePred for this task. Specifically, we formulate the role prediction problem as an in-filling task and construct prompts for a pre-trained language model to generate candidate roles. By extracting and analyzing the candidate arguments, the event-specific roles are further merged and selected. To standardize the research of this task, we collect a new human-annotated event extraction dataset including 143 customized argument roles with rich semantics. On this dataset, RolePred outperforms the existing methods by a large margin.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Jiao2022OpenVocabularyArgumentRole_Open-Vocabulary Argument Role Prediction For Event Extraction.pdf}
}

@inproceedings{JimenezGutierrez2022ThinkingGPT3InContext,
  title = {Thinking about {{GPT-3 In-Context Learning}} for {{Biomedical IE}}? {{Think Again}}},
  shorttitle = {Thinking about {{GPT-3 In-Context Learning}} for {{Biomedical IE}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Jimenez Gutierrez, Bernal and McNeal, Nikolas and Washington, Clayton and Chen, You and Li, Lang and Sun, Huan and Su, Yu},
  year = {2022},
  month = dec,
  pages = {4497--4512},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.329},
  urldate = {2023-02-09},
  abstract = {Large pre-trained language models (PLMs) such as GPT-3 have shown strong in-context learning capabilities, which are highly appealing for domains such as biomedicine that feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the first systematic and comprehensive study to compare the few-shot performance of GPT-3 in-context learning with fine-tuning smaller (i.e., BERT-sized) PLMs on two representative biomedical information extraction (IE) tasks: named entity recognition and relation extraction. We follow the true few-shot setting to avoid overestimating models' few-shot performance by model selection over a large validation set. We also optimize GPT-3's performance with known techniques such as contextual calibration and dynamic in-context example retrieval. However, our results show that GPT-3 still significantly underperforms compared to simply fine-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. More in-depth analyses further reveal issues of in-context learning that may be detrimental to IE tasks in general. Given the high cost of experimenting with GPT-3, we hope our study provides helpful guidance for biomedical researchers and practitioners towards more practical solutions such as fine-tuning small PLMs before better in-context learning is available for biomedical IE.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/JimenezGutierrez2022ThinkingGPT3InContext_Thinking about GPT-3 In-Context Learning for Biomedical IE.pdf}
}

@article{Jin2023EDGEFORMERSGRAPHEMPOWEREDTRANSFORM,
  title = {{{EDGEFORMERS}}: {{GRAPH-EMPOWERED TRANSFORM- ERS FOR REPRESENTATION LEARNING ON TEXTUAL- EDGE NETWORKS}}},
  author = {Jin, Bowen and Zhang, Yu and Meng, Yu and Han, Jiawei},
  year = {2023},
  abstract = {Edges in many real-world social/information networks are associated with rich text information (e.g., user-user communications or user-product reviews). However, mainstream network representation learning models focus on propagating and aggregating node attributes, lacking specific designs to utilize text semantics on edges. While there exist edge-aware graph neural networks, they directly initialize edge attributes as a feature vector, which cannot fully capture the contextualized text semantics of edges. In this paper, we propose Edgeformers1, a framework built upon graph-enhanced Transformers, to perform edge and node representation learning by modeling texts on edges in a contextualized way. Specifically, in edge representation learning, we inject network information into each Transformer layer when encoding edge texts; in node representation learning, we aggregate edge representations through an attention mechanism within each node's ego-graph. On five public datasets from three different domains, Edgeformers consistently outperform state-of-the-art baselines in edge classification and link prediction, demonstrating the efficacy in learning edge and node representations, respectively.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/JC5NESZI/Jin et al. - 2023 - EDGEFORMERS GRAPH-EMPOWERED TRANSFORM- ERS FOR RE.pdf}
}

@inproceedings{Jin2023HeterformerTransformerbasedDeep,
  title = {Heterformer: {{Transformer-based Deep Node Representation Learning}} on {{Heterogeneous Text-Rich Networks}}},
  shorttitle = {Heterformer},
  booktitle = {Proceedings of the 29th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Jin, Bowen and Zhang, Yu and Zhu, Qi and Han, Jiawei},
  year = {2023},
  month = aug,
  pages = {1020--1031},
  publisher = {{ACM}},
  address = {{Long Beach CA USA}},
  doi = {10.1145/3580305.3599376},
  url = {https://dl.acm.org/doi/10.1145/3580305.3599376},
  urldate = {2023-10-19},
  isbn = {9798400701030},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Jin2023HeterformerTransformerbasedDeep_Heterformer.pdf}
}

@inproceedings{Jin2023PattonLanguageModel,
  title = {Patton: {{Language Model Pretraining}} on {{Text-Rich Networks}}},
  shorttitle = {Patton},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Jin, Bowen and Zhang, Wentao and Zhang, Yu and Meng, Yu and Zhang, Xinyang and Zhu, Qi and Han, Jiawei},
  year = {2023},
  month = jul,
  pages = {7005--7020},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.387},
  url = {https://aclanthology.org/2023.acl-long.387},
  urldate = {2023-10-19},
  abstract = {A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Jin2023PattonLanguageModel_Patton.pdf}
}

@misc{Josifoski2023ExploitingAsymmetrySynthetic,
  title = {Exploiting {{Asymmetry}} for {{Synthetic Training Data Generation}}: {{SynthIE}} and the {{Case}} of {{Information Extraction}}},
  shorttitle = {Exploiting {{Asymmetry}} for {{Synthetic Training Data Generation}}},
  author = {Josifoski, Martin and Sakota, Marija and Peyrard, Maxime and West, Robert},
  year = {2023},
  month = mar,
  number = {arXiv:2303.04132},
  eprint = {2303.04132},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.04132},
  urldate = {2023-03-17},
  abstract = {Large language models (LLMs) show great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by the LLM: we show that, for problems with structured outputs, it is possible to prompt an LLM to perform the task in the opposite direction, to generate plausible text for the target structure. Leveraging the asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, demonstrate its superior quality compared to existing datasets in a human evaluation and use it to finetune small models (220M and 770M parameters). The models we introduce, SynthIE, outperform existing baselines of comparable size with a substantial gap of 57 and 79 absolute points in micro and macro F1, respectively. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Josifoski2023ExploitingAsymmetrySynthetic_Exploiting Asymmetry for Synthetic Training Data Generation.pdf;/Users/ma/Zotero/storage/QMK7C5QY/2303.html}
}

@misc{Kazemi2022LAMBADABackwardChaining,
  title = {{{LAMBADA}}: {{Backward Chaining}} for {{Automated Reasoning}} in {{Natural Language}}},
  shorttitle = {{{LAMBADA}}},
  author = {Kazemi, Seyed Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13894},
  eprint = {2212.13894},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.13894},
  url = {http://arxiv.org/abs/2212.13894},
  urldate = {2023-05-09},
  abstract = {Remarkable progress has been made on automated reasoning with knowledge specified as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is significantly more efficient at proof-finding problems. We import this intuition into the LM setting and develop a Backward Chaining algorithm, which we call LAMBADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference. We show that LAMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Kazemi2022LAMBADABackwardChaining_LAMBADA.pdf;/Users/ma/Zotero/storage/769TS3E2/2212.html}
}

@inproceedings{Khandelwal2019GeneralizationMemorizationNearest,
  title = {Generalization through {{Memorization}}: {{Nearest Neighbor Language Models}}},
  shorttitle = {Generalization through {{Memorization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  year = {2019},
  month = sep,
  url = {https://openreview.net/forum?id=HklBjCEKvH},
  urldate = {2023-10-26},
  abstract = {We introduce \$k\$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a \$k\$-nearest neighbors (\$k\$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this transformation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our \$k\$NN-LM achieves a new state-of-the-art perplexity of 15.79 -- a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Khandelwal2019GeneralizationMemorizationNearest_Generalization through Memorization.pdf}
}

@article{Khope2023StrategiesPredictiveSchemes,
  title = {Strategies of {{Predictive Schemes}} and {{Clinical Diagnosis}} for {{Prognosis Using MIMIC-III}}: {{A Systematic Review}}},
  shorttitle = {Strategies of {{Predictive Schemes}} and {{Clinical Diagnosis}} for {{Prognosis Using MIMIC-III}}},
  author = {Khope, Sarika R. and Elias, Susan},
  year = {2023},
  month = jan,
  journal = {Healthcare},
  volume = {11},
  number = {5},
  pages = {710},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2227-9032},
  doi = {10.3390/healthcare11050710},
  url = {https://www.mdpi.com/2227-9032/11/5/710},
  urldate = {2023-11-02},
  abstract = {The prime purpose of the proposed study is to construct a novel predictive scheme for assisting in the prognosis of criticality using the MIMIC-III dataset. With the adoption of various analytics and advanced computing in the healthcare system, there is an increasing trend toward developing an effective prognostication mechanism. Predictive-based modeling is the best alternative to work in this direction. This paper discusses various scientific contributions using desk research methodology towards the Medical Information Mart for Intensive Care (MIMIC-III). This open-access dataset is meant to help predict patient trajectories for various purposes ranging from mortality forecasting to treatment planning. With a dominant machine learning approach in this perspective, there is a need to discover the effectiveness of existing predictive methods. The resultant outcome of this paper offers an inclusive discussion about various available predictive schemes and clinical diagnoses using MIMIC-III in order to contribute toward better information associated with its strengths and weaknesses. Therefore, the paper provides a clear visualization of existing schemes for clinical diagnosis using a systematic review approach.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {critical care,deep learning,ICD9,machine learning,MIMIC-III,predictive},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Khope2023StrategiesPredictiveSchemes_Strategies of Predictive Schemes and Clinical Diagnosis for Prognosis Using.pdf}
}

@article{Kirkpatrick2017OvercomingCatastrophicForgetting,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {114},
  number = {13},
  eprint = {1612.00796},
  primaryclass = {cs, stat},
  pages = {3521--3526},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1611835114},
  url = {http://arxiv.org/abs/1612.00796},
  urldate = {2023-06-20},
  abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Kirkpatrick2017OvercomingCatastrophicForgetting_Overcoming catastrophic forgetting in neural networks.pdf;/Users/ma/Zotero/storage/6WDBSUMU/1612.html}
}

@inproceedings{Kolluru2022AlignmentAugmentedConsistentTranslation,
  title = {Alignment-{{Augmented Consistent Translation}} for {{Multilingual Open Information Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kolluru, Keshav and Mohammed, Muqeeth and Mittal, Shubham and Chakrabarti, Soumen and ., Mausam},
  year = {2022},
  month = may,
  pages = {2502--2517},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.179},
  url = {https://aclanthology.org/2022.acl-long.179},
  urldate = {2023-08-11},
  abstract = {Progress with supervised Open Information Extraction (OpenIE) has been primarily limited to English due to the scarcity of training data in other languages. In this paper, we explore techniques to automatically convert English text for training OpenIE systems in other languages. We introduce the Alignment-Augmented Constrained Translation (AACTrans) model to translate English sentences and their corresponding extractions consistently with each other \textemdash{} with no changes to vocabulary or semantic meaning which may result from independent translations. Using the data generated with AACTrans, we train a novel two-stage generative OpenIE model, which we call Gen2OIE, that outputs for each sentence: 1) relations in the first stage and 2) all extractions containing the relation in the second stage. Gen2OIE increases relation coverage using a training data transformation technique that is generalizable to multiple languages, in contrast to existing models that use an English-specific training loss. Evaluations on 5 languages \textemdash{} Spanish, Portuguese, Chinese, Hindi and Telugu \textemdash{} show that the Gen2OIE with AACTrans data outperforms prior systems by a margin of 6-25\% in F1.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Kolluru2022AlignmentAugmentedConsistentTranslation_Alignment-Augmented Consistent Translation for Multilingual Open Information.pdf}
}

@misc{Korbak2023PretrainingLanguageModels,
  title = {Pretraining {{Language Models}} with {{Human Preferences}}},
  author = {Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika and Buckley, Christopher L. and Phang, Jason and Bowman, Samuel R. and Perez, Ethan},
  year = {2023},
  month = feb,
  number = {arXiv:2302.08582},
  eprint = {2302.08582},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.08582},
  url = {http://arxiv.org/abs/2302.08582},
  urldate = {2023-05-30},
  abstract = {Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Korbak2023PretrainingLanguageModels_Pretraining Language Models with Human Preferences.pdf;/Users/ma/Zotero/storage/VXQ5G9G3/2302.html}
}

@inproceedings{Kumar2020DataAugmentationUsing,
  title = {Data {{Augmentation}} Using {{Pre-trained Transformer Models}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Life-long Learning}} for {{Spoken Language Systems}}},
  author = {Kumar, Varun and Choudhary, Ashutosh and Cho, Eunah},
  year = {2020},
  month = dec,
  pages = {18--26},
  publisher = {{Association for Computational Linguistics}},
  address = {{Suzhou, China}},
  url = {https://aclanthology.org/2020.lifelongnlp-1.3},
  urldate = {2023-05-24},
  abstract = {Language model based pre-trained models such as BERT have provided significant gains across different NLP tasks. In this paper, we study different types of transformer based pre-trained models such as auto-regressive models (GPT-2), auto-encoder models (BERT), and seq2seq models (BART) for conditional data augmentation. We show that prepending the class labels to text sequences provides a simple yet effective way to condition the pre-trained models for data augmentation. Additionally, on three classification benchmarks, pre-trained Seq2Seq model outperforms other data augmentation methods in a low-resource setting. Further, we explore how different pre-trained model based data augmentation differs in-terms of data diversity, and how well such methods preserve the class-label information.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Kumar2020DataAugmentationUsing_Data Augmentation using Pre-trained Transformer Models.pdf}
}

@misc{Kundu2023SpecificGeneralPrinciples,
  title = {Specific versus {{General Principles}} for {{Constitutional AI}}},
  author = {Kundu, Sandipan and Bai, Yuntao and Kadavath, Saurav and Askell, Amanda and Callahan, Andrew and Chen, Anna and Goldie, Anna and Balwit, Avital and Mirhoseini, Azalia and McLean, Brayden and Olsson, Catherine and Evraets, Cassie and {Tran-Johnson}, Eli and Durmus, Esin and Perez, Ethan and Kernion, Jackson and Kerr, Jamie and Ndousse, Kamal and Nguyen, Karina and Elhage, Nelson and Cheng, Newton and Schiefer, Nicholas and DasSarma, Nova and Rausch, Oliver and Larson, Robin and Yang, Shannon and Kravec, Shauna and {Telleen-Lawton}, Timothy and Liao, Thomas I. and Henighan, Tom and Hume, Tristan and {Hatfield-Dodds}, Zac and Mindermann, S{\"o}ren and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13798},
  eprint = {2310.13798},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.13798},
  url = {http://arxiv.org/abs/2310.13798},
  urldate = {2023-10-25},
  abstract = {Human feedback can prevent overtly harmful utterances in conversational models, but may not automatically mitigate subtle problematic behaviors such as a stated desire for self-preservation or power. Constitutional AI offers an alternative, replacing human feedback with feedback from AI models conditioned only on a list of written principles. We find this approach effectively prevents the expression of such behaviors. The success of simple principles motivates us to ask: can models learn general ethical behaviors from only a single written principle? To test this, we run experiments using a principle roughly stated as "do what's best for humanity". We find that the largest dialogue models can generalize from this short constitution, resulting in harmless assistants with no stated interest in specific motivations like power. A general principle may thus partially avoid the need for a long list of constitutions targeting potentially harmful behaviors. However, more detailed constitutions still improve fine-grained control over specific types of harms. This suggests both general and specific principles have value for steering AI safely.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/FM4HRG6J/Kundu et al. - 2023 - Specific versus General Principles for Constitutio.pdf;/Users/ma/Zotero/storage/KFS7T39C/2310.html}
}

@misc{Kweon2023PubliclyShareableClinical,
  title = {Publicly {{Shareable Clinical Large Language Model Built}} on {{Synthetic Clinical Notes}}},
  author = {Kweon, Sunjun and Kim, Junu and Kim, Jiyoun and Im, Sujeong and Cho, Eunbyeol and Bae, Seongsu and Oh, Jungwoo and Lee, Gyubok and Moon, Jong Hak and You, Seng Chan and Baek, Seungjin and Han, Chang Hoon and Jung, Yoon Bin and Jo, Yohan and Choi, Edward},
  year = {2023},
  month = sep,
  number = {arXiv:2309.00237},
  eprint = {2309.00237},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2309.00237},
  urldate = {2023-10-17},
  abstract = {The development of large language models tailored for handling patients' clinical notes is often hindered by the limited accessibility and usability of these notes due to strict privacy regulations. To address these challenges, we first create synthetic large-scale clinical notes using publicly available case reports extracted from biomedical literature. We then use these synthetic notes to train our specialized clinical large language model, Asclepius. While Asclepius is trained on synthetic data, we assess its potential performance in real-world applications by evaluating it using real clinical notes. We benchmark Asclepius against several other large language models, including GPT-3.5-turbo and other open-source alternatives. To further validate our approach using synthetic notes, we also compare Asclepius with its variants trained on real clinical notes. Our findings convincingly demonstrate that synthetic clinical notes can serve as viable substitutes for real ones when constructing high-performing clinical language models. This conclusion is supported by detailed evaluations conducted by both GPT-4 and medical professionals. All resources including weights, codes, and data used in the development of Asclepius are made publicly accessible for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Kweon2023PubliclyShareableClinical_Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical.pdf;/Users/ma/Zotero/storage/BBA7GXT7/2309.html}
}

@inproceedings{Lai2022ImprovingCandidateRetrieval,
  title = {Improving {{Candidate Retrieval}} with {{Entity Profile Generation}} for {{Wikidata Entity Linking}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Lai, Tuan and Ji, Heng and Zhai, ChengXiang},
  year = {2022},
  month = may,
  pages = {3696--3711},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.292},
  url = {https://aclanthology.org/2022.findings-acl.292},
  urldate = {2023-05-01},
  abstract = {Entity linking (EL) is the task of linking entity mentions in a document to referent entities in a knowledge base (KB). Many previous studies focus on Wikipedia-derived KBs. There is little work on EL over Wikidata, even though it is the most extensive crowdsourced KB. The scale of Wikidata can open up many new real-world applications, but its massive number of entities also makes EL challenging. To effectively narrow down the search space, we propose a novel candidate retrieval paradigm based on entity profiling. Wikidata entities and their textual fields are first indexed into a text search engine (e.g., Elasticsearch). During inference, given a mention and its context, we use a sequence-to-sequence (seq2seq) model to generate the profile of the target entity, which consists of its title and description. We use the profile to query the indexed search engine to retrieve candidate entities. Our approach complements the traditional approach of using a Wikipedia anchor-text dictionary, enabling us to further design a highly effective hybrid method for candidate retrieval. Combined with a simple cross-attention reranker, our complete EL framework achieves state-of-the-art results on three Wikidata-based datasets and strong performance on TACKBP-2010.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lai2022ImprovingCandidateRetrieval_Improving Candidate Retrieval with Entity Profile Generation for Wikidata.pdf}
}

@article{Lai2023KEBLMKnowledgeEnhancedBiomedical,
  title = {{{KEBLM}}: {{Knowledge-Enhanced Biomedical Language Models}}},
  shorttitle = {{{KEBLM}}},
  author = {Lai, Tuan Manh and Zhai, ChengXiang and Ji, Heng},
  year = {2023},
  month = jul,
  journal = {Journal of Biomedical Informatics},
  volume = {143},
  pages = {104392},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2023.104392},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046423001132},
  urldate = {2023-10-27},
  abstract = {Pretrained language models (PLMs) have demonstrated strong performance on many natural language processing (NLP) tasks. Despite their great success, these PLMs are typically pretrained only on unstructured free texts without leveraging existing structured knowledge bases that are readily available for many domains, especially scientific domains. As a result, these PLMs may not achieve satisfactory performance on knowledge-intensive tasks such as biomedical NLP. Comprehending a complex biomedical document without domain-specific knowledge is challenging, even for humans. Inspired by this observation, we propose a general framework for incorporating various types of domain knowledge from multiple sources into biomedical PLMs. We encode domain knowledge using lightweight adapter modules, bottleneck feed-forward networks that are inserted into different locations of a backbone PLM. For each knowledge source of interest, we pretrain an adapter module to capture the knowledge in a self-supervised way. We design a wide range of self-supervised objectives to accommodate diverse types of knowledge, ranging from entity relations to description sentences. Once a set of pretrained adapters is available, we employ fusion layers to combine the knowledge encoded within these adapters for downstream tasks. Each fusion layer is a parameterized mixer of the available trained adapters that can identify and activate the most useful adapters for a given input. Our method diverges from prior work by including a knowledge consolidation phase, during which we teach the fusion layers to effectively combine knowledge from both the original PLM and newly-acquired external knowledge using a large collection of unannotated texts. After the consolidation phase, the complete knowledge-enhanced model can be fine-tuned for any downstream task of interest to achieve optimal performance. Extensive experiments on many biomedical NLP datasets show that our proposed framework consistently improves the performance of the underlying PLMs on various downstream tasks such as natural language inference, question answering, and entity linking. These results demonstrate the benefits of using multiple sources of external knowledge to enhance PLMs and the effectiveness of the framework for incorporating knowledge into PLMs. While primarily focused on the biomedical domain in this work, our framework is highly adaptable and can be easily applied to other domains, such as the bioenergy sector.},
  keywords = {Domain knowledge,Knowledge bases,Pre-trained language models},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lai2023KEBLMKnowledgeEnhancedBiomedicala_KEBLM.pdf}
}

@inproceedings{Lee2021NeuralClinicalEvent,
  title = {Neural {{Clinical Event Sequence Prediction Through Personalized Online Adaptive Learning}}},
  booktitle = {Artificial {{Intelligence}} in {{Medicine}}},
  author = {Lee, Jeong Min and Hauskrecht, Milos},
  editor = {Tucker, Allan and Henriques Abreu, Pedro and Cardoso, Jaime and Pereira Rodrigues, Pedro and Ria{\~n}o, David},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {175--186},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-77211-6_20},
  abstract = {Clinical event sequences consist of thousands of clinical events that represent records of patient care in time. Developing accurate prediction models for such sequences is of a great importance for defining representations of a patient state and for improving patient care. One important challenge of learning a good predictive model of clinical sequences is patient-specific variability. Based on underlying clinical complications, each patient's sequence may consist of different sets of clinical events. However, population-based models learned from such sequences may not accurately predict patient-specific dynamics of event sequences. To address the problem, we develop a new adaptive event sequence prediction framework that learns to adjust its prediction for individual patients through an online model update.},
  isbn = {978-3-030-77211-6},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lee2021NeuralClinicalEvent_Neural Clinical Event Sequence Prediction Through Personalized Online Adaptive.pdf}
}

@article{LeeFactualityEnhancedLanguage,
  title = {Factuality {{Enhanced Language Models}} for {{Open-Ended Text Generation}}},
  author = {Lee, Nayeon and Ping, Wei and Xu, Peng and Patwary, Mostofa and Fung, Pascale and Shoeybi, Mohammad and Catanzaro, Bryan},
  abstract = {Pretrained language models (LMs) are susceptible to generate text with nonfactual information. In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation. We design the FACTUALITYPROMPTS test set and metrics to measure the factuality of LM generations. Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B. Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions. In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ``uniform randomness'' introduced at every sampling step. We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia). We propose a factuality-enhanced training method that uses TOPICPREFIX for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/AC6VKMPH/Lee et al. - Factuality Enhanced Language Models for Open-Ended.pdf}
}

@inproceedings{Leviathan2023FastInferenceTransformers,
  title = {Fast {{Inference}} from {{Transformers}} via {{Speculative Decoding}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  year = {2023},
  month = jul,
  pages = {19274--19286},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/leviathan23a.html},
  urldate = {2023-08-21},
  abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Leviathan2023FastInferenceTransformers_Fast Inference from Transformers via Speculative Decoding.pdf}
}

@inproceedings{Lewis2020BARTDenoisingSequencetoSequence,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  month = jul,
  pages = {7871--7880},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.703},
  url = {https://aclanthology.org/2020.acl-main.703},
  urldate = {2023-04-29},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lewis2020BARTDenoisingSequencetoSequence_BART.pdf}
}

@inproceedings{Lewis2020BARTDenoisingSequencetoSequencea,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  month = jul,
  pages = {7871--7880},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.703},
  url = {https://aclanthology.org/2020.acl-main.703},
  urldate = {2023-05-24},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lewis2020BARTDenoisingSequencetoSequencea_BART.pdf}
}

@misc{Lewis2021RetrievalAugmentedGenerationKnowledgeIntensive,
  title = {Retrieval-{{Augmented Generation}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  year = {2021},
  month = apr,
  number = {arXiv:2005.11401},
  eprint = {2005.11401},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.11401},
  url = {http://arxiv.org/abs/2005.11401},
  urldate = {2023-10-30},
  abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lewis2021RetrievalAugmentedGenerationKnowledgeIntensive_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf;/Users/ma/Zotero/storage/YZDLWNQL/2005.html}
}

@inproceedings{Li2020LogicguidedSemanticRepresentation,
  title = {Logic-Guided {{Semantic Representation Learning}} for {{Zero-Shot Relation Classification}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Li, Juan and Wang, Ruoxu and Zhang, Ningyu and Zhang, Wen and Yang, Fan and Chen, Huajun},
  year = {2020},
  month = dec,
  pages = {2967--2978},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.265},
  url = {https://aclanthology.org/2020.coling-main.265},
  urldate = {2023-05-01},
  abstract = {Relation classification aims to extract semantic relations between entity pairs from the sentences. However, most existing methods can only identify seen relation classes that occurred during training. To recognize unseen relations at test time, we explore the problem of zero-shot relation classification. Previous work regards the problem as reading comprehension or textual entailment, which have to rely on artificial descriptive information to improve the understandability of relation types. Thus, rich semantic knowledge of the relation labels is ignored. In this paper, we propose a novel logic-guided semantic representation learning model for zero-shot relation classification. Our approach builds connections between seen and unseen relations via implicit and explicit semantic representations with knowledge graph embeddings and logic rules. Extensive experimental results demonstrate that our method can generalize to unseen relation types and achieve promising improvements.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2020LogicguidedSemanticRepresentation_Logic-guided Semantic Representation Learning for Zero-Shot Relation.pdf}
}

@inproceedings{Li2020TrainBigThen,
  title = {Train {{Big}}, {{Then Compress}}: {{Rethinking Model Size}} for {{Efficient Training}} and {{Inference}} of {{Transformers}}},
  shorttitle = {Train {{Big}}, {{Then Compress}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer, Kurt and Klein, Dan and Gonzalez, Joey},
  year = {2020},
  month = nov,
  pages = {5958--5968},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/li20m.html},
  urldate = {2023-04-06},
  abstract = {Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2020TrainBigThen_Train Big, Then Compress.pdf;/Users/ma/Zotero/storage/XSZEN4KS/Li et al. - 2020 - Train Big, Then Compress Rethinking Model Size fo.pdf}
}

@inproceedings{Li2021DocumentLevelEventArgument,
  title = {Document-{{Level Event Argument Extraction}} by {{Conditional Generation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Li, Sha and Ji, Heng and Han, Jiawei},
  year = {2021},
  month = jun,
  pages = {894--908},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.69},
  url = {https://aclanthology.org/2021.naacl-main.69},
  urldate = {2023-02-09},
  abstract = {Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6\% F1 and 5.7\% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3\% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97\% of fully supervised model's trigger extraction performance and 82\% of the argument extraction performance given only access to 10 out of the 33 types on ACE.},
  keywords = {dataset proposal,dataset\_WikiEvents,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2021DocumentLevelEventArgument_Document-Level Event Argument Extraction by Conditional Generation.pdf}
}

@misc{Li2022ContrastiveDecodingOpenended,
  title = {Contrastive {{Decoding}}: {{Open-ended Text Generation}} as {{Optimization}}},
  shorttitle = {Contrastive {{Decoding}}},
  author = {Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15097},
  eprint = {2210.15097},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.15097},
  urldate = {2023-02-07},
  abstract = {Likelihood, although useful as a training loss, is a poor search objective for guiding open-ended generation from language models (LMs). Existing generation algorithms must avoid both unlikely strings, which are incoherent, and highly likely ones, which are short and repetitive. We propose contrastive decoding (CD), a more reliable search objective that returns the difference between likelihood under a large LM (called the expert, e.g. OPT-13b) and a small LM (called the amateur, e.g. OPT-125m). CD is inspired by the fact that the failures of larger LMs are even more prevalent in smaller LMs, and that this difference signals exactly which texts should be preferred. CD requires zero training, and produces higher quality text than decoding from the larger LM alone. It also generalizes across model types (OPT and GPT2) and significantly outperforms four strong decoding algorithms in automatic and human evaluations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2022ContrastiveDecodingOpenended_Contrastive Decoding.pdf;/Users/ma/Zotero/storage/44PB97R8/2210.html}
}

@article{Li2022EmotionAnalyticThinking,
  title = {Emotion, Analytic Thinking and Susceptibility to Misinformation during the {{COVID-19}} Outbreak},
  author = {Li, Ming-Hui and Chen, Zhiqin and Rao, Li-Lin},
  year = {2022},
  month = aug,
  journal = {Computers in Human Behavior},
  volume = {133},
  pages = {107295},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2022.107295},
  abstract = {Misinformation has become prevalent since the beginning of the COVID-19 pandemic. To understand why people believe and share misinformation, we conducted a nationwide survey during the COVID-19 outbreak in China. We found the indirect effects of COVID-19 risk on people's information accuracy judgment and associated information sharing intention through people's emotional states. People faced with a higher level of COVID-19 risk (measured by a 7-day moving average of daily new deaths or new cases) experienced weaker positive and stronger negative emotions, and heightened emotionality (both the positive and negative emotions) was associated with increased belief in and greater likelihood to share the COVID-19 information regardless of veracity. We also found that only the negative emotion mediated the relation between the COVID-19 risk and the truth discernment regarding accuracy judgment. However, the mediating effect of negative emotion disappeared among people with high analytic thinking ability. These findings suggest that the analytic thinking ability could moderate the destructive relationship between negative emotion and accuracy discernment. Based on a large sample, our findings provide actionable insights for the policymakers to respond to the spread of misinformation appropriately and promptly during the pandemic.},
  langid = {english},
  pmcid = {PMC8991995},
  pmid = {35431427},
  keywords = {Analytic thinking,COVID-19,Emotion,Misinformation},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2022EmotionAnalyticThinking_Emotion, analytic thinking and susceptibility to misinformation during the.pdf}
}

@misc{Li2022LargeLanguageModels,
  title = {Large {{Language Models Can Be Strong Differentially Private Learners}}},
  author = {Li, Xuechen and Tram{\`e}r, Florian and Liang, Percy and Hashimoto, Tatsunori},
  year = {2022},
  month = nov,
  number = {arXiv:2110.05679},
  eprint = {2110.05679},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2110.05679},
  urldate = {2023-03-22},
  abstract = {Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models doesn't tend to suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2022LargeLanguageModels_Large Language Models Can Be Strong Differentially Private Learners.pdf;/Users/ma/Zotero/storage/JRA3AQGW/2110.html}
}

@misc{Li2022SpaBERTPretrainedLanguage,
  title = {{{SpaBERT}}: {{A Pretrained Language Model}} from {{Geographic Data}} for {{Geo-Entity Representation}}},
  shorttitle = {{{SpaBERT}}},
  author = {Li, Zekun and Kim, Jina and Chiang, Yao-Yi and Chen, Muhao},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12213},
  eprint = {2210.12213},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.12213},
  urldate = {2023-02-01},
  abstract = {Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an entity. We hypothesize that we shall know the characteristics of a geo-entity by its surrounding entities, similar to knowing word meanings by their linguistic context. Accordingly, we propose a novel spatial language model, SpaBERT, which provides a general-purpose geo-entity representation based on neighboring entities in geospatial data. SpaBERT extends BERT to capture linearized spatial context, while incorporating a spatial coordinate embedding mechanism to preserve spatial relations of entities in the 2-dimensional space. SpaBERT is pretrained with masked language modeling and masked entity prediction tasks to learn spatial dependencies. We apply SpaBERT to two downstream tasks: geo-entity typing and geo-entity linking. Compared with the existing language models that do not use spatial context, SpaBERT shows significant performance improvement on both tasks. We also analyze the entity representation from SpaBERT in various settings and the effect of spatial coordinate embedding.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2022SpaBERTPretrainedLanguage_SpaBERT.pdf;/Users/ma/Zotero/storage/2GW76AMR/2210.html}
}

@inproceedings{Li2022SpaBERTPretrainedLanguagea,
  title = {{{SpaBERT}}: {{A Pretrained Language Model}} from {{Geographic Data}} for {{Geo-Entity Representation}}},
  shorttitle = {{{SpaBERT}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Li, Zekun and Kim, Jina and Chiang, Yao-Yi and Chen, Muhao},
  year = {2022},
  month = dec,
  pages = {2757--2769},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  doi = {10.18653/v1/2022.findings-emnlp.200},
  url = {https://aclanthology.org/2022.findings-emnlp.200},
  urldate = {2023-10-20},
  abstract = {Named geographic entities (geo-entities for short) are the building blocks of many geographic datasets. Characterizing geo-entities is integral to various application domains, such as geo-intelligence and map comprehension, while a key challenge is to capture the spatial-varying context of an entity. We hypothesize that we shall know the characteristics of a geo-entity by its surrounding entities, similar to knowing word meanings by their linguistic context. Accordingly, we propose a novel spatial language model, SpaBERT, which provides a general-purpose geo-entity representation based on neighboring entities in geospatial data. SpaBERT extends BERT to capture linearized spatial context, while incorporating a spatial coordinate embedding mechanism to preserve spatial relations of entities in the 2-dimensional space. SpaBERT is pretrained with masked language modeling and masked entity prediction tasks to learn spatial dependencies. We apply SpaBERT to two downstream tasks: geo-entity typing and geo-entity linking. Compared with the existing language models that do not use spatial context, SpaBERT shows significant performance improvement on both tasks. We also analyze the entity representation from SpaBERT in various settings and the effect of spatial coordinate embedding.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2022SpaBERTPretrainedLanguagea_SpaBERT.pdf}
}

@inproceedings{Li2022UnifiedFineGrainedBiomedical,
  title = {Unified {{Fine-Grained Biomedical Entity Recognition}} as a {{Combination}} of {{Boundary Detection}} and {{Sequence Generation}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Li, Xue and Yang, Yang and Ye, Mingchen and Guan, Yi and Yu, Xuehui and Jiang, Jingchi},
  year = {2022},
  month = dec,
  pages = {483--490},
  doi = {10.1109/BIBM55620.2022.9995683},
  abstract = {Biomedical Named Entity Recognition (BioNER) is a critical component of biomedical information extraction. NER is more challenging in the biomedical domain because of fine-grained entity types and more common nested and discontinuous entity forms. However, none of the BioNER datasets contains a large amount of all three entity forms, including flat, nested, and discontinuous. Not to mention that there is a unified BioNER model for dealing with the above three entity forms simultaneously. Methods in the public domain only focus on identifying text spans and ignore distinguishing fine-grained entity types. To address these issues, we propose a unified framework based on our own BioNER dataset CCNER, which innovatively models the BioNER task as a combination of boundary recognition and sequence generation. CCNER is a comprehensive and fine-grained BioNER dataset, where the proportion of discontinuous, nested, and flat entities in the dataset is 8.9\%, 52.6\%, and 38.5\%, respectively. Meanwhile, it includes five fine-grained entity types. Our proposed framework includes two modules which are boundary detection and entity generation. In the boundary detection module, we propose a sample-based span representation method to determine fine-grained entity boundaries better. Finally, we conduct experiments on four datasets and achieve competitive results1.1Code is available at https://github.com/lx-hit/BioNER.},
  keywords = {Bioinformatics,Biological system modeling,biomedical named entity recognition,comprehensive biomedical corpus,discontinuous entity,fine-grained entity types,Information retrieval,nested entity,Task analysis},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2022UnifiedFineGrainedBiomedical_Unified Fine-Grained Biomedical Entity Recognition as a Combination of Boundary.pdf;/Users/ma/Zotero/storage/IQFCXC6A/9995683.html}
}

@misc{Li2023EvaluatingChatGPTInformation,
  title = {Evaluating {{ChatGPT}}'s {{Information Extraction Capabilities}}: {{An Assessment}} of {{Performance}}, {{Explainability}}, {{Calibration}}, and {{Faithfulness}}},
  shorttitle = {Evaluating {{ChatGPT}}'s {{Information Extraction Capabilities}}},
  author = {Li, Bo and Fang, Gexiang and Yang, Yang and Wang, Quansen and Ye, Wei and Zhao, Wen and Zhang, Shikun},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11633},
  eprint = {2304.11633},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2304.11633},
  urldate = {2023-04-25},
  abstract = {The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT\_for\_IE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2023EvaluatingChatGPTInformation_Evaluating ChatGPT's Information Extraction Capabilities.pdf;/Users/ma/Zotero/storage/CLH9ZQIB/2304.html}
}

@article{Li2023HGV4RiskHierarchicalGlobal,
  title = {{{HGV4Risk}}: {{Hierarchical Global View-guided Sequence Representation Learning}} for {{Risk Prediction}}},
  shorttitle = {{{HGV4Risk}}},
  author = {Li, Youru and Zhu, Zhenfeng and Guo, Xiaobo and Li, Shaoshuai and Yang, Yuchen and Zhao, Yao},
  year = {2023},
  month = aug,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {18},
  number = {1},
  pages = {1:1--1:21},
  issn = {1556-4681},
  doi = {10.1145/3605895},
  url = {https://dl.acm.org/doi/10.1145/3605895},
  urldate = {2023-10-31},
  abstract = {Risk prediction, usually achieved by learning representations from patient's physiological sequence or user's behavioral sequence data, and has been widely applied in healthcare and finance. Despite that, some recent time-aware deep learning methods have led to superior performances in such sequence representation learning tasks, such improvement is limited due to a lack of guidance from hierarchical global view. To address this issue, we propose a novel end-to-end Hierarchical Global View-guided (HGV) sequence representation learning framework. Specifically, the Global Graph Embedding (GGE) module is proposed to learn sequential clip-aware representations from temporal correlation graph (TCG) at instance level. Furthermore, following the way of key-query attention, the harmonic {$\beta$}-attention ({$\beta$}-Attn) is also developed for making a global tradeoff between time-aware decay and observation significance at channel level adaptively. Moreover, the hierarchical representations at both instance level and channel level can be coordinated by the heterogeneous information aggregation under the guidance of global view. Experimental results on both healthcare risk prediction benchmark and SMEs credit overdue risk prediction task from the real-world industrial scenario in MYBank, Ant Group, have illustrated that the proposed model can achieve competitive prediction performance compared with other known baselines. The code has been released public available at: https://github.com/LiYouru0228/HGV.},
  keywords = {Deep learning,risk prediction,sequence representation learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2023HGV4RiskHierarchicalGlobal_HGV4Risk.pdf}
}

@inproceedings{Li2023MLGANMetaLearningBased,
  title = {{{MLGAN}}: A {{Meta-Learning}} Based {{Generative Adversarial Network}} Adapter for Rare Disease Differentiation Tasks},
  shorttitle = {{{MLGAN}}},
  booktitle = {Proceedings of the 14th {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}}, and {{Health Informatics}}},
  author = {Li, Rui and Wen, Andrew and Gao, Jing and Liu, Hongfang},
  year = {2023},
  month = oct,
  series = {{{BCB}} '23},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3584371.3612967},
  url = {https://dl.acm.org/doi/10.1145/3584371.3612967},
  urldate = {2023-10-31},
  abstract = {Rare disease diagnosis is very challenging due to the rarity and lack of scientific knowledge. Many patients with rare diseases take years to get diagnosed and many stay misdiagnosed or are not diagnosed. Comparing with traditional diagnosis prediction task, rare disease detection has the unique challenges of low prevalence and label noise. In this paper, we propose Meta-Learning based Generative Adversarial Network module MLGAN, a rare disease detection enhancement module that can adapt any existing diagnosis prediction methods to rare disease detection task. We use generative adversarial network to generate synthetic positive em-beddings and we use Meta-Weight-Net to automatically assign weight to real data and synthetic data. MLGAN helps us to leverage the time-aware sequential modeling ability in diagnosis prediction methods, and also mitigate the low prevalence and label noise of rare disease dataset. We empirically show that MLGAN can greatly boost the prediction performance and have good robustness on four real-world rare disease datasets. We release our code at https://github.com/ruilialice/MLGAN.},
  isbn = {9798400701269},
  keywords = {generative adversarial networks,meta learning,rare disease diagnosis},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2023MLGANMetaLearningBased_MLGAN.pdf}
}

@inproceedings{Li2023OpenDomainHierarchicalEvent,
  title = {Open-{{Domain Hierarchical Event Schema Induction}} by {{Incremental Prompting}} and {{Verification}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Sha and Zhao, Ruining and Li, Manling and Ji, Heng and {Callison-Burch}, Chris and Han, Jiawei},
  year = {2023},
  pages = {5677--5697},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.312},
  url = {https://aclanthology.org/2023.acl-long.312},
  urldate = {2023-08-11},
  abstract = {Event schemas are a form of world knowledge about the typical progression of events. Recent methods for event schema induction use information extraction systems to construct a large number of event graph instances from documents, and then learn to generalize the schema from such instances. In contrast, we propose to treat event schemas as a form of commonsense knowledge that can be derived from large language models (LLMs). This new paradigm greatly simplifies the schema induction process and allows us to handle both hierarchical relations and temporal relations between events in a straightforward way. Since event schemas have complex graph structures, we design an incremental prompting and verification method INCSCHEMA to break down the construction of a complex event graph into three stages: event skeleton construction, event expansion, and event-event relation verification. Compared to directly using LLMs to generate a linearized graph, INCSCHEMA can generate large and complex schemas with 7.2\% F1 improvement in temporal relations and 31.0\% F1 improvement in hierarchical relations. In addition, compared to the previous state-of-the-art closed-domain schema induction model, human assessors were able to cover {$\sim$}10\% more events when translating the schemas into coherent stories and rated our schemas 1.3 points higher (on a 5-point scale) in terms of readability.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/BH36ULHU/Li et al. - 2023 - Open-Domain Hierarchical Event Schema Induction by.pdf}
}

@article{Li2023PLMmarkSecureRobust,
  title = {{{PLMmark}}: {{A Secure}} and {{Robust Black-Box Watermarking Framework}} for {{Pre-trained Language Models}}},
  shorttitle = {{{PLMmark}}},
  author = {Li, Peixuan and Cheng, Pengzhou and Li, Fangqi and Du, Wei and Zhao, Haodong and Liu, Gongshen},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {12},
  pages = {14991--14999},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i12.26750},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/26750},
  urldate = {2023-09-15},
  abstract = {The huge training overhead, considerable commercial value, and various potential security risks make it urgent to protect the intellectual property (IP) of Deep Neural Networks (DNNs). DNN watermarking has become a plausible method to meet this need. However, most of the existing watermarking schemes focus on image classification tasks. The schemes designed for the textual domain lack security and reliability. Moreover, how to protect the IP of widely-used pre-trained language models (PLMs) remains a blank.  To fill these gaps, we propose PLMmark, the first secure and robust black-box watermarking framework for PLMs. It consists of three phases: (1) In order to generate watermarks that contain owners' identity information, we propose a novel encoding method to establish a strong link between a digital signature and trigger words by leveraging the original vocabulary tables of PLMs. Combining this with public key cryptography ensures the security of our scheme. (2) To embed robust, task-agnostic, and highly transferable watermarks in PLMs, we introduce a supervised contrastive loss to deviate the output representations of trigger sets from that of clean samples. In this way, the watermarked models will respond to the trigger sets anomaly and thus can identify the ownership. (3) To make the model ownership verification results reliable, we perform double verification, which guarantees the unforgeability of ownership. Extensive experiments on text classification tasks demonstrate that the embedded watermark can transfer to all the downstream tasks and can be effectively extracted and verified. The watermarking scheme is robust to watermark removing attacks (fine-pruning and re-initializing) and is secure enough to resist forgery attacks.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {General},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Li2023PLMmarkSecureRobust_PLMmark.pdf}
}

@inproceedings{Liang2022RAATRelationAugmentedAttention,
  title = {{{RAAT}}: {{Relation-Augmented Attention Transformer}} for {{Relation Modeling}} in {{Document-Level Event Extraction}}},
  shorttitle = {{{RAAT}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Liang, Yuan and Jiang, Zhuoxuan and Yin, Di and Ren, Bo},
  year = {2022},
  month = jul,
  pages = {4985--4997},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.367},
  url = {https://aclanthology.org/2022.naacl-main.367},
  urldate = {2023-02-20},
  abstract = {In document-level event extraction (DEE) task, event arguments always scatter across sentences (across-sentence issue) and multipleevents may lie in one document (multi-event issue). In this paper, we argue that the relation information of event arguments is of greatsignificance for addressing the above two issues, and propose a new DEE framework which can model the relation dependencies, calledRelation-augmented Document-level Event Extraction (ReDEE). More specifically, this framework features a novel and tailored transformer,named as Relation-augmented Attention Transformer (RAAT). RAAT is scalable to capture multi-scale and multi-amount argument relations. To further leverage relation information, we introduce a separate event relation prediction task and adopt multi-task learning method to explicitly enhance event extraction performance. Extensive experiments demonstrate the effectiveness of the proposed method, which can achieve state-of-the-art performance on two public datasets.Our code is available at https://github.com/TencentYoutuResearch/RAAT.},
  keywords = {dataset\_Doc2EDAG,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liang2022RAATRelationAugmentedAttention_RAAT.pdf}
}

@article{Liao2020PublicEngagementGovernment,
  title = {Public {{Engagement}} and {{Government Responsiveness}} in the {{Communications About COVID-19 During}} the {{Early Epidemic Stage}} in {{China}}: {{Infodemiology Study}} on {{Social Media Data}}},
  shorttitle = {Public {{Engagement}} and {{Government Responsiveness}} in the {{Communications About COVID-19 During}} the {{Early Epidemic Stage}} in {{China}}},
  author = {Liao, Qiuyan and Yuan, Jiehu and Dong, Meihong and Yang, Lin and Fielding, Richard and Lam, Wendy Wing Tak},
  year = {2020},
  month = may,
  journal = {Journal of Medical Internet Research},
  volume = {22},
  number = {5},
  pages = {e18796},
  issn = {1438-8871},
  doi = {10.2196/18796},
  url = {http://www.jmir.org/2020/5/e18796/},
  urldate = {2023-10-15},
  abstract = {Background: Effective risk communication about the outbreak of a newly emerging infectious disease in the early stage is critical for managing public anxiety and promoting behavioral compliance. China has experienced the unprecedented epidemic of the coronavirus disease (COVID-19) in an era when social media has fundamentally transformed information production and consumption patterns.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/UVXYWJ77/Liao et al. - 2020 - Public Engagement and Government Responsiveness in.pdf}
}

@article{Liao2020PublicEngagementGovernmenta,
  title = {Public {{Engagement}} and {{Government Responsiveness}} in the {{Communications About COVID-19 During}} the {{Early Epidemic Stage}} in {{China}}: {{Infodemiology Study}} on {{Social Media Data}}},
  shorttitle = {Public {{Engagement}} and {{Government Responsiveness}} in the {{Communications About COVID-19 During}} the {{Early Epidemic Stage}} in {{China}}},
  author = {Liao, Qiuyan and Yuan, Jiehu and Dong, Meihong and Yang, Lin and Fielding, Richard and Lam, Wendy Wing Tak},
  year = {2020},
  month = may,
  journal = {Journal of Medical Internet Research},
  volume = {22},
  number = {5},
  pages = {e18796},
  issn = {1438-8871},
  doi = {10.2196/18796},
  url = {http://www.jmir.org/2020/5/e18796/},
  urldate = {2023-10-15},
  abstract = {Background: Effective risk communication about the outbreak of a newly emerging infectious disease in the early stage is critical for managing public anxiety and promoting behavioral compliance. China has experienced the unprecedented epidemic of the coronavirus disease (COVID-19) in an era when social media has fundamentally transformed information production and consumption patterns.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/G5WIS2LS/Liao et al. - 2020 - Public Engagement and Government Responsiveness in.pdf}
}

@article{LiChatGPTAttackTool,
  title = {{{ChatGPT}} as an {{Attack Tool}}: {{Stealthy Textual Backdoor Attack}} via {{Blackbox Generative Model Trigger}}},
  author = {Li, Jiazhao and Yang, Yijin and Wu, Zhuofeng and Vydiswaran, V G Vinod and Xiao, Chaowei},
  abstract = {Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, BGMAttack, could effectively deceive textual classifiers. Compared with the traditional attack methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that BGMAttack achieves comparable attack performance while maintaining superior stealthiness relative to baseline methods.},
  langid = {english},
  keywords = {data generation},
  file = {/Users/ma/Zotero/storage/KLKKB5T9/Li et al. - ChatGPT as an Attack Tool Stealthy Textual Backdo.pdf}
}

@article{LiDefiningNewNLP,
  title = {Defining a {{New NLP Playground}}},
  author = {Li, Sha and Han, Chi and Yu, Pengfei and Edwards, Carl and Li, Manling and Wang, Xingyao and Fung, Yi R and Yu, Charles and Tetreault, Joel R and Hovy, Eduard H and Ji, Heng},
  abstract = {The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.},
  langid = {english},
  keywords = {i1},
  file = {/Users/ma/Zotero/storage/WP3LJJQI/Li et al. - Defining a New NLP Playground.pdf}
}

@article{LiGeoLMEmpoweringLanguage,
  title = {{{GeoLM}}: {{Empowering Language Models}} for {{Geospatially Grounded Language Understanding}}},
  author = {Li, Zekun and Zhou, Wenxuan and Chiang, Yao-Yi and Chen, Muhao},
  abstract = {Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GEOLM ( ), a geospatially grounded language model aiming to enhance the understanding of geo-entities in natural language. GEOLM leverages geo-entity mentions as anchors to connect linguistic information in text corpora with geospatial information extracted from geographical databases. GEOLM connects the two types of contexts through contrastive learning and masked language modeling. It also incorporates a spatial coordinate embedding mechanism to encode distance and direction relations for capturing geospatial context. In the experiment, we demonstrate that GEOLM exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and spatial sciences. The code is publicly available at https://github.com/zekun-li/geolm.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/MAMCRHNL/Li et al. - GeoLM Empowering Language Models for Geospatially.pdf}
}

@inproceedings{Lin2020JointNeuralModel,
  title = {A {{Joint Neural Model}} for {{Information Extraction}} with {{Global Features}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lin, Ying and Ji, Heng and Huang, Fei and Wu, Lingfei},
  year = {2020},
  month = jul,
  pages = {7999--8009},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.713},
  url = {https://aclanthology.org/2020.acl-main.713},
  urldate = {2023-05-01},
  abstract = {Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lin2020JointNeuralModel_A Joint Neural Model for Information Extraction with Global Features.pdf}
}

@inproceedings{Lin2022CUPCurriculumLearning,
  title = {{{CUP}}: {{Curriculum Learning}} Based {{Prompt Tuning}} for {{Implicit Event Argument Extraction}}},
  shorttitle = {{{CUP}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lin, Jiaju and Chen, Qin and Zhou, Jie and Jin, Jian and He, Liang},
  year = {2022},
  month = jul,
  pages = {4245--4251},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Vienna, Austria}},
  doi = {10.24963/ijcai.2022/589},
  url = {https://www.ijcai.org/proceedings/2022/589},
  urldate = {2023-02-09},
  abstract = {Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.},
  isbn = {978-1-956792-00-3},
  langid = {english},
  keywords = {dataset\_RAMS,dataset\_WikiEvents,document-level event extraction},
  file = {/Users/ma/Zotero/storage/VKPGFQFH/Lin et al. - 2022 - CUP Curriculum Learning based Prompt Tuning for I.pdf}
}

@misc{Liu2019RoBERTaRobustlyOptimized,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.11692},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2023-05-24},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2019RoBERTaRobustlyOptimized_RoBERTa.pdf;/Users/ma/Zotero/storage/EE6429J7/1907.html}
}

@inproceedings{Liu2021MachineReadingComprehension,
  title = {Machine {{Reading Comprehension}} as {{Data Augmentation}}: {{A Case Study}} on {{Implicit Event Argument Extraction}}},
  shorttitle = {Machine {{Reading Comprehension}} as {{Data Augmentation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Liu, Jian and Chen, Yufeng and Xu, Jinan},
  year = {2021},
  month = nov,
  pages = {2716--2725},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.214},
  url = {https://aclanthology.org/2021.emnlp-main.214},
  urldate = {2023-02-09},
  abstract = {Implicit event argument extraction (EAE) is a crucial document-level information extraction task that aims to identify event arguments beyond the sentence level. Despite many efforts for this task, the lack of enough training data has long impeded the study. In this paper, we take a new perspective to address the data sparsity issue faced by implicit EAE, by bridging the task with machine reading comprehension (MRC). Particularly, we devise two data augmentation regimes via MRC, including: 1) implicit knowledge transfer, which enables knowledge transfer from other tasks, by building a unified training framework in the MRC formulation, and 2) explicit data augmentation, which can explicitly generate new training examples, by treating MRC models as an annotator. The extensive experiments have justified the effectiveness of our approach \textemdash{} it not only obtains state-of-the-art performance on two benchmarks, but also demonstrates superior results in a data-low scenario.},
  keywords = {dataset\_RAMS,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2021MachineReadingComprehension_Machine Reading Comprehension as Data Augmentation.pdf}
}

@inproceedings{Liu2021NoisyLabeledNERConfidence,
  title = {Noisy-{{Labeled NER}} with {{Confidence Estimation}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Liu, Kun and Fu, Yao and Tan, Chuanqi and Chen, Mosha and Zhang, Ningyu and Huang, Songfang and Gao, Sheng},
  year = {2021},
  month = jun,
  pages = {3437--3445},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.269},
  url = {https://aclanthology.org/2021.naacl-main.269},
  urldate = {2023-05-01},
  abstract = {Recent studies in deep learning have shown significant progress in named entity recognition (NER). However, most existing works assume clean data annotation, while real-world scenarios typically involve a large amount of noises from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose strategies for estimating confidence scores based on local and global independence assumptions. We partially marginalize out labels of low confidence with a CRF model. We further propose a calibration method for confidence scores based on the structure of entity labels. We integrate our approach into a self-training framework for boosting performance. Experiments in general noisy settings with four languages and distantly labeled settings demonstrate the effectiveness of our method.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2021NoisyLabeledNERConfidence_Noisy-Labeled NER with Confidence Estimation.pdf}
}

@inproceedings{Liu2022DynamicPrefixTuningGenerative,
  title = {Dynamic {{Prefix-Tuning}} for {{Generative Template-based Event Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Liu, Xiao and Huang, Heyan and Shi, Ge and Wang, Bo},
  year = {2022},
  month = may,
  pages = {5216--5228},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.358},
  url = {https://aclanthology.org/2022.acl-long.358},
  urldate = {2023-05-01},
  abstract = {We consider event extraction in a generative manner with template-based conditional generation.Although there is a rising trend of casting the task of event extraction as a sequence generation problem with prompts, these generation-based methods have two significant challenges, including using suboptimal prompts and static event type information.In this paper, we propose a generative template-based event extraction method with dynamic prefix (GTEE-DynPref) by integrating context information with type-specific prefixes to learn a context-specific prefix for each context.Experimental results show that our model achieves competitive results with the state-of-the-art classification-based model OneIE on ACE 2005 and achieves the best performances on ERE.Additionally, our model is proven to be portable to new types of events effectively.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2022DynamicPrefixTuningGenerative_Dynamic Prefix-Tuning for Generative Template-based Event Extraction.pdf}
}

@misc{Liu2023ChainHindsightAligns,
  title = {Chain of {{Hindsight Aligns Language Models}} with {{Feedback}}},
  author = {Liu, Hao and Sferrazza, Carmelo and Abbeel, Pieter},
  year = {2023},
  month = mar,
  number = {arXiv:2302.02676},
  eprint = {2302.02676},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.02676},
  urldate = {2023-03-08},
  abstract = {Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, models are trained to generate outputs based on feedback, and models can learn to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We observed significant improvements on summarization and dialogue tasks and our approach is markedly preferred in human evaluations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,RLHF},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2023ChainHindsightAligns_Chain of Hindsight Aligns Language Models with Feedback.pdf;/Users/ma/Zotero/storage/JZKJABR3/2302.html}
}

@misc{Liu2023JailbreakingChatGPTPrompt,
  title = {Jailbreaking {{ChatGPT}} via {{Prompt Engineering}}: {{An Empirical Study}}},
  shorttitle = {Jailbreaking {{ChatGPT}} via {{Prompt Engineering}}},
  author = {Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  year = {2023},
  month = may,
  number = {arXiv:2305.13860},
  eprint = {2305.13860},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.13860},
  url = {http://arxiv.org/abs/2305.13860},
  urldate = {2023-10-27},
  abstract = {Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential but also introduce challenges related to content constraints and potential misuse. Our study investigates three key research questions: (1) the number of different prompt types that can jailbreak LLMs, (2) the effectiveness of jailbreak prompts in circumventing LLM constraints, and (3) the resilience of ChatGPT against these jailbreak prompts. Initially, we develop a classification model to analyze the distribution of existing prompts, identifying ten distinct patterns and three categories of jailbreak prompts. Subsequently, we assess the jailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a dataset of 3,120 jailbreak questions across eight prohibited scenarios. Finally, we evaluate the resistance of ChatGPT against jailbreak prompts, finding that the prompts can consistently evade the restrictions in 40 use-case scenarios. The study underscores the importance of prompt structures in jailbreaking LLMs and discusses the challenges of robust jailbreak prompt generation and prevention.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2023JailbreakingChatGPTPrompt_Jailbreaking ChatGPT via Prompt Engineering.pdf;/Users/ma/Zotero/storage/ZWUB8MNH/2305.html}
}

@misc{Liu2023TextguidedProteinDesign,
  title = {A {{Text-guided Protein Design Framework}}},
  author = {Liu, Shengchao and Zhu, Yutao and Lu, Jiarui and Xu, Zhao and Nie, Weili and Gitter, Anthony and Xiao, Chaowei and Tang, Jian and Guo, Hongyu and Anandkumar, Anima},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04611},
  eprint = {2302.04611},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04611},
  url = {http://arxiv.org/abs/2302.04611},
  urldate = {2023-10-26},
  abstract = {Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins' high-level properties. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP that aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that generates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We empirically verify the effectiveness of ProteinDT from three aspects: (1) consistently superior performance on four out of six protein property prediction benchmarks; (2) over 90\% accuracy for text-guided protein generation; and (3) promising results for zero-shot text-guided protein editing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Liu2023TextguidedProteinDesign_A Text-guided Protein Design Framework.pdf;/Users/ma/Zotero/storage/AP8TXL2X/2302.html}
}

@inproceedings{Locatello2020ObjectCentricLearningSlot,
  title = {Object-{{Centric Learning}} with {{Slot Attention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Locatello, Francesco and Weissenborn, Dirk and Unterthiner, Thomas and Mahendran, Aravindh and Heigold, Georg and Uszkoreit, Jakob and Dosovitskiy, Alexey and Kipf, Thomas},
  year = {2020},
  volume = {33},
  pages = {11525--11538},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html},
  urldate = {2023-05-18},
  abstract = {Learning object-centric representations of complex scenes is a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes. In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots. These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Locatello2020ObjectCentricLearningSlot_Object-Centric Learning with Slot Attention.pdf}
}

@misc{Longpre2023FlanCollectionDesigning,
  title = {The {{Flan Collection}}: {{Designing Data}} and {{Methods}} for {{Effective Instruction Tuning}}},
  shorttitle = {The {{Flan Collection}}},
  author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  year = {2023},
  month = feb,
  number = {arXiv:2301.13688},
  eprint = {2301.13688},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2301.13688},
  urldate = {2023-03-08},
  abstract = {We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Longpre2023FlanCollectionDesigning_The Flan Collection.pdf;/Users/ma/Zotero/storage/NEIYHB4S/2301.html}
}

@misc{Longpre2023FlanCollectionDesigninga,
  title = {The {{Flan Collection}}: {{Designing Data}} and {{Methods}} for {{Effective Instruction Tuning}}},
  shorttitle = {The {{Flan Collection}}},
  author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  year = {2023},
  month = feb,
  number = {arXiv:2301.13688},
  eprint = {2301.13688},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.13688},
  url = {http://arxiv.org/abs/2301.13688},
  urldate = {2023-09-27},
  abstract = {We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Longpre2023FlanCollectionDesigninga_The Flan Collection.pdf;/Users/ma/Zotero/storage/3R5Q6CMN/2301.html}
}

@article{Loomba2021MeasuringImpactCOVID19,
  title = {Measuring the Impact of {{COVID-19}} Vaccine Misinformation on Vaccination Intent in the {{UK}} and {{USA}}},
  author = {Loomba, Sahil and {de Figueiredo}, Alexandre and Piatek, Simon J. and {de Graaf}, Kristen and Larson, Heidi J.},
  year = {2021},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {5},
  number = {3},
  pages = {337--348},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-021-01056-1},
  url = {https://www.nature.com/articles/s41562-021-01056-1},
  urldate = {2023-10-13},
  abstract = {Widespread acceptance of a vaccine for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) will be the next major step in fighting the coronavirus disease 2019 (COVID-19) pandemic, but achieving high uptake will be a challenge and may be impeded by online misinformation. To inform successful vaccination campaigns, we conducted a randomized controlled trial in the UK and the USA to quantify how exposure to online misinformation around COVID-19 vaccines affects intent to vaccinate to protect oneself or others. Here we show that in both countries\textemdash as of September 2020\textemdash fewer people would `definitely' take a vaccine than is likely required for herd immunity, and that, relative to factual information, recent misinformation induced a decline in intent of 6.2 percentage points (95th percentile interval 3.9 to 8.5) in the UK and 6.4 percentage points (95th percentile interval 4.0 to 8.8) in the USA among those who stated that they would definitely accept a vaccine. We also find that some sociodemographic groups are differentially impacted by exposure to misinformation. Finally, we show that scientific-sounding misinformation is more strongly associated with declines in vaccination intent.},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Science,Social policy,technology and society},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Loomba2021MeasuringImpactCOVID19_Measuring the impact of COVID-19 vaccine misinformation on vaccination intent.pdf}
}

@inproceedings{Lou2021MLBiNetCrossSentenceCollective,
  title = {{{MLBiNet}}: {{A Cross-Sentence Collective Event Detection Network}}},
  shorttitle = {{{MLBiNet}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lou, Dongfang and Liao, Zhilin and Deng, Shumin and Zhang, Ningyu and Chen, Huajun},
  year = {2021},
  month = aug,
  pages = {4829--4839},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.373},
  url = {https://aclanthology.org/2021.acl-long.373},
  urldate = {2023-03-03},
  abstract = {We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings. The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level. In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously. Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence. Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information. Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences. We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lou2021MLBiNetCrossSentenceCollective_MLBiNet.pdf}
}

@inproceedings{Lu2021Text2EventControllableSequencetoStructure,
  title = {{{Text2Event}}: {{Controllable Sequence-to-Structure Generation}} for {{End-to-end Event Extraction}}},
  shorttitle = {{{Text2Event}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yaojie and Lin, Hongyu and Xu, Jin and Han, Xianpei and Tang, Jialong and Li, Annan and Sun, Le and Liao, Meng and Chen, Shaoyi},
  year = {2021},
  month = aug,
  pages = {2795--2806},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.217},
  url = {https://aclanthology.org/2021.acl-long.217},
  urldate = {2023-06-23},
  abstract = {Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2021Text2EventControllableSequencetoStructure_Text2Event.pdf}
}

@inproceedings{Lu2022ClinicalT5GenerativeLanguage,
  title = {{{ClinicalT5}}: {{A Generative Language Model}} for {{Clinical Text}}},
  shorttitle = {{{ClinicalT5}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Lu, Qiuhao and Dou, Dejing and Nguyen, Thien},
  year = {2022},
  month = dec,
  pages = {5436--5443},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.398},
  urldate = {2023-02-09},
  abstract = {In the past few years, large pre-trained language models (PLMs) have been widely adopted in different areas and have made fundamental improvements over a variety of downstream tasks in natural language processing (NLP). Meanwhile, domain-specific variants of PLMs are being proposed to address the needs of domains that demonstrate a specific pattern of writing and vocabulary, e.g., BioBERT for the biomedical domain and ClinicalBERT for the clinical domain. Recently, generative language models like BART and T5 are gaining popularity with their competitive performance on text generation as well as on tasks cast as generative problems. However, in the clinical domain, such domain-specific generative variants are still underexplored. To address this need, our work introduces a T5-based text-to-text transformer model pre-trained on clinical text, i.e., ClinicalT5. We evaluate the proposed model both intrinsically and extrinsically over a diverse set of tasks across multiple datasets, and show that ClinicalT5 dramatically outperforms T5 in the domain-specific tasks and compares favorably with its close baselines.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2022ClinicalT5GenerativeLanguage_ClinicalT5.pdf}
}

@inproceedings{Lu2022LearnExplainMultimodal,
  title = {Learn to {{Explain}}: {{Multimodal Reasoning}} via {{Thought Chains}} for {{Science Question Answering}}},
  shorttitle = {Learn to {{Explain}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=HjwK-Tc\_Bc},
  urldate = {2023-03-08},
  abstract = {When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of \textasciitilde 21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
  langid = {english},
  keywords = {Science Instructions},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2022LearnExplainMultimodal_Learn to Explain.pdf}
}

@inproceedings{Lu2022LearnExplainMultimodala,
  title = {Learn to {{Explain}}: {{Multimodal Reasoning}} via {{Thought Chains}} for {{Science Question Answering}}},
  shorttitle = {Learn to {{Explain}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=HjwK-Tc\_Bc},
  urldate = {2023-03-22},
  abstract = {When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of \textasciitilde 21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2022LearnExplainMultimodala_Learn to Explain.pdf}
}

@inproceedings{Lu2022SummarizationIndirectSupervision,
  title = {Summarization as {{Indirect Supervision}} for {{Relation Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Lu, Keming and Hsu, I-Hung and Zhou, Wenxuan and Ma, Mingyu Derek and Chen, Muhao},
  year = {2022},
  month = dec,
  pages = {6575--6594},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.490},
  urldate = {2023-05-24},
  abstract = {Relation extraction (RE) models have been challenged by their reliance on training data with expensive annotations. Considering that summarization tasks aim at acquiring concise expressions of synoptical information from the longer context, these tasks naturally align with the objective of RE, i.e., extracting a kind of synoptical information that describes the relation of entity mentions. We present SuRE, which converts RE into a summarization formulation. SuRE leads to more precise and resource-efficient RE based on indirect supervision from summarization tasks. To achieve this goal, we develop sentence and relation conversion techniques that essentially bridge the formulation of summarization and RE tasks. We also incorporate constraint decoding techniques with Trie scoring to further enhance summarization-based RE with robust inference. Experiments on three RE datasets demonstrate the effectiveness of SuRE in both full-dataset and low-resource settings, showing that summarization is a promising source of indirect supervision signals to improve RE models.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2022SummarizationIndirectSupervision_Summarization as Indirect Supervision for Relation Extraction.pdf}
}

@inproceedings{Lu2022UnifiedStructureGeneration,
  title = {Unified {{Structure Generation}} for {{Universal Information Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  year = {2022},
  month = may,
  pages = {5755--5772},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.395},
  url = {https://aclanthology.org/2022.acl-long.395},
  urldate = {2023-02-21},
  abstract = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism \textendash{} structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2022UnifiedStructureGeneration_Unified Structure Generation for Universal Information Extraction.pdf}
}

@inproceedings{Lu2022UnifiedStructureGenerationa,
  title = {Unified {{Structure Generation}} for {{Universal Information Extraction}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Yaojie and Liu, Qing and Dai, Dai and Xiao, Xinyan and Lin, Hongyu and Han, Xianpei and Sun, Le and Wu, Hua},
  year = {2022},
  month = may,
  pages = {5755--5772},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.395},
  url = {https://aclanthology.org/2022.acl-long.395},
  urldate = {2023-05-01},
  abstract = {Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism \textendash{} structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lu2022UnifiedStructureGenerationa_Unified Structure Generation for Universal Information Extraction.pdf}
}

@misc{Luo2022BioTABQAInstructionLearning,
  title = {{{BioTABQA}}: {{Instruction Learning}} for {{Biomedical Table Question Answering}}},
  shorttitle = {{{BioTABQA}}},
  author = {Luo, Man and Saxena, Sharad and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
  year = {2022},
  month = jul,
  number = {arXiv:2207.02419},
  eprint = {2207.02419},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.02419},
  url = {http://arxiv.org/abs/2207.02419},
  urldate = {2023-03-08},
  abstract = {Table Question Answering (TQA) is an important but under-explored task. Most of the existing QA datasets are in unstructured text format and only few of them use tables as the context. To the best of our knowledge, none of TQA datasets exist in the biomedical domain where tables are frequently used to present information. In this paper, we first curate a table question answering dataset, BioTABQA, using 22 templates and the context from a biomedical textbook on differential diagnosis. BioTABQA can not only be used to teach a model how to answer questions from tables but also evaluate how a model generalizes to unseen questions, an important scenario for biomedical applications. To achieve the generalization evaluation, we divide the templates into 17 training and 5 cross-task evaluations. Then, we develop two baselines using single and multi-tasks learning on BioTABQA. Furthermore, we explore instructional learning, a recent technique showing impressive generalizing performance. Experimental results show that our instruction-tuned model outperforms single and multi-task baselines on an average by \textasciitilde 23\% and \textasciitilde 6\% across various evaluation settings, and more importantly, instruction-tuned model outperforms baselines by \textasciitilde 5\% on cross-tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Instruction,Science Instructions},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Luo2022BioTABQAInstructionLearning_BioTABQA.pdf;/Users/ma/Zotero/storage/BRA6E9ZL/2207.html}
}

@inproceedings{Lyu2021ZeroshotEventExtraction,
  title = {Zero-Shot {{Event Extraction}} via {{Transfer Learning}}: {{Challenges}} and {{Insights}}},
  shorttitle = {Zero-Shot {{Event Extraction}} via {{Transfer Learning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Lyu, Qing and Zhang, Hongming and Sulem, Elior and Roth, Dan},
  year = {2021},
  month = aug,
  pages = {322--332},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-short.42},
  url = {https://aclanthology.org/2021.acl-short.42},
  urldate = {2023-05-23},
  abstract = {Event extraction has long been a challenging task, addressed mostly with supervised methods that require expensive annotation and are not extensible to new event ontologies. In this work, we explore the possibility of zero-shot event extraction by formulating it as a set of Textual Entailment (TE) and/or Question Answering (QA) queries (e.g. ``A city was attacked'' entails ``There is an attack''), exploiting pretrained TE/QA models for direct transfer. On ACE-2005 and ERE, our system achieves acceptable results, yet there is still a large gap from supervised approaches, showing that current QA and TE technologies fail in transferring to a different domain. To investigate the reasons behind the gap, we analyze the remaining key challenges, their respective impact, and possible improvement directions.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lyu2021ZeroshotEventExtraction_Zero-shot Event Extraction via Transfer Learning.pdf}
}

@misc{Lyu2023FaithfulChainofThoughtReasoning,
  title = {Faithful {{Chain-of-Thought Reasoning}}},
  author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and {Callison-Burch}, Chris},
  year = {2023},
  month = feb,
  number = {arXiv:2301.13379},
  eprint = {2301.13379},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2301.13379},
  urldate = {2023-04-25},
  abstract = {While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a faithful-by-construction framework that decomposes a reasoning task into two stages: Translation (Natural Language query \$\textbackslash rightarrow\$ symbolic reasoning chain) and Problem Solving (reasoning chain \$\textbackslash rightarrow\$ answer), using an LM and a deterministic solver respectively. We demonstrate the efficacy of our approach on 10 reasoning datasets from 4 diverse domains. It outperforms traditional CoT prompting on 9 out of the 10 datasets, with an average accuracy gain of 4.4 on Math Word Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1 on Logical Inference, under greedy decoding. Together with self-consistency decoding, we achieve new state-of-the-art few-shot performance on 7 out of the 10 datasets, showing a strong synergy between faithfulness and accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Lyu2023FaithfulChainofThoughtReasoning_Faithful Chain-of-Thought Reasoning.pdf;/Users/ma/Zotero/storage/SU8X6WQF/2301.html}
}

@inproceedings{Ma2017DipoleDiagnosisPrediction,
  title = {Dipole: {{Diagnosis Prediction}} in {{Healthcare}} via {{Attention-based Bidirectional Recurrent Neural Networks}}},
  shorttitle = {Dipole},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ma, Fenglong and Chitta, Radha and Zhou, Jing and You, Quanzeng and Sun, Tong and Gao, Jing},
  year = {2017},
  month = aug,
  series = {{{KDD}} '17},
  pages = {1903--1911},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098088},
  url = {https://dl.acm.org/doi/10.1145/3097983.3098088},
  urldate = {2023-10-31},
  abstract = {Predicting the future health information of patients from the historical Electronic Health Records (EHR) is a core research task in the development of personalized healthcare. Patient EHR data consist of sequences of visits over time, where each visit contains multiple medical codes, including diagnosis, medication, and procedure codes. The most important challenges for this task are to model the temporality and high dimensionality of sequential EHR data and to interpret the prediction results. Existing work solves this problem by employing recurrent neural networks (RNNs) to model EHR data and utilizing simple attention mechanism to interpret the results. However, RNN-based approaches suffer from the problem that the performance of RNNs drops when the length of sequences is large, and the relationships between subsequent visits are ignored by current RNN-based approaches. To address these issues, we propose Dipole, an end-to-end, simple and robust model for predicting patients' future health information. Dipole employs bidirectional recurrent neural networks to remember all the information of both the past visits and the future visits, and it introduces three attention mechanisms to measure the relationships of different visits for the prediction. With the attention mechanisms, Dipole can interpret the prediction results effectively. Dipole also allows us to interpret the learned medical code representations which are confirmed positively by medical experts. Experimental results on two real world EHR datasets show that the proposed Dipole can significantly improve the prediction accuracy compared with the state-of-the-art diagnosis prediction approaches and provide clinically meaningful interpretation.},
  isbn = {978-1-4503-4887-4},
  keywords = {attention mechanism,bidirectional recurrent neural networks,healthcare informatics},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2017DipoleDiagnosisPrediction_Dipole.pdf}
}

@inproceedings{Ma2018GeneralFrameworkDiagnosis,
  title = {A {{General Framework}} for {{Diagnosis Prediction}} via {{Incorporating Medical Code Descriptions}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}} ({{BIBM}})},
  author = {Ma, Fenglong and Wang, Yaqing and Xiao, Houping and Yuan, Ye and Chitta, Radha and Zhou, Jing and Gao, Jing},
  year = {2018},
  month = dec,
  pages = {1070--1075},
  doi = {10.1109/BIBM.2018.8621395},
  url = {https://ieeexplore.ieee.org/document/8621395},
  urldate = {2023-11-02},
  abstract = {Diagnosis prediction aims to predict the future health status of patients according to their historical visit records, which is an important yet challenging task in healthcare informatics. Existing diagnosis prediction approaches mainly employ recurrent neural networks (RNNs) with attention mechanisms to make predictions. However, these approaches ignore the importance of code descriptions, i.e., the medical definitions of diagnosis codes. We believe that taking diagnosis code descriptions into account can help the state-of-the-art models not only to learn meaningful code representations, but also to improve the predictive performance. Thus, in this paper, we propose a simple, but general diagnosis prediction framework, which includes two basic components: diagnosis code embedding and predictive model. To learn the interpretable code embeddings, we apply convolutional neural networks (CNNs) to model medical descriptions of diagnosis codes extracted from online medical websites. The learned medical embedding matrix is used to embed the input visits into vector representations, which are fed into the predictive models. Any existing diagnosis prediction approach (referred to as the base model) can be cast into the proposed framework as the predictive model (called the enhanced model). We conduct experiments on two real medical datasets: the MIMIC-III dataset and the Heart Failure claim dataset. Experimental results show that the enhanced diagnosis prediction approaches significantly improve the prediction performance.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2018GeneralFrameworkDiagnosis_A General Framework for Diagnosis Prediction via Incorporating Medical Code.pdf;/Users/ma/Zotero/storage/KNWBQ53G/8621395.html}
}

@inproceedings{Ma2021EventPlusTemporalEvent,
  title = {{{EventPlus}}: {{A Temporal Event Understanding Pipeline}}},
  shorttitle = {{{EventPlus}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}: {{Demonstrations}}},
  author = {Ma, Mingyu Derek and Sun, Jiao and Yang, Mu and Huang, Kung-Hsiang and Wen, Nuan and Singh, Shikhar and Han, Rujun and Peng, Nanyun},
  year = {2021},
  month = jun,
  pages = {56--65},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-demos.7},
  url = {https://aclanthology.org/2021.naacl-demos.7},
  urldate = {2023-05-24},
  abstract = {We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2021EventPlusTemporalEvent_EventPlus.pdf}
}

@inproceedings{Ma2021HyperExpanTaxonomyExpansion,
  title = {{{HyperExpan}}: {{Taxonomy Expansion}} with {{Hyperbolic Representation Learning}}},
  shorttitle = {{{HyperExpan}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Ma, Mingyu Derek and Chen, Muhao and Wu, Te-Lin and Peng, Nanyun},
  year = {2021},
  month = nov,
  pages = {4182--4194},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.353},
  url = {https://aclanthology.org/2021.findings-emnlp.353},
  urldate = {2023-04-29},
  abstract = {Taxonomies are valuable resources for many applications, but the limited coverage due to the expensive manual curation process hinders their general applicability. Prior works attempt to automatically expand existing taxonomies to improve their coverage by learning concept embeddings in Euclidean space, while taxonomies, inherently hierarchical, more naturally align with the geometric properties of a hyperbolic space. In this paper, we present HyperExpan, a taxonomy expansion algorithm that seeks to preserve the structure of a taxonomy in a more expressive hyperbolic embedding space and learn to represent concepts and their relations with a Hyperbolic Graph Neural Network (HGNN). Specifically, HyperExpan leverages position embeddings to exploit the structure of the existing taxonomies, and characterizes the concept profile information to support the inference on new concepts that are unseen during training. Experiments show that our proposed HyperExpan outperforms baseline models with representation learning in a Euclidean feature space and achieves state-of-the-art performance on the taxonomy expansion benchmarks.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2021HyperExpanTaxonomyExpansion_HyperExpan.pdf}
}

@inproceedings{Ma2022DICEDataEfficientClinical,
  title = {{{DICE}}: {{Data-Efficient Clinical Event Extraction}} with {{Generative Models}}},
  shorttitle = {{{DICE}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ma, Mingyu Derek and Taylor, Alexander K. and Wang, Wei and Peng, Nanyun},
  year = {2022},
  month = aug,
  eprint = {2208.07989},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.07989},
  urldate = {2023-04-29},
  abstract = {Event extraction in the clinical domain is an under-explored research area. The lack of training data in addition to the high volume of domain-specific jargon that includes long entities with vague boundaries make the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and utilizes descriptions provided by domain experts to boost the performance under low-resource settings. Furthermore, DICE learns to locate and bound biomedical mentions with an auxiliary mention identification task trained jointly with event extraction tasks to leverage inter-task dependencies and further incorporates the identified mentions as trigger and argument candidates for their respective tasks. We also introduce MACCROBAT-EE, the first clinical event extraction dataset with event argument annotation. Our experiments demonstrate the robustness of DICE under low data settings for the clinical domain and the benefits of incorporating flexible joint training and mention markers into generative approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2022DICEDataEfficientClinical_DICE.pdf;/Users/ma/Zotero/storage/DCALXR2M/2208.html}
}

@inproceedings{Ma2022PromptExtractionPAIE,
  title = {Prompt for {{Extraction}}? {{PAIE}}: {{Prompting Argument Interaction}} for {{Event Argument Extraction}}},
  shorttitle = {Prompt for {{Extraction}}?},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ma, Yubo and Wang, Zehao and Cao, Yixin and Li, Mukai and Chen, Meiqi and Wang, Kun and Shao, Jing},
  year = {2022},
  month = may,
  pages = {6759--6774},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.466},
  url = {https://aclanthology.org/2022.acl-long.466},
  urldate = {2023-02-09},
  abstract = {In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2022PromptExtractionPAIE_Prompt for Extraction.pdf}
}

@inproceedings{Ma2022PromptExtractionPAIEa,
  title = {Prompt for {{Extraction}}? {{PAIE}}: {{Prompting Argument Interaction}} for {{Event Argument Extraction}}},
  shorttitle = {Prompt for {{Extraction}}?},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Ma, Yubo and Wang, Zehao and Cao, Yixin and Li, Mukai and Chen, Meiqi and Wang, Kun and Shao, Jing},
  year = {2022},
  month = may,
  pages = {6759--6774},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.466},
  url = {https://aclanthology.org/2022.acl-long.466},
  urldate = {2023-05-01},
  abstract = {In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2022PromptExtractionPAIEa_Prompt for Extraction.pdf}
}

@misc{Ma2023LargeLanguageModel,
  title = {Large {{Language Model Is Not}} a {{Good Few-shot Information Extractor}}, but a {{Good Reranker}} for {{Hard Samples}}!},
  author = {Ma, Yubo and Cao, Yixin and Hong, YongChing and Sun, Aixin},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08559},
  eprint = {2303.08559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.08559},
  urldate = {2023-04-25},
  abstract = {Large Language Models (LLMs) have made remarkable strides in various tasks. However, whether they are competitive few-shot solvers for information extraction (IE) tasks and surpass fine-tuned small Pre-trained Language Models (SLMs) remains an open problem. This paper aims to provide a thorough answer to this problem, and moreover, to explore an approach towards effective and economical IE systems that combine the strengths of LLMs and SLMs. Through extensive experiments on eight datasets across three IE tasks, we show that LLMs are not effective few-shot information extractors in general, given their unsatisfactory performance in most settings and the high latency and budget requirements. However, we demonstrate that LLMs can well complement SLMs and effectively solve hard samples that SLMs struggle with. Building on these findings, we propose an adaptive filter-then-rerank paradigm, in which SLMs act as filters and LLMs act as rerankers. By utilizing LLMs to rerank a small portion of difficult samples identified by SLMs, our preliminary system consistently achieves promising improvements (2.1\% F1-gain on average) on various IE tasks, with acceptable cost of time and money.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2023LargeLanguageModel_Large Language Model Is Not a Good Few-shot Information Extractor, but a Good.pdf;/Users/ma/Zotero/storage/GPVAVJYP/2303.html}
}

@misc{Ma2023MIDDAGWhereDoes,
  title = {{{MIDDAG}}: {{Where Does Our News Go}}? {{Investigating Information Diffusion}} via {{Community-Level Information Pathways}}},
  shorttitle = {{{MIDDAG}}},
  author = {Ma, Mingyu Derek and Taylor, Alexander K. and Wen, Nuan and Liu, Yanchen and Kung, Po-Nien and Qin, Wenna and Wen, Shicheng and Zhou, Azure and Yang, Diyi and Ma, Xuezhe and Peng, Nanyun and Wang, Wei},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02529},
  eprint = {2310.02529},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.02529},
  url = {http://arxiv.org/abs/2310.02529},
  urldate = {2023-10-12},
  abstract = {We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Social and Information Networks},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2023MIDDAGWhereDoes_MIDDAG.pdf;/Users/ma/Zotero/storage/D2QVTLHS/2310.html}
}

@misc{Ma2023STARBoostingLowResource,
  title = {{{STAR}}: {{Boosting Low-Resource Event Extraction}} by {{Structure-to-Text Data Generation}} with {{Large Language Models}}},
  shorttitle = {{{STAR}}},
  author = {Ma, Mingyu Derek and Wang, Xiaoxuan and Kung, Po-Nien and Brantingham, P. Jeffrey and Peng, Nanyun and Wang, Wei},
  year = {2023},
  month = may,
  number = {arXiv:2305.15090},
  eprint = {2305.15090},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.15090},
  url = {http://arxiv.org/abs/2305.15090},
  urldate = {2023-10-02},
  abstract = {Structure prediction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies, thus they still heavily rely on task-specific training data to obtain reasonable performance. Due to the high cost of human annotation, low-resource event extraction, which requires minimal human cost, is urgently needed in real-world information extraction applications. We propose to synthesize data instances given limited seed demonstrations to boost low-resource event extraction performance. We propose STAR, a structure-to-text data generation method that first generates complicated event structures (Y) and then generates input passages (X), all with Large Language Models. We design fine-grained step-by-step instructions and the error cases and quality issues identified through self-reflection can be self-refined. Our experiments indicate that data generated by STAR can significantly improve the low-resource event extraction performance and they are even more effective than human-curated data points in some cases.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ma2023STARBoostingLowResource_STAR.pdf;/Users/ma/Zotero/storage/8ZG5V5XQ/2305.html}
}

@misc{Madaan2023SelfRefineIterativeRefinement,
  title = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
  shorttitle = {Self-{{Refine}}},
  author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Welleck, Sean and Majumder, Bodhisattwa Prasad and Gupta, Shashank and Yazdanbakhsh, Amir and Clark, Peter},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17651},
  eprint = {2303.17651},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.17651},
  urldate = {2023-04-03},
  abstract = {Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving on average by absolute 20\% across tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Madaan2023SelfRefineIterativeRefinement_Self-Refine.pdf;/Users/ma/Zotero/storage/2QT9DBUY/2303.html}
}

@misc{Madaan2023SelfRefineIterativeRefinementa,
  title = {Self-{{Refine}}: {{Iterative Refinement}} with {{Self-Feedback}}},
  shorttitle = {Self-{{Refine}}},
  author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Welleck, Sean and Majumder, Bodhisattwa Prasad and Gupta, Shashank and Yazdanbakhsh, Amir and Clark, Peter},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17651},
  eprint = {2303.17651},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.17651},
  url = {http://arxiv.org/abs/2303.17651},
  urldate = {2023-04-20},
  abstract = {Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving on average by absolute 20\% across tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Madaan2023SelfRefineIterativeRefinementa_Self-Refine2.pdf;/Users/ma/Zotero/storage/II3G3EUZ/2303.html}
}

@inproceedings{MedinaSerrano2020DancingPartisanBeat,
  title = {Dancing to the {{Partisan Beat}}: {{A First Analysis}} of {{Political Communication}} on {{TikTok}}},
  shorttitle = {Dancing to the {{Partisan Beat}}},
  booktitle = {12th {{ACM Conference}} on {{Web Science}}},
  author = {Medina Serrano, Juan Carlos and Papakyriakopoulos, Orestis and Hegelich, Simon},
  year = {2020},
  month = jul,
  series = {{{WebSci}} '20},
  pages = {257--266},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3394231.3397916},
  url = {https://dl.acm.org/doi/10.1145/3394231.3397916},
  urldate = {2023-04-10},
  abstract = {TikTok is a video-sharing social networking service, whose popularity is increasing rapidly. It was the world's second-most downloaded app in 2019. Although the platform is known for having users posting videos of themselves dancing, lip-syncing, or showcasing other talents, user-videos expressing political views have seen a recent spurt. This study aims to perform a primary evaluation of political communication on TikTok. We collect a set of US partisan Republican and Democratic videos to investigate how users communicated with each other about political issues. With the help of computer vision, natural language processing, and statistical tools, we illustrate that political communication on TikTok is much more interactive in comparison to other social media platforms, with users combining multiple information channels to spread their messages. We show that political communication takes place in the form of communication trees since users generate branches of responses to existing content. In terms of user demographics, we find that users belonging to both the US parties are young and behave similarly on the platform. However, Republican users generated more political content and their videos received more responses; on the other hand, Democratic users engaged significantly more in cross-partisan discussions.},
  isbn = {978-1-4503-7989-2},
  keywords = {political communication,social media,TikTok,US politics},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Dancing to the Partisan Beat.pdf}
}

@misc{Meng2021COCOLMCorrectingContrasting,
  title = {{{COCO-LM}}: {{Correcting}} and {{Contrasting Text Sequences}} for {{Language Model Pretraining}}},
  shorttitle = {{{COCO-LM}}},
  author = {Meng, Yu and Xiong, Chenyan and Bajaj, Payal and Tiwary, Saurabh and Bennett, Paul and Han, Jiawei and Song, Xia},
  year = {2021},
  month = oct,
  number = {arXiv:2102.08473},
  eprint = {2102.08473},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.08473},
  url = {http://arxiv.org/abs/2102.08473},
  urldate = {2023-02-17},
  abstract = {We present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50\% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Meng2021COCOLMCorrectingContrasting_COCO-LM.pdf;/Users/ma/Zotero/storage/33NT8BQY/2102.html}
}

@misc{Meng2022GeneratingTrainingData,
  title = {Generating {{Training Data}} with {{Language Models}}: {{Towards Zero-Shot Language Understanding}}},
  shorttitle = {Generating {{Training Data}} with {{Language Models}}},
  author = {Meng, Yu and Huang, Jiaxin and Zhang, Yu and Han, Jiawei},
  year = {2022},
  month = oct,
  number = {arXiv:2202.04538},
  eprint = {2202.04538},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.04538},
  urldate = {2023-03-21},
  abstract = {Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,data generation},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Meng2022GeneratingTrainingData_Generating Training Data with Language Models.pdf;/Users/ma/Zotero/storage/5NIB6BEA/2202.html}
}

@misc{Meng2022TuningLanguageModels,
  title = {Tuning {{Language Models}} as {{Training Data Generators}} for {{Augmentation-Enhanced Few-Shot Learning}}},
  author = {Meng, Yu and Michalski, Martin and Huang, Jiaxin and Zhang, Yu and Abdelzaher, Tarek and Han, Jiawei},
  year = {2022},
  month = nov,
  number = {arXiv:2211.03044},
  eprint = {2211.03044},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.03044},
  urldate = {2023-03-21},
  abstract = {Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the few-shot and the synthetic samples with regularization for better generalization and stability. Our approach FewGen achieves an overall better result across seven classification tasks of the GLUE benchmark than existing few-shot learning methods, improving no-augmentation methods by 5+ average points, and outperforming augmentation methods by 3+ average points.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Meng2022TuningLanguageModels_Tuning Language Models as Training Data Generators for Augmentation-Enhanced.pdf;/Users/ma/Zotero/storage/CAIUMGIL/2211.html}
}

@misc{Mildenhall2020NeRFRepresentingScenes,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2023-05-18},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Mildenhall2020NeRFRepresentingScenes_NeRF.pdf;/Users/ma/Zotero/storage/2CBVSACV/2003.html}
}

@article{Mildenhall2021NeRFRepresentingScenes,
  title = {{{NeRF}}: Representing Scenes as Neural Radiance Fields for View Synthesis},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2021},
  month = dec,
  journal = {Communications of the ACM},
  volume = {65},
  number = {1},
  pages = {99--106},
  issn = {0001-0782},
  doi = {10.1145/3503250},
  url = {https://dl.acm.org/doi/10.1145/3503250},
  urldate = {2023-05-18},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (\texttheta, {$\phi$})) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Mildenhall2021NeRFRepresentingScenes_NeRF.pdf}
}

@misc{Min2022RethinkingRoleDemonstrations,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = oct,
  number = {arXiv:2202.12837},
  eprint = {2202.12837},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.12837},
  urldate = {2023-04-25},
  abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Min2022RethinkingRoleDemonstrations_Rethinking the Role of Demonstrations.pdf;/Users/ma/Zotero/storage/YYDPP29C/2202.html}
}

@misc{Min2022RethinkingRoleDemonstrationsa,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = oct,
  number = {arXiv:2202.12837},
  eprint = {2202.12837},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.12837},
  url = {http://arxiv.org/abs/2202.12837},
  urldate = {2023-08-19},
  abstract = {Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Min2022RethinkingRoleDemonstrationsa_Rethinking the Role of Demonstrations.pdf;/Users/ma/Zotero/storage/F5QBIMCQ/2202.html}
}

@inproceedings{Min2022RethinkingRoleDemonstrationsb,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = dec,
  pages = {11048--11064},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  doi = {10.18653/v1/2022.emnlp-main.759},
  url = {https://aclanthology.org/2022.emnlp-main.759},
  urldate = {2023-10-23},
  abstract = {Large language models (LMs) are able to in-context learn\textemdash perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required\textemdash randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Min2022RethinkingRoleDemonstrationsb_Rethinking the Role of Demonstrations.pdf}
}

@article{MinRethinkingRoleDemonstrations,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  abstract = {Large language models (LMs) are able to incontext learn\textemdash perform a new task via inference alone by conditioning on a few inputlabel pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required\textemdash randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/JWUY8NFS/Min et al. - Rethinking the Role of Demonstrations What Makes .pdf}
}

@inproceedings{Mishra2022CrossTaskGeneralizationNatural,
  title = {Cross-{{Task Generalization}} via {{Natural Language Crowdsourcing Instructions}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  year = {2022},
  month = may,
  pages = {3470--3487},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.244},
  url = {https://aclanthology.org/2022.acl-long.244},
  urldate = {2023-03-08},
  abstract = {Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19\% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.},
  keywords = {dataset\_NaturalInstructions,Instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Mishra2022CrossTaskGeneralizationNatural_Cross-Task Generalization via Natural Language Crowdsourcing Instructions.pdf}
}

@misc{MisinformationItsCorrection,
  title = {Misinformation and {{Its Correction}}: {{Continued Influence}} and {{Successful Debiasing}} - {{Stephan Lewandowsky}}, {{Ullrich K}}. {{H}}. {{Ecker}}, {{Colleen M}}. {{Seifert}}, {{Norbert Schwarz}}, {{John Cook}}, 2012},
  url = {https://journals.sagepub.com/doi/10.1177/1529100612451018},
  urldate = {2023-10-13}
}

@article{Morid2023TimeSeriesPrediction,
  title = {Time {{Series Prediction Using Deep Learning Methods}} in {{Healthcare}}},
  author = {Morid, Mohammad Amin and Sheng, Olivia R. Liu and Dunbar, Joseph},
  year = {2023},
  month = jan,
  journal = {ACM Transactions on Management Information Systems},
  volume = {14},
  number = {1},
  pages = {2:1--2:29},
  issn = {2158-656X},
  doi = {10.1145/3531326},
  url = {https://dl.acm.org/doi/10.1145/3531326},
  urldate = {2023-10-17},
  abstract = {Traditional machine learning methods face unique challenges when applied to healthcare predictive analytics. The high-dimensional nature of healthcare data necessitates labor-intensive and time-consuming processes when selecting an appropriate set of features for each new task. Furthermore, machine learning methods depend heavily on feature engineering to capture the sequential nature of patient data, oftentimes failing to adequately leverage the temporal patterns of medical events and their dependencies. In contrast, recent deep learning (DL) methods have shown promising performance for various healthcare prediction tasks by specifically addressing the high-dimensional and temporal challenges of medical data. DL techniques excel at learning useful representations of medical concepts and patient clinical data as well as their nonlinear interactions from high-dimensional raw or minimally processed healthcare data. In this article, we systematically reviewed research works that focused on advancing deep neural networks to leverage patient structured time series data for healthcare prediction tasks. To identify relevant studies, we searched MEDLINE, IEEE, Scopus, and ACM Digital Library for relevant publications through November 4, 2021. Overall, we found that researchers have contributed to deep time series prediction literature in 10 identifiable research streams: DL models, missing value handling, addressing temporal irregularity, patient representation, static data inclusion, attention mechanisms, interpretation, incorporation of medical ontologies, learning strategies, and scalability. This study summarizes research insights from these literature streams, identifies several critical research gaps, and suggests future research opportunities for DL applications using patient time series data.},
  keywords = {deep learning methods,healthcare predictive analytics,patient time series,Systematic review},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Morid2023TimeSeriesPrediction_Time Series Prediction Using Deep Learning Methods in Healthcare.pdf}
}

@article{Morid2023TimeSeriesPredictiona,
  title = {Time {{Series Prediction Using Deep Learning Methods}} in {{Healthcare}}},
  author = {Morid, Mohammad Amin and Sheng, Olivia R. Liu and Dunbar, Joseph},
  year = {2023},
  month = mar,
  journal = {ACM Transactions on Management Information Systems},
  volume = {14},
  number = {1},
  pages = {1--29},
  issn = {2158-656X, 2158-6578},
  doi = {10.1145/3531326},
  url = {https://dl.acm.org/doi/10.1145/3531326},
  urldate = {2023-10-27},
  abstract = {Traditional machine learning methods face unique challenges when applied to healthcare predictive analytics. The high-dimensional nature of healthcare data necessitates labor-intensive and time-consuming processes when selecting an appropriate set of features for each new task. Furthermore, machine learning methods depend heavily on feature engineering to capture the sequential nature of patient data, oftentimes failing to adequately leverage the temporal patterns of medical events and their dependencies. In contrast, recent deep learning (DL) methods have shown promising performance for various healthcare prediction tasks by specifically addressing the high-dimensional and temporal challenges of medical data. DL techniques excel at learning useful representations of medical concepts and patient clinical data as well as their nonlinear interactions from high-dimensional raw or minimally processed healthcare data.             In this article, we systematically reviewed research works that focused on advancing deep neural networks to leverage patient structured time series data for healthcare prediction tasks. To identify relevant studies, we searched MEDLINE, IEEE, Scopus, and ACM Digital Library for relevant publications through November 4, 2021. Overall, we found that researchers have contributed to deep time series prediction literature in 10 identifiable research streams: DL models, missing value handling, addressing temporal irregularity, patient representation, static data inclusion, attention mechanisms, interpretation, incorporation of medical ontologies, learning strategies, and scalability. This study summarizes research insights from these literature streams, identifies several critical research gaps, and suggests future research opportunities for DL applications using patient time series data.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Morid2023TimeSeriesPredictiona_Time Series Prediction Using Deep Learning Methods in Healthcare.pdf}
}

@misc{Mu2023LearningCompressPrompts,
  title = {Learning to {{Compress Prompts}} with {{Gist Tokens}}},
  author = {Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
  year = {2023},
  month = jul,
  number = {arXiv:2304.08467},
  eprint = {2304.08467},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2304.08467},
  urldate = {2023-10-11},
  abstract = {Prompting is the primary way to utilize the multitask capabilities of language models (LMs), but prompts occupy valuable space in the input context window, and repeatedly encoding the same prompt is computationally inefficient. Finetuning and distillation methods allow for specialization of LMs without prompting, but require retraining the model for each task. To avoid this trade-off entirely, we present gisting, which trains an LM to compress prompts into smaller sets of "gist" tokens which can be cached and reused for compute efficiency. Gist models can be trained with no additional cost over standard instruction finetuning by simply modifying Transformer attention masks to encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40\% FLOPs reductions, 4.2\% wall time speedups, and storage savings, all with minimal loss in output quality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Mu2023LearningCompressPromptsa_Learning to Compress Prompts with Gist Tokens.pdf;/Users/ma/Zotero/storage/JWDUAKTE/2304.html}
}

@inproceedings{Mueller2022TexttoTextMultiTaskLearners,
  title = {Do {{Text-to-Text Multi-Task Learners Suffer}} from {{Task Conflict}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Mueller, David and Andrews, Nicholas and Dredze, Mark},
  year = {2022},
  month = dec,
  pages = {2843--2858},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.findings-emnlp.206},
  urldate = {2023-02-09},
  abstract = {Traditional multi-task learning architectures learn a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can lead to negative transfer. A new type of multi-task learning within NLP homogenizes multi-task architectures as a shared encoder and language model decoder, which does surprisingly well across a range of diverse tasks. Does this new architecture suffer from task-conflicts that require specialized training algorithms? We study how certain factors in the shift towards text-to-text models affects multi-task conflict and negative transfer, finding that both directional conflict and transfer are surprisingly constant across architectures.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Mueller2022TexttoTextMultiTaskLearners_Do Text-to-Text Multi-Task Learners Suffer from Task Conflict.pdf}
}

@misc{Mueller2023HowPlantTrees,
  title = {How to {{Plant Trees}} in {{Language Models}}: {{Data}} and {{Architectural Effects}} on the {{Emergence}} of {{Syntactic Inductive Biases}}},
  shorttitle = {How to {{Plant Trees}} in {{Language Models}}},
  author = {Mueller, Aaron and Linzen, Tal},
  year = {2023},
  month = may,
  number = {arXiv:2305.19905},
  eprint = {2305.19905},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.19905},
  urldate = {2023-06-06},
  abstract = {Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features - as opposed to incorrect linear features - when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Mueller2023HowPlantTrees_How to Plant Trees in Language Models.pdf;/Users/ma/Zotero/storage/SSML77IG/2305.html}
}

@article{MuNaturalLanguageProcessing,
  title = {Natural {{Language Processing}} with {{Deep Learning CS224N}}/{{Ling284}}},
  author = {Mu, Jesse},
  langid = {english},
  file = {/Users/ma/Zotero/storage/UX2V3AYS/Mu - Natural Language Processing with Deep Learning CS2.pdf}
}

@article{Nan2022WhyPeopleBelieve,
  title = {Why Do People Believe Health Misinformation and Who Is at Risk? {{A}} Systematic Review of Individual Differences in Susceptibility to Health Misinformation},
  shorttitle = {Why Do People Believe Health Misinformation and Who Is at Risk?},
  author = {Nan, Xiaoli and Wang, Yuan and Thier, Kathryn},
  year = {2022},
  month = dec,
  journal = {Social Science \& Medicine},
  volume = {314},
  pages = {115398},
  issn = {0277-9536},
  doi = {10.1016/j.socscimed.2022.115398},
  url = {https://www.sciencedirect.com/science/article/pii/S0277953622007043},
  urldate = {2023-05-22},
  abstract = {Rationale Health misinformation poses a significant threat to public health. Understanding why people believe health misinformation and who is at risk is crucial for developing effective interventions to reduce the harmful impact of misinformation. Approach We conducted a systematic review of published empirical research that examined individual differences in susceptibility to health misinformation, focusing on the psychological, demographic, and behavioral correlates of health misinformation susceptibility. To guide our review on psychological correlates, we developed an integrative psychological model of susceptibility to health misinformation based on one's ability and motivation to reason. Results We identified 47 publications (61 empirical studies) that met our criteria. Our review suggests that subject knowledge, literacy and numeracy, analytical thinking (vs. intuitive thinking), and trust in science confer strong resistance to health misinformation, whereas conspiracy thinking, religiosity, conservative ideology, and conservative party identification are associated with more susceptibility to health misinformation. Demographically, older age and higher educational attainment predict less susceptibility to health misinformation, whereas racial minority status is associated with greater susceptibility. Behaviorally, relying on health professionals or scientists as information sources predicts less susceptibility to health misinformation, whereas social media use is associated with greater susceptibility. Conclusions Susceptibility to health misinformation is driven by multiple psychological processes. Interventions for reducing the spread and impact of health misinformation should be tailored to the psychological mechanism underlying susceptibility to health misinformation. Limited resources should be used to support interventions targeted at individuals at risk.},
  langid = {english},
  keywords = {Analytical thinking,Communication,Literacy,Misinformation,Political ideology,Public health,Social media},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Nan2022WhyPeopleBelieve_Why do people believe health misinformation and who is at risk.pdf;/Users/ma/Zotero/storage/R2YWZYZF/S0277953622007043.html}
}

@article{Nan2022WhyPeopleBelievea,
  title = {Why Do People Believe Health Misinformation and Who Is at Risk? {{A}} Systematic Review of Individual Differences in Susceptibility to Health Misinformation},
  shorttitle = {Why Do People Believe Health Misinformation and Who Is at Risk?},
  author = {Nan, Xiaoli and Wang, Yuan and Thier, Kathryn},
  year = {2022},
  month = dec,
  journal = {Social Science \& Medicine},
  volume = {314},
  pages = {115398},
  issn = {0277-9536},
  doi = {10.1016/j.socscimed.2022.115398},
  url = {https://www.sciencedirect.com/science/article/pii/S0277953622007043},
  urldate = {2023-10-12},
  abstract = {Rationale Health misinformation poses a significant threat to public health. Understanding why people believe health misinformation and who is at risk is crucial for developing effective interventions to reduce the harmful impact of misinformation. Approach We conducted a systematic review of published empirical research that examined individual differences in susceptibility to health misinformation, focusing on the psychological, demographic, and behavioral correlates of health misinformation susceptibility. To guide our review on psychological correlates, we developed an integrative psychological model of susceptibility to health misinformation based on one's ability and motivation to reason. Results We identified 47 publications (61 empirical studies) that met our criteria. Our review suggests that subject knowledge, literacy and numeracy, analytical thinking (vs. intuitive thinking), and trust in science confer strong resistance to health misinformation, whereas conspiracy thinking, religiosity, conservative ideology, and conservative party identification are associated with more susceptibility to health misinformation. Demographically, older age and higher educational attainment predict less susceptibility to health misinformation, whereas racial minority status is associated with greater susceptibility. Behaviorally, relying on health professionals or scientists as information sources predicts less susceptibility to health misinformation, whereas social media use is associated with greater susceptibility. Conclusions Susceptibility to health misinformation is driven by multiple psychological processes. Interventions for reducing the spread and impact of health misinformation should be tailored to the psychological mechanism underlying susceptibility to health misinformation. Limited resources should be used to support interventions targeted at individuals at risk.},
  keywords = {Analytical thinking,Communication,Literacy,Misinformation,Political ideology,Public health,Social media},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Nan2022WhyPeopleBelievea_Why do people believe health misinformation and who is at risk2.pdf}
}

@article{NerellaTransformersHealthcareSurvey,
  title = {Transformers in {{Healthcare}}: {{A Survey}}},
  author = {Nerella, Subhash and Bandyopadhyay, Sabyasachi and Zhang, Jiaqing and Contreras, Miguel and Bumin, Aysegul and Silva, Brandon and Sena, Jessica and Shickel, Benjamin and Bihorac, Azra and Rashidi, Parisa},
  abstract = {With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformers in healthcare and examine issues such as computational cost, model interpretability, fairness, alignment with human values, ethical implications, and environmental impact.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/E3QY5LNV/Nerella et al. - Transformers in Healthcare A Survey.pdf}
}

@inproceedings{NEURIPS2022_2e32d3a1,
  title = {{{InsNet}}: {{An}} Efficient, Flexible, and Performant Insertion-Based Text Generation Model},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Lu, Sidi and Meng, Tao and Peng, Nanyun},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {7011--7023},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper\_files/paper/2022/file/2e32d3a10985fc94c7e11ee6ea165cca-Paper-Conference.pdf}
}

@inproceedings{Nguyen2022LearningCrossTaskDependencies,
  title = {Learning {{Cross-Task Dependencies}} for {{Joint Extraction}} of {{Entities}}, {{Events}}, {{Event Arguments}}, and {{Relations}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Nguyen, Minh Van and Min, Bonan and Dernoncourt, Franck and Nguyen, Thien},
  year = {2022},
  month = dec,
  pages = {9349--9360},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.634},
  urldate = {2023-02-09},
  abstract = {Extracting entities, events, event arguments, and relations (i.e., task instances) from text represents four main challenging tasks in information extraction (IE), which have been solved jointly (JointIE) to boost the overall performance for IE. As such, previous work often leverages two types of dependencies between the tasks, i.e., cross-instance and cross-type dependencies representing relatedness between task instances and correlations between information types of the tasks. However, the cross-task dependencies in prior work are not optimal as they are only designed manually according to some task heuristics. To address this issue, we propose a novel model for JointIE that aims to learn cross-task dependencies from data. In particular, we treat each task instance as a node in a dependency graph where edges between the instances are inferred through information from different layers of a pretrained language model (e.g., BERT). Furthermore, we utilize the Chow-Liu algorithm to learn a dependency tree between information types for JointIE by seeking to approximate the joint distribution of the types from data. Finally, the Chow-Liu dependency tree is used to generate cross-type patterns, serving as anchor knowledge to guide the learning of representations and dependencies between instances for JointIE. Experimental results show that our proposed model significantly outperforms strong JointIE baselines over four datasets with different languages.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Nguyen2022LearningCrossTaskDependencies_Learning Cross-Task Dependencies for Joint Extraction of Entities, Events,.pdf}
}

@misc{Nie2022CrossLingualRetrievalAugmented,
  title = {Cross-{{Lingual Retrieval Augmented Prompt}} for {{Low-Resource Languages}}},
  author = {Nie, Ercong and Liang, Sheng and Schmid, Helmut and Sch{\"u}tze, Hinrich},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09651},
  eprint = {2212.09651},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2212.09651},
  urldate = {2023-02-28},
  abstract = {Multilingual Pretrained Language Models (MPLMs) have shown their strong multilinguality in recent empirical cross-lingual transfer studies. In this paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC) pipeline to improve the zero-shot performance on low-resource languages (LRLs) by augmenting the context with semantically similar sentences retrieved from a high-resource language (HRL) as prompts. PARC improves the zero-shot performance on three downstream tasks (binary sentiment classification, topic categorization and natural language inference) with multilingual parallel test sets across 10 LRLs covering 6 language families in both unlabeled settings (+5.1\%) and labeled settings (+16.3\%). PARC-labeled also outperforms the finetuning baseline by 3.7\%. We find a significant positive correlation between cross-lingual transfer performance on one side, and the similarity between the high- and low-resource languages as well as the amount of low-resource pretraining data on the other side. A robustness analysis suggests that PARC has the potential to achieve even stronger performance with more powerful MPLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Nie2022CrossLingualRetrievalAugmented_Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages.pdf;/Users/ma/Zotero/storage/Z6VYAEU6/2212.html}
}

@misc{Oniani2023LargeLanguageModels,
  title = {Large {{Language Models Vote}}: {{Prompting}} for {{Rare Disease Identification}}},
  shorttitle = {Large {{Language Models Vote}}},
  author = {Oniani, David and Hilsman, Jordan and Dong, Hang and Gao, Fengyi and Verma, Shiven and Wang, Yanshan},
  year = {2023},
  month = aug,
  number = {arXiv:2308.12890},
  eprint = {2308.12890},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.12890},
  url = {http://arxiv.org/abs/2308.12890},
  urldate = {2023-10-17},
  abstract = {The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases affect a small fraction of the population. Rare disease identification from clinical notes inherently requires FSL techniques due to limited data availability. Manual data collection and annotation is both expensive and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FSL, available to those who signed the MIMIC-IV Data Use Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple times, substantially increasing the time needed for manual annotation, and to address this, we assess the feasibility of using JSON for automating generative LLM evaluation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Oniani2023LargeLanguageModels_Large Language Models Vote.pdf;/Users/ma/Zotero/storage/M58TKIPD/2308.html}
}

@misc{OTATopics345,
  title = {{{OTA Topics}} 3-4-5},
  journal = {Google Docs},
  url = {https://docs.google.com/document/d/1qeiVgP3uXfv5qDWCAMPfkE9YtQlYRg8sEPrR1r5-3Zo/edit?usp=embed\_facebook},
  urldate = {2023-05-19},
  abstract = {Summary  Section 3: Knowledge Product Standardization The Knowledge Coordination (KC) aims to establish standards for knowledge products generated by CF programs and their Data Coordinating Centers (DCCs). This involves creating a Knowledge Production Committee consisting of representatives from...},
  langid = {english},
  file = {/Users/ma/Zotero/storage/LDRTGAX4/edit.html}
}

@misc{Ouyang2022TrainingLanguageModels,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02155},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-03-01},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ouyang2022TrainingLanguageModels_Training language models to follow instructions with human feedback.pdf;/Users/ma/Zotero/storage/VVEKBC83/2203.html}
}

@inproceedings{Paolini2022StructuredPredictionTranslation,
  title = {Structured {{Prediction}} as {{Translation}} between {{Augmented Natural Languages}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Paolini, Giovanni and Athiwaratkun, Ben and Krone, Jason and Ma, Jie and Achille, Alessandro and Anubhai, Rishita and dos Santos, Cicero Nogueira and Xiang, Bing and Soatto, Stefano},
  year = {2022},
  month = feb,
  url = {https://openreview.net/forum?id=US-TP-xnXI},
  urldate = {2023-05-01},
  abstract = {We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks, and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Paolini2022StructuredPredictionTranslation_Structured Prediction as Translation between Augmented Natural Languages.pdf}
}

@misc{Parekh2022GENEVAPushingLimit,
  title = {{{GENEVA}}: {{Pushing}} the {{Limit}} of {{Generalizability}} for {{Event Argument Extraction}} with 100+ {{Event Types}}},
  shorttitle = {{{GENEVA}}},
  author = {Parekh, Tanmay and Hsu, I.-Hung and Huang, Kuan-Hao and Chang, Kai-Wei and Peng, Nanyun},
  year = {2022},
  month = may,
  number = {arXiv:2205.12505},
  eprint = {2205.12505},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.12505},
  url = {http://arxiv.org/abs/2205.12505},
  urldate = {2023-05-24},
  abstract = {Numerous events occur worldwide and are documented in the news, social media, and various online platforms in raw text. Extracting useful and succinct information about these events is crucial to various downstream applications. Event Argument Extraction (EAE) deals with the task of extracting event-specific information from natural language text. In order to cater to new events and domains in a realistic low-data setting, there is a growing urgency for EAE models to be generalizable. Consequentially, there is a necessity for benchmarking setups to evaluate the generalizability of EAE models. But most existing benchmarking datasets like ACE and ERE have limited coverage in terms of events and cannot adequately evaluate the generalizability of EAE models. To alleviate this issue, we introduce a new dataset GENEVA covering a diverse range of 115 events and 187 argument roles. Using this dataset, we create four benchmarking test suites to assess the model's generalization capability from different perspectives. We benchmark various representative models on these test suites and compare their generalizability relatively. Finally, we propose a new model SCAD that outperforms the previous models and serves as a strong benchmark for these test suites.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Parekh2022GENEVAPushingLimit_GENEVA.pdf;/Users/ma/Zotero/storage/L62NK93L/2205.html}
}

@misc{Parekh2023ContextualLabelProjection,
  title = {Contextual {{Label Projection}} for {{Cross-Lingual Structure Extraction}}},
  author = {Parekh, Tanmay and Hsu, I.-Hung and Huang, Kuan-Hao and Chang, Kai-Wei and Peng, Nanyun},
  year = {2023},
  month = sep,
  number = {arXiv:2309.08943},
  eprint = {2309.08943},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2309.08943},
  urldate = {2023-09-28},
  abstract = {Translating training data into target languages has proven beneficial for cross-lingual transfer. However, for structure extraction tasks, translating data requires a label projection step, which translates input text and obtains translated labels in the translated text jointly. Previous research in label projection mostly compromises translation quality by either facilitating easy identification of translated labels from translated text or using word-level alignment between translation pairs to assemble translated phrase-level labels from the aligned words. In this paper, we introduce CLAP, which first translates text to the target language and performs contextual translation on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We compare CLAP with other label projection techniques for creating pseudo-training data in target languages on event argument extraction, a representative structure extraction task. Results show that CLAP improves by 2-2.5 F1-score over other methods on the Chinese and Arabic ACE05 datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Parekh2023ContextualLabelProjection_Contextual Label Projection for Cross-Lingual Structure Extraction.pdf;/Users/ma/Zotero/storage/9XZM4BST/2309.html}
}

@inproceedings{Park2022GraphTextMultiModalPretraining,
  title = {Graph-{{Text Multi-Modal Pre-training}} for {{Medical Representation Learning}}},
  booktitle = {Proceedings of the {{Conference}} on {{Health}}, {{Inference}}, and {{Learning}}},
  author = {Park, Sungjin and Bae, Seongsu and Kim, Jiho and Kim, Tackeun and Choi, Edward},
  year = {2022},
  month = apr,
  pages = {261--281},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v174/park22a.html},
  urldate = {2023-10-31},
  abstract = {As the volume of Electronic Health Records (EHR) sharply grows, there has been emerging interest in learning the representation of EHR for healthcare applications. Representation learning of EHR requires appropriate modeling of the two dominant modalities in EHR: structured data and unstructured text. In this paper, we present MedGTX, a pre-trained model for multi-modal representation learning of the structured and textual EHR data. MedGTX uses a novel graph encoder to exploit the graphical nature of structured EHR data, and a text encoder to handle unstructured text, and a cross-modal encoder to learn a joint representation space. We pre-train our model through four proxy tasks on MIMIC-III, an open-source EHR data, and evaluate our model on two clinical benchmarks and three novel downstream tasks which tackle real-world problems in EHR data. The results consistently show the effectiveness of pre-training the model for joint representation of both structured and unstructured information from EHR. Given the promising performance of MedGTX, we believe this work opens a new door to jointly understanding the two fundamental modalities of EHR data.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Park2022GraphTextMultiModalPretraining_Graph-Text Multi-Modal Pre-training for Medical Representation Learning.pdf}
}

@misc{Park2023GenerativeAgentsInteractivea,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = aug,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.03442},
  url = {http://arxiv.org/abs/2304.03442},
  urldate = {2023-11-01},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Park2023GenerativeAgentsInteractivea_Generative Agents.pdf;/Users/ma/Zotero/storage/EPY9P4AW/2304.html}
}

@inproceedings{Parmar2022InBoXBARTGetInstructions,
  title = {In-{{BoXBART}}: {{Get Instructions}} into {{Biomedical Multi-Task Learning}}},
  shorttitle = {In-{{BoXBART}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Parmar, Mihir and Mishra, Swaroop and Purohit, Mirali and Luo, Man and Mohammad, Murad and Baral, Chitta},
  year = {2022},
  month = jul,
  pages = {112--128},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.10},
  url = {https://aclanthology.org/2022.findings-naacl.10},
  urldate = {2023-03-08},
  abstract = {Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multi-tasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multi-task generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional prompts for biomedical MTL. We introduce the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Using this meta-dataset, we propose a unified model termed as In-BoXBART, that can jointly learn all tasks of the BoX without any task-specific modules. To the best of our knowledge, this is the first attempt to propose a unified model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks. Experimental results indicate that the proposed model: 1) outperforms single-task baseline by \textbackslash textasciitilde3\% and multi-task (without instruction) baseline by \textbackslash textasciitilde18\% on an average, and 2) shows \textbackslash textasciitilde23\% improvement compared to single-task baseline in few-shot learning (i.e., 32 instances per task) on an average. Our analysis indicates that there is significant room for improvement across tasks in the BoX, implying the scope for future research direction.},
  keywords = {Instruction,Science Instructions},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Parmar2022InBoXBARTGetInstructions_In-BoXBART.pdf}
}

@misc{Parmar2023DonBlameAnnotator,
  title = {Don't {{Blame}} the {{Annotator}}: {{Bias Already Starts}} in the {{Annotation Instructions}}},
  shorttitle = {Don't {{Blame}} the {{Annotator}}},
  author = {Parmar, Mihir and Mishra, Swaroop and Geva, Mor and Baral, Chitta},
  year = {2023},
  month = feb,
  number = {arXiv:2205.00415},
  eprint = {2205.00415},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.00415},
  urldate = {2023-03-08},
  abstract = {In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Parmar2023DonBlameAnnotator_Don't Blame the Annotator.pdf;/Users/ma/Zotero/storage/K557FPVM/2205.html}
}

@misc{Peng2023AreYouCopying,
  title = {Are {{You Copying My Model}}? {{Protecting}} the {{Copyright}} of {{Large Language Models}} for {{EaaS}} via {{Backdoor Watermark}}},
  shorttitle = {Are {{You Copying My Model}}?},
  author = {Peng, Wenjun and Yi, Jingwei and Wu, Fangzhao and Wu, Shangxi and Zhu, Bin and Lyu, Lingjuan and Jiao, Binxing and Xu, Tong and Sun, Guangzhong and Xie, Xing},
  year = {2023},
  month = jun,
  number = {arXiv:2305.10036},
  eprint = {2305.10036},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.10036},
  urldate = {2023-07-18},
  abstract = {Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Peng2023AreYouCopying_Are You Copying My Model.pdf;/Users/ma/Zotero/storage/QBBWMX4E/2305.html}
}

@misc{Peng2023InstructionTuningGPT4,
  title = {Instruction {{Tuning}} with {{GPT-4}}},
  author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03277},
  eprint = {2304.03277},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.03277},
  url = {http://arxiv.org/abs/2304.03277},
  urldate = {2023-04-07},
  abstract = {Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Peng2023InstructionTuningGPT4_Instruction Tuning with GPT-4.pdf;/Users/ma/Zotero/storage/K9DFMVBP/2304.html}
}

@article{PengINSTRUCTIONTUNINGGPT4,
  title = {{{INSTRUCTION TUNING WITH GPT-4}}},
  author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  langid = {english},
  file = {/Users/ma/Zotero/storage/VLI6QKD3/Peng et al. - INSTRUCTION TUNING WITH GPT-4.pdf}
}

@article{Pennycook2020WhoFallsFake,
  title = {Who Falls for Fake News? {{The}} Roles of Bullshit Receptivity, Overclaiming, Familiarity, and Analytic Thinking},
  shorttitle = {Who Falls for Fake News?},
  author = {Pennycook, Gordon and Rand, David G.},
  year = {2020},
  journal = {Journal of Personality},
  volume = {88},
  number = {2},
  pages = {185--200},
  issn = {1467-6494},
  doi = {10.1111/jopy.12476},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jopy.12476},
  urldate = {2023-10-13},
  abstract = {Objective Fake news represents a particularly egregious and direct avenue by which inaccurate beliefs have been propagated via social media. We investigate the psychological profile of individuals who fall prey to fake news. Method We recruited 1,606 participants from Amazon's Mechanical Turk for three online surveys. Results The tendency to ascribe profundity to randomly generated sentences\textemdash pseudo-profound bullshit receptivity\textemdash correlates positively with perceptions of fake news accuracy, and negatively with the ability to differentiate between fake and real news (media truth discernment). Relatedly, individuals who overclaim their level of knowledge also judge fake news to be more accurate. We also extend previous research indicating that analytic thinking correlates negatively with perceived accuracy by showing that this relationship is not moderated by the presence/absence of the headline's source (which has no effect on accuracy), or by familiarity with the headlines (which correlates positively with perceived accuracy of fake and real news). Conclusion Our results suggest that belief in fake news may be driven, to some extent, by a general tendency to be overly accepting of weak claims. This tendency, which we refer to as reflexive open-mindedness, may be partly responsible for the prevalence of epistemically suspect beliefs writ large.},
  copyright = {\textcopyright{} 2019 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {analytic thinking,bullshit receptivity,fake news,news media,social media},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Pennycook2020WhoFallsFake_Who falls for fake news.pdf}
}

@inproceedings{PouranBenVeyseh2021ModelingDocumentLevelContext,
  title = {Modeling {{Document-Level Context}} for {{Event Detection}} via {{Important Context Selection}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Pouran Ben Veyseh, Amir and Nguyen, Minh Van and Ngo Trung, Nghia and Min, Bonan and Nguyen, Thien Huu},
  year = {2021},
  month = nov,
  pages = {5403--5413},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.439},
  url = {https://aclanthology.org/2021.emnlp-main.439},
  urldate = {2023-02-20},
  abstract = {The task of Event Detection (ED) in Information Extraction aims to recognize and classify trigger words of events in text. The recent progress has featured advanced transformer-based language models (e.g., BERT) as a critical component in state-of-the-art models for ED. However, the length limit for input texts is a barrier for such ED models as they cannot encode long-range document-level context that has been shown to be beneficial for ED. To address this issue, we propose a novel method to model document-level context for ED that dynamically selects relevant sentences in the document for the event prediction of the target sentence. The target sentence will be then augmented with the selected sentences and consumed entirely by transformer-based language models for improved representation learning for ED. To this end, the REINFORCE algorithm is employed to train the relevant sentence selection for ED. Several information types are then introduced to form the reward function for the training process, including ED performance, sentence similarity, and discourse relations. Our extensive experiments on multiple benchmark datasets reveal the effectiveness of the proposed model, leading to new state-of-the-art performance.},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/PouranBenVeyseh2021ModelingDocumentLevelContext_Modeling Document-Level Context for Event Detection via Important Context.pdf}
}

@inproceedings{PouranBenVeyseh2022DocumentLevelEventArgument,
  title = {Document-{{Level Event Argument Extraction}} via {{Optimal Transport}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Pouran Ben Veyseh, Amir and Nguyen, Minh Van and Dernoncourt, Franck and Min, Bonan and Nguyen, Thien},
  year = {2022},
  month = may,
  pages = {1648--1658},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.130},
  url = {https://aclanthology.org/2022.findings-acl.130},
  urldate = {2023-02-20},
  abstract = {Event Argument Extraction (EAE) is one of the sub-tasks of event extraction, aiming to recognize the role of each entity mention toward a specific event trigger. Despite the success of prior works in sentence-level EAE, the document-level setting is less explored. In particular, whereas syntactic structures of sentences have been shown to be effective for sentence-level EAE, prior document-level EAE models totally ignore syntactic structures for documents. Hence, in this work, we study the importance of syntactic structures in document-level EAE. Specifically, we propose to employ Optimal Transport (OT) to induce structures of documents based on sentence-level syntactic structures and tailored to EAE task. Furthermore, we propose a novel regularization technique to explicitly constrain the contributions of unrelated context words in the final prediction for EAE. We perform extensive experiments on the benchmark document-level EAE dataset RAMS that leads to the state-of-the-art performance. Moreover, our experiments on the ACE 2005 dataset reveals the effectiveness of the proposed model in the sentence-level EAE by establishing new state-of-the-art results.},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/PouranBenVeyseh2022DocumentLevelEventArgument_Document-Level Event Argument Extraction via Optimal Transport.pdf}
}

@misc{Qian2023CREATORToolCreation,
  title = {{{CREATOR}}: {{Tool Creation}} for {{Disentangling Abstract}} and {{Concrete Reasoning}} of {{Large Language Models}}},
  shorttitle = {{{CREATOR}}},
  author = {Qian, Cheng and Han, Chi and Fung, Yi R. and Qin, Yujia and Liu, Zhiyuan and Ji, Heng},
  year = {2023},
  month = oct,
  number = {arXiv:2305.14318},
  eprint = {2305.14318},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.14318},
  url = {http://arxiv.org/abs/2305.14318},
  urldate = {2023-10-26},
  abstract = {Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Qian2023CREATORToolCreation_CREATOR.pdf;/Users/ma/Zotero/storage/T6MCKKHF/2305.html}
}

@misc{Qin2023ChatGPTGeneralPurposeNatural,
  title = {Is {{ChatGPT}} a {{General-Purpose Natural Language Processing Task Solver}}?},
  author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06476},
  eprint = {2302.06476},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.06476},
  urldate = {2023-06-02},
  abstract = {Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Qin2023ChatGPTGeneralPurposeNatural_Is ChatGPT a General-Purpose Natural Language Processing Task Solver.pdf;/Users/ma/Zotero/storage/3FY97HCT/2302.html}
}

@misc{Raffel2023ExploringLimitsTransfer,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2023},
  month = sep,
  number = {arXiv:1910.10683},
  eprint = {1910.10683},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.10683},
  url = {http://arxiv.org/abs/1910.10683},
  urldate = {2023-10-02},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Raffel2023ExploringLimitsTransfer_Exploring the Limits of Transfer Learning with a Unified Text-to-Text.pdf;/Users/ma/Zotero/storage/6335RVAV/1910.html}
}

@article{Rasmy2021MedBERTPretrainedContextualized,
  title = {Med-{{BERT}}: Pretrained Contextualized Embeddings on Large-Scale Structured Electronic Health Records for Disease Prediction},
  shorttitle = {Med-{{BERT}}},
  author = {Rasmy, Laila and Xiang, Yang and Xie, Ziqian and Tao, Cui and Zhi, Degui},
  year = {2021},
  month = may,
  journal = {npj Digital Medicine},
  volume = {4},
  number = {1},
  pages = {1--13},
  publisher = {{Nature Publishing Group}},
  issn = {2398-6352},
  doi = {10.1038/s41746-021-00455-y},
  url = {https://www.nature.com/articles/s41746-021-00455-y},
  urldate = {2023-10-17},
  abstract = {Deep learning (DL)-based predictive models from electronic health records (EHRs) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required by these models to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pretraining of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. Inspired by BERT, we propose Med-BERT, which adapts the BERT framework originally developed for the text domain to the structured EHR domain. Med-BERT is a contextualized embedding model pretrained on a structured EHR dataset of 28,490,650 patients. Fine-tuning experiments showed that Med-BERT substantially improves the prediction accuracy, boosting the area under the receiver operating characteristics curve (AUC) by 1.21\textendash 6.14\% in two disease prediction tasks from two clinical databases. In particular, pretrained Med-BERT obtains promising performances on tasks with small fine-tuning training sets and can boost the AUC by more than 20\% or obtain an AUC as high as a model trained on a training set ten times larger, compared with deep learning models without Med-BERT. We believe that Med-BERT will benefit disease prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Disease prevention,Experimental models of disease,Health care},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Rasmy2021MedBERTPretrainedContextualized_Med-BERT.pdf}
}

@inproceedings{Reif2022RecipeArbitraryText,
  title = {A {{Recipe}} for {{Arbitrary Text Style Transfer}} with {{Large Language Models}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and {Callison-Burch}, Chris and Wei, Jason},
  year = {2022},
  month = may,
  pages = {837--848},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-short.94},
  url = {https://aclanthology.org/2022.acl-short.94},
  urldate = {2023-10-23},
  abstract = {In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as `make this melodramatic' or `insert a metaphor.'},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Reif2022RecipeArbitraryText_A Recipe for Arbitrary Text Style Transfer with Large Language Models.pdf}
}

@inproceedings{Ren2022CLIORoleinteractiveMultievent,
  title = {{{CLIO}}: {{Role-interactive Multi-event Head Attention Network}} for {{Document-level Event Extraction}}},
  shorttitle = {{{CLIO}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Ren, Yubing and Cao, Yanan and Fang, Fang and Guo, Ping and Lin, Zheng and Ma, Wei and Liu, Yi},
  year = {2022},
  month = oct,
  pages = {2504--2514},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  url = {https://aclanthology.org/2022.coling-1.221},
  urldate = {2023-02-09},
  abstract = {Transforming the large amounts of unstructured text on the Internet into structured event knowledge is a critical, yet unsolved goal of NLP, especially when addressing document-level text. Existing methods struggle in Document-level Event Extraction (DEE) due to its two intrinsic challenges: (a) Nested arguments, which means one argument is the sub-string of another one. (b) Multiple events, which indicates we should identify multiple events and assemble the arguments for them. In this paper, we propose a role-interactive multi-event head attention network (CLIO) to solve these two challenges jointly. The key idea is to map different events to multiple subspaces (i.e. multi-event head). In each event subspace, we draw the semantic representation of each role closer to its corresponding arguments, then we determine whether the current event exists. To further optimize event representation, we propose an event representation enhancing strategy to regularize pre-trained embedding space to be more isotropic. Our experiments on two widely used DEE datasets show that CLIO achieves consistent improvements over previous methods.},
  keywords = {dataset\_MUC4,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ren2022CLIORoleinteractiveMultievent_CLIO.pdf}
}

@misc{Rodrigues-Jr2019PatientTrajectoryPrediction,
  title = {Patient Trajectory Prediction in the {{Mimic-III}} Dataset, Challenges and Pitfalls},
  author = {{Rodrigues-Jr}, Jose F. and Spadon, Gabriel and Brandoli, Bruno and {Amer-Yahia}, Sihem},
  year = {2019},
  month = nov,
  number = {arXiv:1909.04605},
  eprint = {1909.04605},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1909.04605},
  urldate = {2023-11-02},
  abstract = {Automated medical prognosis has gained interest as artificial intelligence evolves and the potential for computer-aided medicine becomes evident. Nevertheless, it is challenging to design an effective system that, given a patient's medical history, is able to predict probable future conditions. Previous works, mostly carried out over private datasets, have tackled the problem by using artificial neural network architectures that cannot deal with low-cardinality datasets, or by means of non-generalizable inference approaches. We introduce a Deep Learning architecture whose design results from an intensive experimental process. The final architecture is based on two parallel Minimal Gated Recurrent Unit networks working in bi-directional manner, which was extensively tested with the open-access Mimic-III dataset. Our results demonstrate significant improvements in automated medical prognosis, as measured with Recall@k. We summarize our experience as a set of relevant insights for the design of Deep Learning architectures. Our work improves the performance of computer-aided medicine and can serve as a guide in designing artificial neural networks used in prediction tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Rodrigues-Jr2019PatientTrajectoryPrediction_Patient trajectory prediction in the Mimic-III dataset, challenges and pitfalls.pdf;/Users/ma/Zotero/storage/SBY6ZIB4/1909.html}
}

@article{Roozenbeek2020PrebunkingInterventionsBased,
  title = {Prebunking Interventions Based on ``Inoculation'' Theory Can Reduce Susceptibility to Misinformation across Cultures},
  author = {Roozenbeek, Jon and van der Linden, Sander and Nygren, Thomas},
  year = {2020},
  month = feb,
  journal = {Harvard Kennedy School Misinformation Review},
  volume = {1},
  number = {2},
  doi = {10.37016//mr-2020-008},
  url = {https://misinforeview.hks.harvard.edu/article/global-vaccination-badnews/},
  urldate = {2023-10-12},
  abstract = {This study finds that the online ``fake news'' game, Bad News, can confer psychological resistance against common online misinformation strategies (e.g., conspiracy theories, manipulating emotions, political polarization) across different cultures (Sweden, German, Poland, and Greece). The intervention draws on the theory of psychological inoculation: analogous to the process of medical immunization, we find that ``prebunking'' or preemptively warning and exposing people to weakened doses of misinformation can help cultivate ``mental antibodies'' against fake news. We conclude that social impact games rooted in basic insights from social psychology can boost immunity against misinformation across a variety of cultural, linguistic, and political settings.},
  langid = {american},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Roozenbeek2020PrebunkingInterventionsBased_Prebunking interventions based on “inoculation” theory can reduce.pdf}
}

@article{Roozenbeek2020SusceptibilityMisinformationCOVID19,
  title = {Susceptibility to Misinformation about {{COVID-19}} around the World},
  author = {Roozenbeek, Jon and Schneider, Claudia R. and Dryhurst, Sarah and Kerr, John and Freeman, Alexandra L. J. and Recchia, Gabriel and {van der Bles}, Anne Marthe and {van der Linden}, Sander},
  year = {2020},
  month = oct,
  journal = {Royal Society Open Science},
  volume = {7},
  number = {10},
  pages = {201199},
  issn = {2054-5703},
  doi = {10.1098/rsos.201199},
  abstract = {Misinformation about COVID-19 is a major threat to public health. Using five national samples from the UK (n = 1050 and n = 1150), Ireland (n = 700), the USA (n = 700), Spain (n = 700) and Mexico (n = 700), we examine predictors of belief in the most common statements about the virus that contain misinformation. We also investigate the prevalence of belief in COVID-19 misinformation across different countries and the role of belief in such misinformation in predicting relevant health behaviours. We find that while public belief in misinformation about COVID-19 is not particularly common, a substantial proportion views this type of misinformation as highly reliable in each country surveyed. In addition, a small group of participants find common factual information about the virus highly unreliable. We also find that increased susceptibility to misinformation negatively affects people's self-reported compliance with public health guidance about COVID-19, as well as people's willingness to get vaccinated against the virus and to recommend the vaccine to vulnerable friends and family. Across all countries surveyed, we find that higher trust in scientists and having higher numeracy skills were associated with lower susceptibility to coronavirus-related misinformation. Taken together, these results demonstrate a clear link between susceptibility to misinformation and both vaccine hesitancy and a reduced likelihood to comply with health guidance measures, and suggest that interventions which aim to improve critical thinking and trust in science may be a promising avenue for future research.},
  langid = {english},
  pmcid = {PMC7657933},
  pmid = {33204475},
  keywords = {COVID-19,fake news,misinformation,vaccine hesitancy},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Roozenbeek2020SusceptibilityMisinformationCOVID19_Susceptibility to misinformation about COVID-19 around the world.pdf}
}

@article{Roozenbeek2022PsychologicalInoculationImproves,
  title = {Psychological Inoculation Improves Resilience against Misinformation on Social Media},
  author = {Roozenbeek, Jon and {van der Linden}, Sander and Goldberg, Beth and Rathje, Steve and Lewandowsky, Stephan},
  year = {2022},
  month = aug,
  journal = {Science Advances},
  volume = {8},
  number = {34},
  pages = {eabo6254},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/sciadv.abo6254},
  url = {https://www.science.org/doi/10.1126/sciadv.abo6254},
  urldate = {2023-10-12},
  abstract = {Online misinformation continues to have adverse consequences for society. Inoculation theory has been put forward as a way to reduce susceptibility to misinformation by informing people about how they might be misinformed, but its scalability has been elusive both at a theoretical level and a practical level. We developed five short videos that inoculate people against manipulation techniques commonly used in misinformation: emotionally manipulative language, incoherence, false dichotomies, scapegoating, and ad hominem attacks. In seven preregistered studies, i.e., six randomized controlled studies (n = 6464) and an ecologically valid field study on YouTube (n = 22,632), we find that these videos improve manipulation technique recognition, boost confidence in spotting these techniques, increase people's ability to discern trustworthy from untrustworthy content, and improve the quality of their sharing decisions. These effects are robust across the political spectrum and a wide variety of covariates. We show that psychological inoculation campaigns on social media are effective at improving misinformation resilience at scale.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Roozenbeek2022PsychologicalInoculationImproves_Psychological inoculation improves resilience against misinformation on social.pdf}
}

@article{Roozenbeek2022SusceptibilityMisinformationConsistent,
  title = {Susceptibility to Misinformation Is Consistent across Question Framings and Response Modes and Better Explained by Myside Bias and Partisanship than Analytical Thinking},
  author = {Roozenbeek, Jon and Maertens, Rakoen and Herzog, Stefan M. and Geers, Michael and Kurvers, Ralf and Sultan, Mubashir and van der Linden, Sander},
  year = {2022},
  month = may,
  journal = {Judgment and Decision Making},
  volume = {17},
  number = {3},
  pages = {547--573},
  publisher = {{Cambridge University Press}},
  issn = {1930-2975},
  doi = {10.1017/S1930297500003570},
  url = {https://www.cambridge.org/core/journals/judgment-and-decision-making/article/susceptibility-to-misinformation-is-consistent-across-question-framings-and-response-modes-and-better-explained-by-myside-bias-and-partisanship-than-analytical-thinking/1121D46E918815B2CD7A9C6C237464A2\#},
  urldate = {2023-10-12},
  abstract = {Misinformation presents a significant societal problem. To measure individuals' susceptibility to misinformation and study its predictors, researchers have used a broad variety of ad-hoc item sets, scales, question framings, and response modes. Because of this variety, it remains unknown whether results from different studies can be compared (e.g., in meta-analyses). In this preregistered study (US sample; N = 2,622), we compare five commonly used question framings (eliciting perceived headline accuracy, manipulativeness, reliability, trustworthiness, and whether a headline is real or fake) and three response modes (binary, 6-point and 7-point scales), using the psychometrically validated Misinformation Susceptibility Test (MIST). We test 1) whether different question framings and response modes yield similar responses for the same item set, 2) whether people's confidence in their primary judgments is affected by question framings and response modes, and 3) which key psychological factors (myside bias, political partisanship, cognitive reflection, and numeracy skills) best predict misinformation susceptibility across assessment methods. Different response modes and question framings yield similar (but not identical) responses for both primary ratings and confidence judgments. We also find a similar nomological net across conditions, suggesting cross-study comparability. Finally, myside bias and political conservatism were strongly positively correlated with misinformation susceptibility, whereas numeracy skills and especially cognitive reflection were less important (although we note potential ceiling effects for numeracy). We thus find more support for an ``integrative'' account than a ``classical reasoning'' account of misinformation belief.},
  langid = {english},
  keywords = {actively open-minded thinking,analytical thinking,confidence,fake news,misinformation,partisanship,susceptibility},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Roozenbeek2022SusceptibilityMisinformationConsistent_Susceptibility to misinformation is consistent across question framings and.pdf}
}

@article{Rosenzweig2021HappinessSurpriseAre,
  title = {Happiness and Surprise Are Associated with Worse Truth Discernment of {{COVID-19}} Headlines among Social Media Users in {{Nigeria}}},
  author = {Rosenzweig, Leah R. and Bago, Bence and Berinsky, Adam J. and Rand, David G.},
  year = {2021},
  month = aug,
  journal = {Harvard Kennedy School Misinformation Review},
  doi = {10.37016/mr-2020-75},
  url = {https://misinforeview.hks.harvard.edu/article/happiness-and-surprise-are-associated-with-worse-truth-discernment-of-covid-19-headlines-among-social-media-users-in-nigeria/},
  urldate = {2023-10-13},
  abstract = {Do emotions we experience after reading headlines help us discern true from false information or cloud our judgement? Understanding whether emotions are associated with distinguishing truth from fiction and sharing information has implications for interventions designed to curb the spread of misinformation. Among 1,341 Facebook users in Nigeria, we find that emotions\textemdash specifically happiness and surprise\textemdash are},
  langid = {american},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Rosenzweig2021HappinessSurpriseAre_Happiness and surprise are associated with worse truth discernment of COVID-19.pdf}
}

@article{Ruani2023SusceptibilityCOVID19Nutrition,
  title = {Susceptibility to {{COVID-19 Nutrition Misinformation}} and {{Eating Behavior Change}} during {{Lockdowns}}: {{An International Web-Based Survey}}},
  shorttitle = {Susceptibility to {{COVID-19 Nutrition Misinformation}} and {{Eating Behavior Change}} during {{Lockdowns}}},
  author = {Ruani, Maria A. and Reiss, Michael J.},
  year = {2023},
  month = jan,
  journal = {Nutrients},
  volume = {15},
  number = {2},
  pages = {451},
  issn = {2072-6643},
  doi = {10.3390/nu15020451},
  url = {https://www.mdpi.com/2072-6643/15/2/451},
  urldate = {2023-10-12},
  abstract = {To understand the susceptibility to nutrition-health misinformation related to preventing, treating, or mitigating the risk of COVID-19 during the initial lockdowns around the world, the present international web-based survey study (15 April\textendash 15 May 2020) gauged participants' (n = 3707) level of nutrition-health misinformation discernment by presenting them with 25 statements (including unfounded or unproven claims circulated at the time), alongside the influence of information sources of varying quality on the frequency of changes in their eating behavior and the extent of misinformation held, depending on the source used for such changes. Results revealed widespread misinformation about food, eating, and health practices related to COVID-19, with the 25 statements put to participants receiving up to 43\% misinformed answers (e.g., `It is safe to eat fruits and vegetables that have been washed with soap or diluted bleach'). Whereas higher quality information sources (nutrition scientists, nutrition professionals) had the biggest influence on eating behavior change, we found greater misinformation susceptibility when relying on poor quality sources for changing diet. Appropriate discernment of misinformation was weakest amongst participants who more frequently changed their eating behavior because of information from poor quality sources, suggesting disparities in the health risks/safety of the changes performed.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/LYATJMT5/Ruani and Reiss - 2023 - Susceptibility to COVID-19 Nutrition Misinformatio.pdf}
}

@inproceedings{Rupp2023ExBEHRTExtendedTransformer,
  title = {{{ExBEHRT}}: {{Extended Transformer}} for~{{Electronic Health Records}}},
  shorttitle = {{{ExBEHRT}}},
  booktitle = {Trustworthy {{Machine Learning}}  for {{Healthcare}}},
  author = {Rupp, Maurice and Peter, Oriane and Pattipaka, Thirupathi},
  editor = {Chen, Hao and Luo, Luyang},
  year = {2023},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {73--84},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-39539-0_7},
  abstract = {In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health record data) and applied various algorithms to interpret its results. While BEHRT only considers diagnoses and patient age, we extend the feature space to several multi-modal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications and lab tests by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various down-stream tasks in different diseases. To ensure robustness, we interpret the model predictions using an adaption of expected gradients, which has not been applied to transformers with EHR data so far and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the models' representations of oncology patients, we show that the model has implicit understanding of the disease and is able to classify patients with same cancer type into different risk groups. Given the additional features and interpretability, ExBEHRT can help making informed decisions about disease progressions, diagnoses and risk factors of various diseases.},
  isbn = {978-3-031-39539-0},
  langid = {english},
  keywords = {BERT,Interpretability,Patient Subtyping,RWE},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Rupp2023ExBEHRTExtendedTransformer_ExBEHRT.pdf}
}

@misc{Saha2023BranchSolveMergeImprovesLarge,
  title = {Branch-{{Solve-Merge Improves Large Language Model Evaluation}} and {{Generation}}},
  author = {Saha, Swarnadeep and Levy, Omer and Celikyilmaz, Asli and Bansal, Mohit and Weston, Jason and Li, Xian},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15123},
  eprint = {2310.15123},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.15123},
  url = {http://arxiv.org/abs/2310.15123},
  urldate = {2023-10-25},
  abstract = {Large Language Models (LLMs) are frequently used for multi-faceted language generation and evaluation tasks that involve satisfying intricate user constraints or taking into account multiple aspects and criteria. However, their performance can fall short, due to the model's lack of coherence and inability to plan and decompose the problem. We propose Branch-Solve-Merge (BSM), a Large Language Model program (Schlag et al., 2023) for tackling such challenging natural language tasks. It consists of branch, solve, and merge modules that are parameterized with specific prompts to the base LLM. These three modules plan a decomposition of the task into multiple parallel sub-tasks, independently solve them, and fuse the solutions to the sub-tasks. We apply our method to the tasks of LLM response evaluation and constrained text generation and evaluate its effectiveness with multiple LLMs, including Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and consistency for each LLM by enhancing human-LLM agreement by up to 26\%, reducing length and pairwise position biases by up to 50\%, and allowing LLaMA-2-chat to match or outperform GPT-4 on most domains. On the constraint story generation task, BSM improves the coherence of the stories while also improving constraint satisfaction by 12\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Saha2023BranchSolveMergeImprovesLarge_Branch-Solve-Merge Improves Large Language Model Evaluation and Generation.pdf;/Users/ma/Zotero/storage/FPBJYW95/2310.html}
}

@misc{Sahu2022UnpackingLargeLanguage,
  title = {Unpacking {{Large Language Models}} with {{Conceptual Consistency}}},
  author = {Sahu, Pritish and Cogswell, Michael and Gong, Yunye and Divakaran, Ajay},
  year = {2022},
  month = sep,
  number = {arXiv:2209.15093},
  eprint = {2209.15093},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.15093},
  url = {http://arxiv.org/abs/2209.15093},
  urldate = {2023-10-17},
  abstract = {If a Large Language Model (LLM) answers "yes" to the question "Are mountains tall?" then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Sahu2022UnpackingLargeLanguage_Unpacking Large Language Models with Conceptual Consistency.pdf;/Users/ma/Zotero/storage/6KH8J45A/2209.html}
}

@inproceedings{Sainz2022TextualEntailmentEvent,
  title = {Textual {{Entailment}} for {{Event Argument Extraction}}: {{Zero-}} and {{Few-Shot}} with {{Multi-Source Learning}}},
  shorttitle = {Textual {{Entailment}} for {{Event Argument Extraction}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Sainz, Oscar and {Gonzalez-Dios}, Itziar and {Lopez de Lacalle}, Oier and Min, Bonan and Agirre, Eneko},
  year = {2022},
  month = jul,
  pages = {2439--2455},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.187},
  url = {https://aclanthology.org/2022.findings-naacl.187},
  urldate = {2023-02-09},
  abstract = {Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as a Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50\% and 20\% in ACE and WikiEvents, respectively, while achieving the same performance as with full training. More importantly, we show that recasting EAE as entailment alleviates the dependency on schemas, which has been a roadblock for transferring annotations between domains. Thanks to entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10\% and 5\% (respectively) of the full training without transfer.Our analysis shows that key to good results is the use of several entailment datasets to pre-train the entailment model. Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument types is needed; comparable results can be achieved from users of different level of expertise.},
  keywords = {dataset\_WikiEvents,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Sainz2022TextualEntailmentEvent_Textual Entailment for Event Argument Extraction.pdf}
}

@article{Salovich2021CanConfidenceHelp,
  title = {Can Confidence Help Account for and Redress the Effects of Reading Inaccurate Information?},
  author = {Salovich, Nikita A. and Donovan, Amalia M. and Hinze, Scott R. and Rapp, David N.},
  year = {2021},
  month = feb,
  journal = {Memory \& Cognition},
  volume = {49},
  number = {2},
  pages = {293--310},
  issn = {1532-5946},
  doi = {10.3758/s13421-020-01096-4},
  url = {https://doi.org/10.3758/s13421-020-01096-4},
  urldate = {2023-10-13},
  abstract = {Being exposed to inaccurate information in fiction can negatively influence post-reading judgments and decisions. For example, people make more errors judging the validity of statements after reading stories containing related inaccurate as compared to related accurate assertions. While these effects have been demonstrated in a variety of studies, people's confidence in their post-reading judgments has received little attention. The current experiments examined whether exposure to accurate and inaccurate information embedded in fiction influences readers' confidence in judging the validity of related claims. Participants read an extended story containing accurate and inaccurate assertions about the world (Experiment 1a) or a control story omitting those assertions (Experiment 1b). Afterwards they judged the validity of single statements related to the critical assertions and provided confidence ratings for each judgment. While participants made more judgment errors after having read inaccurate assertions than after having read accurate assertions or stories without assertions, they were overall less confident in their incorrect as compared to correct judgments. Given the observed relationship between confidence and judgment accuracy, in Experiments 2 and 3 we tested whether allowing and instructing participants to withhold responses might reduce judgment errors. This withholding option reduced participants' incorrect and correct judgments, failing to specifically eliminate the negative consequences of exposure to inaccurate assertions. These findings are discussed with respect to accounts documenting the influence of inaccurate information, and highlight confidence as a relevant but understudied factor in previous empirical demonstrations of such effects.},
  langid = {english},
  keywords = {Confidence,Inaccurate information,Judgments,Metacognition,Reading comprehension},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Salovich2021CanConfidenceHelp_Can confidence help account for and redress the effects of reading inaccurate.pdf}
}

@inproceedings{Sanh2020LearningOthersMistakes,
  title = {Learning from Others' Mistakes: {{Avoiding}} Dataset Biases without Modeling Them},
  shorttitle = {Learning from Others' Mistakes},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sanh, Victor and Wolf, Thomas and Belinkov, Yonatan and Rush, Alexander M.},
  year = {2020},
  month = oct,
  url = {https://openreview.net/forum?id=Hf3qXoiNkR},
  urldate = {2023-08-27},
  abstract = {State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Sanh2020LearningOthersMistakes_Learning from others' mistakes.pdf}
}

@misc{Schaeffer2023AreEmergentAbilities,
  title = {Are {{Emergent Abilities}} of {{Large Language Models}} a {{Mirage}}?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = apr,
  number = {arXiv:2304.15004},
  eprint = {2304.15004},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-05-01},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities, (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show how similar metric decisions suggest apparent emergent abilities on vision tasks in diverse deep network architectures (convolutional, autoencoder, transformers). In all three analyses, we find strong supporting evidence that emergent abilities may not be a fundamental property of scaling AI models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Schaeffer2023AreEmergentAbilities_Are Emergent Abilities of Large Language Models a Mirage.pdf;/Users/ma/Zotero/storage/V4AVVC63/2304.html}
}

@article{Scherer2020WhoSusceptibleOnline,
  title = {Who {{Is Susceptible}} to {{Online Health Misinformation}}?},
  author = {Scherer, Laura D. and Pennycook, Gordon},
  year = {2020},
  month = oct,
  journal = {American Journal of Public Health},
  volume = {110},
  number = {Suppl 3},
  pages = {S276-S277},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2020.305908},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7532320/},
  urldate = {2023-10-12},
  pmcid = {PMC7532320},
  pmid = {33001736},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Scherer2020WhoSusceptibleOnline_Who Is Susceptible to Online Health Misinformation2.pdf}
}

@article{Scherer2021WhoSusceptibleOnline,
  title = {Who Is Susceptible to Online Health Misinformation? {{A}} Test of Four Psychosocial Hypotheses},
  shorttitle = {Who Is Susceptible to Online Health Misinformation?},
  author = {Scherer, Laura D. and McPhetres, Jon and link will open in a new tab {Link to external site}, this and Pennycook, Gordon and Kempe, Allison and Allen, Larry A. and link will open in a new tab {Link to external site}, this and Knoepke, Christopher E. and link will open in a new tab {Link to external site}, this and Tate, Channing E. and link will open in a new tab {Link to external site}, this and Matlock, Daniel D.},
  year = {2021},
  month = apr,
  journal = {Health Psychology},
  volume = {40},
  number = {4},
  pages = {274--284},
  publisher = {{American Psychological Association}},
  address = {{Washington, US}},
  issn = {0278-6133},
  doi = {10.1037/hea0000978},
  url = {https://www.proquest.com/docview/2494542278/abstract/A58493CB0C114E19PQ/1},
  urldate = {2023-10-12},
  abstract = {Objective: Health misinformation on social media threatens public health. One question that could lend insight into how and through whom misinformation spreads is whether certain people are susceptible to many types of health misinformation, regardless of the health topic at hand. This study provided an initial answer to this question and also tested four hypotheses concerning the psychosocial attributes of people who are susceptible to health misinformation: (1) deficits in knowledge or skill, (2) preexisting attitudes, (3) trust in health care and/or science, and (4) cognitive miserliness. Method: Participants in a national U.S. survey (N = 923) rated the perceived accuracy and influence of true and false social media posts about statin medications, cancer treatment, and the Human Papilloma Virus (HPV) vaccine and then responded to individual difference and demographic questions. Results: Perceived accuracy of health misinformation was strongly correlated across statins, cancer, and the HPV vaccine (rs {$\geq$} .70), indicating that individuals who are susceptible to misinformation about one of these topics are very likely to believe misinformation about the other topics as well. Misinformation susceptibility across all three topics was most strongly predicted by lower educational attainment and health literacy, distrust in the health care system, and positive attitudes toward alternative medicine. Conclusions: A person who is susceptible to online misinformation about one health topic may be susceptible to many types of health misinformation. Individuals who were more susceptible to health misinformation had less education and health literacy, less health care trust, and more positive attitudes toward alternative medicine. (PsycInfo Database Record (c) 2021 APA, all rights reserved) (Source: journal abstract)},
  copyright = {\textcopyright{} 2021, American Psychological Association},
  langid = {english},
  keywords = {Drug Therapy (major),Health Attitudes,Health Care Services,Health Knowledge,Human Papillomavirus,Immunization (major),Neoplasms (major),Psychosocial Factors,Social Media (major),Statins,Treatment (major),Trust (Social Behavior)},
  annotation = {(US)},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Scherer2021WhoSusceptibleOnline_Who is susceptible to online health misinformation.pdf}
}

@inproceedings{Schick2021GeneratingDatasetsPretrained,
  title = {Generating {{Datasets}} with {{Pretrained Language Models}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Schick, Timo and Sch{\"u}tze, Hinrich},
  year = {2021},
  month = nov,
  pages = {6943--6951},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.555},
  url = {https://aclanthology.org/2021.emnlp-main.555},
  urldate = {2023-06-24},
  abstract = {To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Schick2021GeneratingDatasetsPretrained_Generating Datasets with Pretrained Language Models.pdf}
}

@misc{Shaikh2022SecondThoughtLet,
  title = {On {{Second Thought}}, {{Let}}'s {{Not Think Step}} by {{Step}}! {{Bias}} and {{Toxicity}} in {{Zero-Shot Reasoning}} 222},
  author = {Shaikh, Omar and Zhang, Hongxin and Held, William and Bernstein, Michael and Yang, Diyi},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08061},
  eprint = {2212.08061},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08061},
  url = {http://arxiv.org/abs/2212.08061},
  urldate = {2023-01-31},
  abstract = {Generating a chain of thought (CoT) can increase large language model (LLM) performance on a wide range of tasks. Zero-shot CoT evaluations, however, have been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In this paper, we perform a controlled evaluation of zero-shot CoT across two sensitive domains: harmful questions and stereotype benchmarks. We find that using zero-shot CoT reasoning in a prompt can significantly increase a model's likelihood to produce undesirable output. Without future advances in alignment or explicit mitigation instructions, zero-shot CoT should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Shaikh2022SecondThoughtLet_On Second Thought, Let's Not Think Step by Step.pdf;/Users/ma/Zotero/storage/P64BPVKV/2212.html}
}

@inproceedings{Shang2019PretrainingGraphAugmentedb,
  title = {Pre-Training of {{Graph Augmented Transformers}} for {{Medication Recommendation}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Shang, Junyuan and Ma, Tengfei and Xiao, Cao and Sun, Jimeng},
  year = {2019},
  month = aug,
  pages = {5953--5959},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/825},
  url = {https://www.ijcai.org/proceedings/2019/825},
  urldate = {2023-11-03},
  abstract = {Medication recommendation is an important healthcare application. It is commonly formulated as a temporal prediction task. Hence, most existing works only utilize longitudinal electronic health records (EHRs) from a small number of patients with multiple visits ignoring a large number of patients with a single visit (selection bias). Moreover, important hierarchical knowledge such as diagnosis hierarchy is not leveraged in the representation learning process. To address these challenges, we propose G-BERT, a new model to combine the power of Graph Neural Networks (GNNs) and BERT (Bidirectional Encoder Representations from Transformers) for medical code representation and medication recommendation. We use GNNs to represent the internal hierarchical structures of medical codes. Then we integrate the GNN representation into a transformer-based visit encoder and pre-train it on EHR data from patients only with a single visit. The pre-trained visit encoder and representation are then fine-tuned for downstream predictive tasks on longitudinal EHRs from patients with multiple visits. G-BERT is the first to bring the language model pre-training schema into the healthcare domain and it achieved state-of-the-art performance on the medication recommendation task.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/Users/ma/Zotero/storage/KJIDAYE3/Shang et al. - 2019 - Pre-training of Graph Augmented Transformers for M.pdf}
}

@misc{Shao2023SyntheticPromptingGenerating,
  title = {Synthetic {{Prompting}}: {{Generating Chain-of-Thought Demonstrations}} for {{Large Language Models}}},
  shorttitle = {Synthetic {{Prompting}}},
  author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00618},
  eprint = {2302.00618},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.00618},
  urldate = {2023-05-12},
  abstract = {Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Shao2023SyntheticPromptingGenerating_Synthetic Prompting.pdf;/Users/ma/Zotero/storage/DV6YBBQN/2302.html}
}

@article{Sharma2023SystematicReviewRelationship,
  title = {A Systematic Review of the Relationship between Emotion and Susceptibility to Misinformation},
  author = {Sharma, Prerika R. and Wade, Kimberley A. and Jobson, Laura},
  year = {2023},
  month = jan,
  journal = {Memory (Hove, England)},
  volume = {31},
  number = {1},
  pages = {1--21},
  issn = {1464-0686},
  doi = {10.1080/09658211.2022.2120623},
  abstract = {Inaccurate memory reports can have serious consequences within forensic and clinical settings, where emotion and misinformation are two common sources of memory distortion. Many studies have investigated how these factors are related; does emotion protect memory or leave it more vulnerable to the distorting effects of misinformation? The findings remain diffused. Thus, the present review aimed to clarify the relationship between emotion and susceptibility to misinformation. 39 eligible studies were reviewed. Results varied according to the type and dimension of emotion measured. Level of arousal may be unrelated to susceptibility to misinformation when retrieval occurs without delay; studies including delayed retrieval were limited. Stimuli valence may be associated with increased susceptibility to peripheral misinformation but unrelated to other misinformation. The following results were reported by limited studies: short-term distress and moderate levels of stress may decrease susceptibility, while anger and greater cortisol response to stress may increase susceptibility to misinformation. Source memory may also be unaffected by emotion. The results have important potential implications for forensic and clinical practice, for example by highlighting the value of enquiring witnesses' source memory. Methodological recommendations for future studies are made.},
  langid = {english},
  pmid = {36093958},
  keywords = {Arousal,Communication,Emotion,Emotions,Health Status,Humans,Memory Disorders,Mental Recall,misinformation,suggestibility,suggestion-induced false memories}
}

@misc{Shen2020SimpleToughtoBeatData,
  title = {A {{Simple}} but {{Tough-to-Beat Data Augmentation Approach}} for {{Natural Language Understanding}} and {{Generation}}},
  author = {Shen, Dinghan and Zheng, Mingzhi and Shen, Yelong and Qu, Yanru and Chen, Weizhu},
  year = {2020},
  month = oct,
  number = {arXiv:2009.13818},
  eprint = {2009.13818},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.13818},
  url = {http://arxiv.org/abs/2009.13818},
  urldate = {2023-02-17},
  abstract = {Adversarial training has been shown effective at endowing the learned representations with stronger generalization ability. However, it typically requires expensive computation to determine the direction of the injected perturbations. In this paper, we introduce a set of simple yet effective data augmentation strategies dubbed cutoff, where part of the information within an input sentence is erased to yield its restricted views (during the fine-tuning stage). Notably, this process relies merely on stochastic sampling and thus adds little computational overhead. A Jensen-Shannon Divergence consistency loss is further utilized to incorporate these augmented samples into the training objective in a principled manner. To verify the effectiveness of the proposed strategies, we apply cutoff to both natural language understanding and generation problems. On the GLUE benchmark, it is demonstrated that cutoff, in spite of its simplicity, performs on par or better than several competitive adversarial-based approaches. We further extend cutoff to machine translation and observe significant gains in BLEU scores (based upon the Transformer Base model). Moreover, cutoff consistently outperforms adversarial training and achieves state-of-the-art results on the IWSLT2014 German-English dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Shen2020SimpleToughtoBeatData_A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language.pdf;/Users/ma/Zotero/storage/HEIIUR3P/2009.html}
}

@misc{Shi2023BadGPTExploringSecurity,
  title = {{{BadGPT}}: {{Exploring Security Vulnerabilities}} of {{ChatGPT}} via {{Backdoor Attacks}} to {{InstructGPT}}},
  shorttitle = {{{BadGPT}}},
  author = {Shi, Jiawen and Liu, Yixin and Zhou, Pan and Sun, Lichao},
  year = {2023},
  month = feb,
  number = {arXiv:2304.12298},
  eprint = {2304.12298},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12298},
  url = {http://arxiv.org/abs/2304.12298},
  urldate = {2023-04-25},
  abstract = {Recently, ChatGPT has gained significant attention in research due to its ability to interact with humans effectively. The core idea behind this model is reinforcement learning (RL) fine-tuning, a new paradigm that allows language models to align with human preferences, i.e., InstructGPT. In this study, we propose BadGPT, the first backdoor attack against RL fine-tuning in language models. By injecting a backdoor into the reward model, the language model can be compromised during the fine-tuning stage. Our initial experiments on movie reviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated text through BadGPT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Shi2023BadGPTExploringSecurity_BadGPT.pdf;/Users/ma/Zotero/storage/JG6RHJF3/2304.html}
}

@misc{Shi2023RedTeamingLanguage,
  title = {Red {{Teaming Language Model Detectors}} with {{Language Models}}},
  author = {Shi, Zhouxing and Wang, Yihan and Yin, Fan and Chen, Xiangning and Chang, Kai-Wei and Hsieh, Cho-Jui},
  year = {2023},
  month = may,
  number = {arXiv:2305.19713},
  eprint = {2305.19713},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.19713},
  urldate = {2023-06-06},
  abstract = {The prevalence and high capacity of large language models (LLMs) present significant safety and ethical risks when malicious users exploit them for automated content generation. To prevent the potentially deceptive usage of LLMs, recent works have proposed several algorithms to detect machine-generated text. In this paper, we systematically test the reliability of the existing detectors, by designing two types of attack strategies to fool the detectors: 1) replacing words with their synonyms based on the context; 2) altering the writing style of generated text. These strategies are implemented by instructing LLMs to generate synonymous word substitutions or writing directives that modify the style without human involvement, and the LLMs leveraged in the attack can also be protected by detectors. Our research reveals that our attacks effectively compromise the performance of all tested detectors, thereby underscoring the urgent need for the development of more robust machine-generated text detection systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Shi2023RedTeamingLanguage_Red Teaming Language Model Detectors with Language Models.pdf;/Users/ma/Zotero/storage/FW7MAFLT/2305.html}
}

@misc{Shinn2023ReflexionLanguageAgents,
  title = {Reflexion: {{Language Agents}} with {{Verbal Reinforcement Learning}}},
  shorttitle = {Reflexion},
  author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  year = {2023},
  month = oct,
  number = {arXiv:2303.11366},
  eprint = {2303.11366},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.11366},
  url = {http://arxiv.org/abs/2303.11366},
  urldate = {2023-11-01},
  abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Shinn2023ReflexionLanguageAgents_Reflexion.pdf;/Users/ma/Zotero/storage/KX8NE36C/2303.html}
}

@article{ShokrollahiComprehensiveReviewGenerative,
  title = {A {{Comprehensive Review}} of {{Generative AI}} in {{Healthcare}}},
  author = {Shokrollahi, Yasin and Yarmohammadtoosky, Sahar and Nikahd, Matthew M and Dong, Pengfei and Gu, Linxia},
  abstract = {The advancement of Artificial Intelligence (AI) has catalyzed revolutionary changes across various sectors, notably in healthcare. Among the significant developments in this field are the applications of generative AI models, specifically transformers and diffusion models. These models have played a crucial role in analyzing diverse forms of data, including medical imaging (encompassing image reconstruction, image-to-image translation, image generation, and image classification), protein structure prediction, clinical documentation, diagnostic assistance, radiology interpretation, clinical decision support, medical coding, and billing, as well as drug design and molecular representation. Such applications have enhanced clinical diagnosis, data reconstruction, and drug synthesis. This review paper aims to offer a thorough overview of the generative AI applications in healthcare, focusing on transformers and diffusion models. Additionally, we propose potential directions for future research to tackle the existing limitations and meet the evolving demands of the healthcare sector. Intended to serve as a comprehensive guide for researchers and practitioners interested in the healthcare applications of generative AI, this review provides valuable insights into the current state of the art, challenges faced, and prospective future directions.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/UILGMRDL/Shokrollahi et al. - A Comprehensive Review of Generative AI in Healthc.pdf}
}

@misc{Si2022PromptingGPT3Be,
  title = {Prompting {{GPT-3 To Be Reliable}}},
  author = {Si, Chenglei and Gan, Zhe and Yang, Zhengyuan and Wang, Shuohang and Wang, Jianfeng and {Boyd-Graber}, Jordan and Wang, Lijuan},
  year = {2022},
  month = oct,
  number = {arXiv:2210.09150},
  eprint = {2210.09150},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.09150},
  url = {http://arxiv.org/abs/2210.09150},
  urldate = {2023-01-31},
  abstract = {Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, existing research focuses on models' accuracy on standard benchmarks and largely ignores their reliability, which is crucial for avoiding catastrophic real-world harms. While reliability is a broad and vaguely defined term, this work decomposes reliability into four facets: generalizability, fairness, calibration, and factuality. We establish simple and effective prompts to demonstrate GPT-3's reliability in these four aspects: 1) generalize out-of-domain, 2) balance demographic distribution to reduce social biases, 3) calibrate language model probabilities, and 4) update the LLM's knowledge. We find that by employing appropriate prompts, GPT-3 outperforms smaller-scale supervised models by large margins on all these facets. We release all processed datasets, evaluation scripts, and model predictions to facilitate future analysis. Our findings not only shed new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use large language models like GPT-3.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Si2022PromptingGPT3Be_Prompting GPT-3 To Be Reliable.pdf;/Users/ma/Zotero/storage/K6PPJ79C/2210.html}
}

@misc{Song2023ConsistencyModels,
  title = {Consistency {{Models}}},
  author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
  year = {2023},
  month = mar,
  number = {arXiv:2303.01469},
  eprint = {2303.01469},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.01469},
  urldate = {2023-04-14},
  abstract = {Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Song2023ConsistencyModels_Consistency Models.pdf;/Users/ma/Zotero/storage/YVL2M5PN/2303.html}
}

@article{Song2023TaxonPromptTaxonomyawareCurriculum,
  title = {{{TaxonPrompt}}: {{Taxonomy-aware}} Curriculum Prompt Learning for Few-Shot Event Classification},
  shorttitle = {{{TaxonPrompt}}},
  author = {Song, Chengyu and Cai, Fei and Wang, Mengru and Zheng, Jianming and Shao, Taihua},
  year = {2023},
  month = mar,
  journal = {Knowledge-Based Systems},
  volume = {264},
  pages = {110290},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2023.110290},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705123000400},
  urldate = {2023-01-31},
  abstract = {Event classification (EC) aims to assign the event labels to unlabeled sentences and tends to struggle in real-world applications when only a few annotated samples are available. Previous studies have mainly focused on using meta-learning to overcome the low-resource problem where label data from other tasks are still required for model learning and selection. Accordingly, prompt learning-based approaches are proposed to address the low-resource issue. However, such approaches generally ignore task-specific information and adopt demonstration learning for fine-tuning, which fails to leverage the most informative examples for training and hurts performance. Thus, we propose a taxonomy-aware prompt learning framework TaxonPrompt that trains the language model with samples from easy to hard by imitating the human curricula, which effectively alleviates the classification bottleneck caused by insufficient data. We first design an event prompt generation (EPG) for automatically generating task-specific templates using sentences, labels, and trigger words. Then, we propose a Fisher information-based demonstration filtering (FDF) to dynamically select the most informative support examples for each query to train the model. We have conducted extensive experiments on two EC datasets: FewEvent and RAMS. The experimental results demonstrate the superiority of the proposed model over state-of-the-art baselines. In particular, our approach works well in the scenario of an extremely small number of available task resources and therefore constitutes a solution for few-shot event classification.},
  langid = {english},
  keywords = {Event classification,Few-shot learning,Information extraction,Pre-trained language model,Prompt tuning,Template generation},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Song2023TaxonPromptTaxonomyawareCurriculum_TaxonPrompt.pdf;/Users/ma/Zotero/storage/5V5QIU7T/S0950705123000400.html}
}

@misc{StanfordCRFM,
  title = {Stanford {{CRFM}}},
  url = {https://crfm.stanford.edu/2022/12/15/pubmedgpt.html},
  urldate = {2023-04-20},
  file = {/Users/ma/Zotero/storage/A58XSHH8/pubmedgpt.html}
}

@misc{Stear2023PetagraphLargescaleUnifying,
  title = {Petagraph: {{A}} Large-Scale Unifying Knowledge Graph Framework for Integrating Biomolecular and Biomedical Data},
  shorttitle = {Petagraph},
  author = {Stear, Benjamin J. and Ahooyi, Taha Mohseni and Vasisht, Shubha and Simmons, Alan and Beigel, Katherine and Callahan, Tiffany J. and Silverstein, Jonathan C. and Taylor, Deanne M.},
  year = {2023},
  month = feb,
  primaryclass = {New Results},
  pages = {2023.02.11.528088},
  publisher = {{bioRxiv}},
  doi = {10.1101/2023.02.11.528088},
  url = {https://www.biorxiv.org/content/10.1101/2023.02.11.528088v1},
  urldate = {2023-05-04},
  abstract = {The use of biomedical knowledge graphs (BMKG) for knowledge representation and data integration has increased drastically in the past several years due to the size, diversity, and complexity of biomedical datasets and databases. Data extraction from a single dataset or database is usually not particularly challenging. However, if a scientific question must rely on integrative analysis across multiple databases or datasets, it can often take many hours to correctly and reproducibly extract and integrate data towards effective analysis. To overcome this issue, we created Petagraph, a large-scale BMKG that integrates biomolecular data into a schema incorporating the Unified Medical Language System (UMLS). Petagraph is instantiated on the Neo4j graph platform, and to date, has fifteen integrated biomolecular datasets. The majority of the data consists of entities or relationships related to genes, animal models, human phenotypes, drugs, and chemicals. Quantitative data sets containing values from gene expression analyses, chromatin organization, and genetic analyses have also been included. By incorporating models of biomolecular data types, the datasets can be traversed with hundreds of ontologies and controlled vocabularies native to the UMLS, effectively bringing the data to the ontologies. Petagraph allows users to analyze relationships between complex multi-omics data quickly and efficiently.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Stear2023PetagraphLargescaleUnifying_Petagraph.pdf}
}

@inproceedings{Stern2019InsertionTransformerFlexible,
  title = {Insertion {{Transformer}}: {{Flexible Sequence Generation}} via {{Insertion Operations}}},
  shorttitle = {Insertion {{Transformer}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  year = {2019},
  month = may,
  pages = {5976--5985},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/stern19a.html},
  urldate = {2023-02-28},
  abstract = {We present the Insertion Transformer, an iterative, partially autoregressive model for sequence generation based on insertion operations. Unlike typical autoregressive models which rely on a fixed, often left-to-right ordering of the output, our approach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow specific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entropy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and partially autoregressive generation (simultaneous insertions at multiple locations). We validate our approach by analyzing its performance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outperforms many prior non-autoregressive approaches to translation at comparable or better levels of parallelism, and successfully recovers the performance of the original Transformer while requiring only logarithmically many iterations during decoding.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Stern2019InsertionTransformerFlexible_Insertion Transformer.pdf}
}

@inproceedings{Stiennon2020LearningSummarizeHuman,
  title = {Learning to Summarize with Human Feedback},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  year = {2020},
  volume = {33},
  pages = {3008--3021},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html},
  urldate = {2023-02-28},
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task.  For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about---summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences.  We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning.  We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.  We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans.  We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Stiennon2020LearningSummarizeHuman_Learning to summarize with human feedback.pdf}
}

@misc{Sumers2023CognitiveArchitecturesLanguage,
  title = {Cognitive {{Architectures}} for {{Language Agents}}},
  author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02427},
  eprint = {2309.02427},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.02427},
  url = {http://arxiv.org/abs/2309.02427},
  urldate = {2023-10-26},
  abstract = {Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Sumers2023CognitiveArchitecturesLanguage_Cognitive Architectures for Language Agents.pdf;/Users/ma/Zotero/storage/3VBNHQAY/2309.html}
}

@misc{Sun2023HeadtoTailHowKnowledgeable,
  title = {Head-to-{{Tail}}: {{How Knowledgeable}} Are {{Large Language Models}} ({{LLM}})? {{A}}.{{K}}.{{A}}. {{Will LLMs Replace Knowledge Graphs}}?},
  shorttitle = {Head-to-{{Tail}}},
  author = {Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
  year = {2023},
  month = aug,
  number = {arXiv:2308.10168},
  eprint = {2308.10168},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2308.10168},
  urldate = {2023-08-23},
  abstract = {Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs? To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 14 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Sun2023HeadtoTailHowKnowledgeable_Head-to-Tail.pdf;/Users/ma/Zotero/storage/C9YMDCXK/2308.html}
}

@misc{Sun2023HeadtoTailHowKnowledgeablea,
  title = {Head-to-{{Tail}}: {{How Knowledgeable}} Are {{Large Language Models}} ({{LLM}})? {{A}}.{{K}}.{{A}}. {{Will LLMs Replace Knowledge Graphs}}?},
  shorttitle = {Head-to-{{Tail}}},
  author = {Sun, Kai and Xu, Yifan Ethan and Zha, Hanwen and Liu, Yue and Dong, Xin Luna},
  year = {2023},
  month = aug,
  number = {arXiv:2308.10168},
  eprint = {2308.10168},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.10168},
  url = {http://arxiv.org/abs/2308.10168},
  urldate = {2023-09-07},
  abstract = {Since the recent prosperity of Large Language Models (LLMs), there have been interleaved discussions regarding how to reduce hallucinations from LLM responses, how to increase the factuality of LLMs, and whether Knowledge Graphs (KGs), which store the world knowledge in a symbolic form, will be replaced with LLMs. In this paper, we try to answer these questions from a new angle: How knowledgeable are LLMs? To answer this question, we constructed Head-to-Tail, a benchmark that consists of 18K question-answer (QA) pairs regarding head, torso, and tail facts in terms of popularity. We designed an automated evaluation method and a set of metrics that closely approximate the knowledge an LLM confidently internalizes. Through a comprehensive evaluation of 14 publicly available LLMs, we show that existing LLMs are still far from being perfect in terms of their grasp of factual knowledge, especially for facts of torso-to-tail entities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/UB6DV6YZ/Sun et al. - 2023 - Head-to-Tail How Knowledgeable are Large Language.pdf;/Users/ma/Zotero/storage/88AZAFW7/2308.html}
}

@misc{Sun2023SALMONSelfAlignmentPrincipleFollowing,
  title = {{{SALMON}}: {{Self-Alignment}} with {{Principle-Following Reward Models}}},
  shorttitle = {{{SALMON}}},
  author = {Sun, Zhiqing and Shen, Yikang and Zhang, Hongxin and Zhou, Qinhong and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05910},
  eprint = {2310.05910},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.05910},
  url = {http://arxiv.org/abs/2310.05910},
  urldate = {2023-10-25},
  abstract = {Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Zotero/storage/MPNVHNXT/Sun et al. - 2023 - SALMON Self-Alignment with Principle-Following Re.pdf;/Users/ma/Zotero/storage/5RSRAPCI/2310.html}
}

@misc{Tang2023StrucBenchAreLarge,
  title = {Struc-{{Bench}}: {{Are Large Language Models Really Good}} at {{Generating Complex Structured Data}}?},
  shorttitle = {Struc-{{Bench}}},
  author = {Tang, Xiangru and Zong, Yiming and Phang, Jason and Zhao, Yilun and Zhou, Wangchunshu and Cohan, Arman and Gerstein, Mark},
  year = {2023},
  month = sep,
  number = {arXiv:2309.08963},
  eprint = {2309.08963},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.08963},
  url = {http://arxiv.org/abs/2309.08963},
  urldate = {2023-10-25},
  abstract = {Despite the power of Large Language Models (LLMs) like GPT-4, they still struggle with tasks that require generating complex, structured outputs. In this study, we assess the capability of Current LLMs in generating complex structured data and propose a structure-aware fine-tuning approach as a solution to improve this ability. To perform a comprehensive evaluation, we propose Struc-Bench, include five representative LLMs (i.e., GPT-NeoX 20B, GPT-3.5, GPT-4, and Vicuna) and evaluate them on our carefully constructed datasets spanning raw text, HTML, and LaTeX tables. Based on our analysis of current model performance, we identify specific common formatting errors and areas of potential improvement. To address complex formatting requirements, we utilize FormatCoT (Chain-of-Thought) to generate format instructions from target outputs. Our experiments show that our structure-aware fine-tuning method, when applied to LLaMA-7B, significantly improves adherence to natural language constraints, outperforming other evaluated LLMs. Based on these results, we present an ability map of model capabilities from six dimensions (i.e., coverage, formatting, reasoning, comprehension, pragmatics, and hallucination). This map highlights the weaknesses of LLMs in handling complex structured outputs and suggests promising directions for future work. Our code and models can be found at https://github.com/gersteinlab/Struc-Bench.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Tang2023StrucBenchAreLarge_Struc-Bench.pdf;/Users/ma/Zotero/storage/NPZPFHJ4/2309.html}
}

@misc{Tay2023UL2UnifyingLanguage,
  title = {{{UL2}}: {{Unifying Language Learning Paradigms}}},
  shorttitle = {{{UL2}}},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  year = {2023},
  month = feb,
  number = {arXiv:2205.05131},
  eprint = {2205.05131},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.05131},
  urldate = {2023-05-02},
  abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized \& unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 \& GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B \& Flan-UL2 20B.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Tay2023UL2UnifyingLanguage_UL2.pdf;/Users/ma/Zotero/storage/P67VI4A4/2205.html}
}

@misc{Taylor2022GalacticaLargeLanguage,
  title = {Galactica: {{A Large Language Model}} for {{Science}}},
  shorttitle = {Galactica},
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09085},
  eprint = {2211.09085},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.09085},
  url = {http://arxiv.org/abs/2211.09085},
  urldate = {2023-10-26},
  abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Taylor2022GalacticaLargeLanguage_Galactica.pdf;/Users/ma/Zotero/storage/PYMYBCGT/2211.html}
}

@inproceedings{Taylor2023WhereDoesYour,
  title = {Where {{Does Your News Come From}}? {{Predicting Information Pathways}} in {{Social Media}}},
  shorttitle = {Where {{Does Your News Come From}}?},
  booktitle = {Proceedings of the 46th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Taylor, Alexander K. and Wen, Nuan and Kung, Po-Nien and Chen, Jiaao and Peng, Violet and Wang, Wei},
  year = {2023},
  month = jul,
  series = {{{SIGIR}} '23},
  pages = {2511--2515},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3539618.3592087},
  url = {https://dl.acm.org/doi/10.1145/3539618.3592087},
  urldate = {2023-10-02},
  abstract = {As social networks become further entrenched in modern society, it becomes increasingly important to understand and predict how information (e.g., news coverage of a given event) is propagated across social media (i.e., information pathway), which helps the understandings of the impact of real-world information. Thus, in this paper, we propose a novel task, Information Pathway Prediction (IPP), which depicts the propagation paths of a given passage as a community tree (rooted at the information source) on constructed community interaction graphs where we first aggregate individual users into communities formed around news sources and influential users, and then elucidate the patterns of information dissemination across media based on such community nodes. We argue that this is an important and useful task because, on one hand, community-level interactions offer more stability than those at the user level; on the other hand, individual users are often influenced by their community, and modeling community-level information propagation will help the traditional link-prediction problem. To tackle the IPP task, we introduce Lightning, a novel content-aware link prediction GNN model and demonstrate using a large Twitter dataset consisting of all COVID related tweets that Lightning outperforms state-of-the-art link prediction baselines by a significant margin.},
  isbn = {978-1-4503-9408-6},
  keywords = {data mining,graph neural networks,machine learning,social networks},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Taylor2023WhereDoesYour_Where Does Your News Come From.pdf}
}

@misc{Teng2021CharacterizingUserSusceptibility,
  title = {Characterizing {{User Susceptibility}} to {{COVID-19 Misinformation}} on {{Twitter}}},
  author = {Teng, Xian and Lin, Yu-Ru and Chung, Wen-Ting and Li, Ang and Kovashka, Adriana},
  year = {2021},
  month = sep,
  number = {arXiv:2109.09532},
  eprint = {2109.09532},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2109.09532},
  urldate = {2023-10-17},
  abstract = {Though significant efforts such as removing false claims and promoting reliable sources have been increased to combat COVID-19 "misinfodemic", it remains an unsolved societal challenge if lacking a proper understanding of susceptible online users, i.e., those who are likely to be attracted by, believe and spread misinformation. This study attempts to answer \{\textbackslash it who\} constitutes the population vulnerable to the online misinformation in the pandemic, and what are the robust features and short-term behavior signals that distinguish susceptible users from others. Using a 6-month longitudinal user panel on Twitter collected from a geopolitically diverse network-stratified samples in the US, we distinguish different types of users, ranging from social bots to humans with various level of engagement with COVID-related misinformation. We then identify users' online features and situational predictors that correlate with their susceptibility to COVID-19 misinformation. This work brings unique contributions: First, contrary to the prior studies on bot influence, our analysis shows that social bots' contribution to misinformation sharing was surprisingly low, and human-like users' misinformation behaviors exhibit heterogeneity and temporal variability. While the sharing of misinformation was highly concentrated, the risk of occasionally sharing misinformation for average users remained alarmingly high. Second, our findings highlight the political sensitivity activeness and responsiveness to emotionally-charged content among susceptible users. Third, we demonstrate a feasible solution to efficiently predict users' transient susceptibility solely based on their short-term news consumption and exposure from their networks. Our work has an implication in designing effective intervention mechanism to mitigate the misinformation dissipation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Social and Information Networks},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Teng2021CharacterizingUserSusceptibility_Characterizing User Susceptibility to COVID-19 Misinformation on Twitter.pdf;/Users/ma/Zotero/storage/M7QSRW7A/2109.html}
}

@incollection{Thier2021HealthMisinformation,
  title = {Health {{Misinformation}}},
  booktitle = {The {{Routledge Handbook}} of {{Health Communication}}},
  author = {Nan, Xiaoli and Wang, Yuan and Thier, Kathryn},
  year = {2021},
  edition = {3},
  publisher = {{Routledge}},
  abstract = {Research on health misinformation has grown rapidly as concerns about the potential harmful effects of health misinformation on individuals and society intensify amid a ``post-truth'' era. In this chapter, we provide a broad overview of current research and evidence concerning the many facets of health misinformation, including its sources, prevalence, characteristics (both content and diffusion features), impact, and mitigation. We conclude that health misinformation originates from many sources, most notably mass and social media; is fairly prevalent, both in interpersonal and mediated settings; and tends to feature negative sentiments, anecdotal evidence, and anti-science narratives. Although there is no conclusive evidence that health misinformation spreads more broadly than scientific information, health misinformation reliably leads to misperceptions on health issues. Efforts to mitigate the impact of health misinformation show early promise in correcting misperceptions. We offer several directions for future research.},
  isbn = {978-1-00-304337-9},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Thier2021HealthMisinformation_Health Misinformation.pdf}
}

@misc{Thoppilan2022LaMDALanguageModels,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and {Meier-Hellstern}, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and {Hoffman-John}, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and {Aguera-Arcas}, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  year = {2022},
  month = feb,
  number = {arXiv:2201.08239},
  eprint = {2201.08239},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.08239},
  url = {http://arxiv.org/abs/2201.08239},
  urldate = {2023-03-01},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Thoppilan2022LaMDALanguageModels_LaMDA.pdf;/Users/ma/Zotero/storage/M78FL9LL/2201.html}
}

@misc{Tian2023GraphNeuralPrompting,
  title = {Graph {{Neural Prompting}} with {{Large Language Models}}},
  author = {Tian, Yijun and Song, Huan and Wang, Zichen and Wang, Haozhu and Hu, Ziqing and Wang, Fang and Chawla, Nitesh V. and Xu, Panpan},
  year = {2023},
  month = sep,
  number = {arXiv:2309.15427},
  eprint = {2309.15427},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.15427},
  url = {http://arxiv.org/abs/2309.15427},
  urldate = {2023-10-07},
  abstract = {Large Language Models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. In addition, how to leverage the pre-trained LLMs and avoid training a customized model from scratch remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,graph-enhanced LM},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Tian2023GraphNeuralPrompting_Graph Neural Prompting with Large Language Models.pdf;/Users/ma/Zotero/storage/YN62PGFR/2309.html}
}

@misc{Tian2023JustAskCalibration,
  title = {Just {{Ask}} for {{Calibration}}: {{Strategies}} for {{Eliciting Calibrated Confidence Scores}} from {{Language Models Fine-Tuned}} with {{Human Feedback}}},
  shorttitle = {Just {{Ask}} for {{Calibration}}},
  author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
  year = {2023},
  month = may,
  number = {arXiv:2305.14975},
  eprint = {2305.14975},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.14975},
  urldate = {2023-07-18},
  abstract = {A trustworthy real-world prediction system should be well-calibrated; that is, its confidence in an answer is indicative of the likelihood that the answer is correct, enabling deferral to a more expensive expert in cases of low-confidence predictions. While recent studies have shown that unsupervised pre-training produces large language models (LMs) that are remarkably well-calibrated, the most widely-used LMs in practice are fine-tuned with reinforcement learning with human feedback (RLHF-LMs) after the initial unsupervised pre-training stage, and results are mixed as to whether these models preserve the well-calibratedness of their ancestors. In this paper, we conduct a broad evaluation of computationally feasible methods for extracting confidence scores from LLMs fine-tuned with RLHF. We find that with the right prompting strategy, RLHF-LMs verbalize probabilities that are much better calibrated than the model's conditional probabilities, enabling fairly well-calibrated predictions. Through a combination of prompting strategy and temperature scaling, we find that we can reduce the expected calibration error of RLHF-LMs by over 50\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Tian2023JustAskCalibration_Just Ask for Calibration.pdf;/Users/ma/Zotero/storage/SRK4NRZS/2305.html}
}

@misc{Tian2023JustAskCalibrationa,
  title = {Just {{Ask}} for {{Calibration}}: {{Strategies}} for {{Eliciting Calibrated Confidence Scores}} from {{Language Models Fine-Tuned}} with {{Human Feedback}}},
  shorttitle = {Just {{Ask}} for {{Calibration}}},
  author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2305.14975v1},
  urldate = {2023-10-23},
  abstract = {A trustworthy real-world prediction system should be well-calibrated; that is, its confidence in an answer is indicative of the likelihood that the answer is correct, enabling deferral to a more expensive expert in cases of low-confidence predictions. While recent studies have shown that unsupervised pre-training produces large language models (LMs) that are remarkably well-calibrated, the most widely-used LMs in practice are fine-tuned with reinforcement learning with human feedback (RLHF-LMs) after the initial unsupervised pre-training stage, and results are mixed as to whether these models preserve the well-calibratedness of their ancestors. In this paper, we conduct a broad evaluation of computationally feasible methods for extracting confidence scores from LLMs fine-tuned with RLHF. We find that with the right prompting strategy, RLHF-LMs verbalize probabilities that are much better calibrated than the model's conditional probabilities, enabling fairly well-calibrated predictions. Through a combination of prompting strategy and temperature scaling, we find that we can reduce the expected calibration error of RLHF-LMs by over 50\%.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Tian2023JustAskCalibrationa_Just Ask for Calibration.pdf}
}

@inproceedings{Tong2022DocEELargeScaleFinegrained,
  title = {{{DocEE}}: {{A Large-Scale}} and {{Fine-grained Benchmark}} for {{Document-level Event Extraction}}},
  shorttitle = {{{DocEE}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Tong, MeiHan and Xu, Bin and Wang, Shuai and Han, Meihuan and Cao, Yixin and Zhu, Jiangqi and Chen, Siyu and Hou, Lei and Li, Juanzi},
  year = {2022},
  month = jul,
  pages = {3970--3982},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.291},
  url = {https://aclanthology.org/2022.naacl-main.291},
  urldate = {2023-02-03},
  abstract = {Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentence-level event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote document-level event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: large-scale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41\% Vs 85\% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/tongmeihan1995/DocEE.git.},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Tong2022DocEELargeScaleFinegrained_DocEE.pdf}
}

@misc{Touvron2023LLaMAOpenEfficient,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.13971},
  url = {http://arxiv.org/abs/2302.13971},
  urldate = {2023-04-29},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Touvron2023LLaMAOpenEfficient_LLaMA.pdf;/Users/ma/Zotero/storage/YZQ2RYZJ/2302.html}
}

@article{TouvronLlamaOpenFoundation,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/SSHRX9Y4/Touvron et al. - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf}
}

@article{Traberg2022BirdsFeatherAre,
  title = {Birds of a Feather Are Persuaded Together: {{Perceived}} Source Credibility Mediates the Effect of Political Bias on Misinformation Susceptibility},
  shorttitle = {Birds of a Feather Are Persuaded Together},
  author = {Traberg, Cecilie Steenbuch and {van der Linden}, Sander},
  year = {2022},
  month = feb,
  journal = {Personality and Individual Differences},
  volume = {185},
  pages = {111269},
  issn = {0191-8869},
  doi = {10.1016/j.paid.2021.111269},
  url = {https://www.sciencedirect.com/science/article/pii/S0191886921006486},
  urldate = {2023-10-12},
  abstract = {The viral spread of misinformation poses a threat to societies around the world. Recently, researchers have started to study how motivated reasoning about news content influences misinformation susceptibility. However, because the importance of source credibility in the persuasion process is well-documented, and given that source similarity contributes to credibility evaluations, this raises the question of whether individuals are more susceptible to misinformation from ideologically congruent news sources because they find them to be more credible. In a large between-subject pilot (N~=~656) and a pre-registered online mixed-subject experiment with a US sample (N~=~150) using simulated social media posts, we find clear evidence that both liberals and conservatives judge misinformation to be more accurate when the source is politically congruent, and that this effect is mediated by perceived source credibility. We show that source effects play a greater role in veracity judgements for liberals than conservatives, but that individuals from both sides of the spectrum judge politically congruent sources as less slanted and more credible. These findings add to our current understanding of source effects in online news environments and provide evidence for the influential effect of perceived source similarity and perceived credibility in misinformation susceptibility.},
  keywords = {Misinformation,Persuasion,Political bias,Reasoning,Source effects},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Traberg2022BirdsFeatherAre_Birds of a feather are persuaded together.pdf}
}

@inproceedings{Utama2020DebiasingNLUModels,
  title = {Towards {{Debiasing NLU Models}} from {{Unknown Biases}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Utama, Prasetya Ajie and Moosavi, Nafise Sadat and Gurevych, Iryna},
  year = {2020},
  month = nov,
  pages = {7597--7610},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.613},
  url = {https://aclanthology.org/2020.emnlp-main.613},
  urldate = {2023-08-27},
  abstract = {NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Utama2020DebiasingNLUModels_Towards Debiasing NLU Models from Unknown Biases.pdf}
}

@article{vanderLinden2022MisinformationSusceptibilitySpread,
  title = {Misinformation: Susceptibility, Spread, and Interventions to Immunize the Public},
  shorttitle = {Misinformation},
  author = {{van der Linden}, Sander},
  year = {2022},
  month = mar,
  journal = {Nature Medicine},
  volume = {28},
  number = {3},
  pages = {460--467},
  issn = {1546-170X},
  doi = {10.1038/s41591-022-01713-6},
  abstract = {The spread of misinformation poses a considerable threat to public health and the successful management of a global pandemic. For example, studies find that exposure to misinformation can undermine vaccination uptake and compliance with public-health guidelines. As research on the science of misinformation is rapidly emerging, this conceptual Review summarizes what we know along three key dimensions of the infodemic: susceptibility, spread, and immunization. Extant research is evaluated on the questions of why (some) people are (more) susceptible to misinformation, how misinformation spreads in online social networks, and which interventions can help to boost psychological immunity to misinformation. Implications for managing the infodemic are discussed.},
  langid = {english},
  pmid = {35273402},
  keywords = {Communication,COVID-19,Humans,Pandemics,SARS-CoV-2,Social Media},
  file = {/Users/ma/Drive_ma/Papers_Zotero/vanderLinden2022MisinformationSusceptibilitySpread_Misinformation.pdf}
}

@inproceedings{Veyseh2021InducingRichInteraction,
  title = {Inducing {{Rich Interaction Structures Between Words}} for {{Document-Level Event Argument Extraction}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}: 25th {{Pacific-Asia Conference}}, {{PAKDD}} 2021, {{Virtual Event}}, {{May}} 11\textendash 14, 2021, {{Proceedings}}, {{Part II}}},
  author = {Veyseh, Amir Pouran Ben and Dernoncourt, Franck and Tran, Quan and Manjunatha, Varun and Wang, Lidan and Jain, Rajiv and Kim, Doo Soon and Chang, Walter and Nguyen, Thien Huu},
  year = {2021},
  month = may,
  pages = {703--715},
  publisher = {{Springer-Verlag}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-030-75765-6_56},
  url = {https://doi.org/10.1007/978-3-030-75765-6\_56},
  urldate = {2023-02-20},
  abstract = {Event Argument Extraction (EAE) is the task of identifying roles of entity mentions/arguments in events evoked by trigger words. Most existing works have focused on sentence-level EAE, leaving document-level EAE (i.e., event triggers and arguments belong to different sentences in documents) an under-studied problem in the literature. This paper introduces a new deep learning model for document-level EAE where document structures/graphs are utilized to represent input documents and aid the representation learning. Our model employs different types of interactions between important context words in documents (i.e., syntax, semantic, and discourse) to enhance document representations. Extensive experiments are conducted to demonstratethe effectiveness of the proposed model, leading to the state-of-the-art performance for document-level EAE.},
  isbn = {978-3-030-75764-9},
  keywords = {Document structures,Event Argument Extraction}
}

@inproceedings{Wadden2019EntityRelationEvent,
  title = {Entity, {{Relation}}, and {{Event Extraction}} with {{Contextualized Span Representations}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Wadden, David and Wennberg, Ulme and Luan, Yi and Hajishirzi, Hannaneh},
  year = {2019},
  month = nov,
  pages = {5784--5789},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1585},
  url = {https://aclanthology.org/D19-1585},
  urldate = {2023-02-17},
  abstract = {We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wadden2019EntityRelationEvent_Entity, Relation, and Event Extraction with Contextualized Span Representations.pdf}
}

@misc{Wallace2021ConcealedDataPoisoning,
  title = {Concealed {{Data Poisoning Attacks}} on {{NLP Models}}},
  author = {Wallace, Eric and Zhao, Tony Z. and Feng, Shi and Singh, Sameer},
  year = {2021},
  month = apr,
  number = {arXiv:2010.12563},
  eprint = {2010.12563},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.12563},
  urldate = {2023-02-08},
  abstract = {Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains "James Bond". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling ("Apple iPhone" triggers negative generations) and machine translation ("iced coffee" mistranslated as "hot coffee"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wallace2021ConcealedDataPoisoning_Concealed Data Poisoning Attacks on NLP Models.pdf;/Users/ma/Zotero/storage/YC56WGZE/2010.html}
}

@misc{Wallace2021ConcealedDataPoisoninga,
  title = {Concealed {{Data Poisoning Attacks}} on {{NLP Models}}},
  author = {Wallace, Eric and Zhao, Tony Z. and Feng, Shi and Singh, Sameer},
  year = {2021},
  month = apr,
  number = {arXiv:2010.12563},
  eprint = {2010.12563},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2010.12563},
  url = {http://arxiv.org/abs/2010.12563},
  urldate = {2023-03-05},
  abstract = {Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains "James Bond". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling ("Apple iPhone" triggers negative generations) and machine translation ("iced coffee" mistranslated as "hot coffee"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wallace2021ConcealedDataPoisoninga_Concealed Data Poisoning Attacks on NLP Models.pdf;/Users/ma/Zotero/storage/A8F46GNR/2010.html}
}

@misc{Wan2023PoisoningLanguageModels,
  title = {Poisoning {{Language Models During Instruction Tuning}}},
  author = {Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  year = {2023},
  month = may,
  number = {arXiv:2305.00944},
  eprint = {2305.00944},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.00944},
  url = {http://arxiv.org/abs/2305.00944},
  urldate = {2023-05-02},
  abstract = {Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions "Joe Biden", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wan2023PoisoningLanguageModels_Poisoning Language Models During Instruction Tuning.pdf;/Users/ma/Zotero/storage/LXUKN6WL/2305.html}
}

@misc{Wan2023PoisoningLanguageModelsa,
  title = {Poisoning {{Language Models During Instruction Tuning}}},
  author = {Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2305.00944v1},
  urldate = {2023-05-09},
  abstract = {Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions "Joe Biden", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wan2023PoisoningLanguageModelsa_Poisoning Language Models During Instruction Tuning.pdf}
}

@misc{Wan2023PoisoningLanguageModelsb,
  title = {Poisoning {{Language Models During Instruction Tuning}}},
  author = {Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  year = {2023},
  month = may,
  number = {arXiv:2305.00944},
  eprint = {2305.00944},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.00944},
  urldate = {2023-05-12},
  abstract = {Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions "Joe Biden", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wan2023PoisoningLanguageModelsb_Poisoning Language Models During Instruction Tuning.pdf;/Users/ma/Zotero/storage/QEKP8FET/2305.html}
}

@misc{Wang2021CLEVEContrastivePretraining,
  title = {{{CLEVE}}: {{Contrastive Pre-training}} for {{Event Extraction}}},
  shorttitle = {{{CLEVE}}},
  author = {Wang, Ziqi and Wang, Xiaozhi and Han, Xu and Lin, Yankai and Hou, Lei and Liu, Zhiyuan and Li, Peng and Li, Juanzi and Zhou, Jie},
  year = {2021},
  month = may,
  number = {arXiv:2105.14485},
  eprint = {2105.14485},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2105.14485},
  urldate = {2023-06-28},
  abstract = {Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning. However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data. To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers. CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively. Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures. The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised "liberal" EE, which requires jointly extracting events and discovering event schemata without any annotated data. Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting. The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2021CLEVEContrastivePretraining_CLEVE.pdf;/Users/ma/Zotero/storage/QYVRPK8E/2105.html}
}

@inproceedings{Wang2021WantReduceLabeling,
  title = {Want {{To Reduce Labeling Cost}}? {{GPT-3 Can Help}}},
  shorttitle = {Want {{To Reduce Labeling Cost}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  year = {2021},
  month = nov,
  pages = {4195--4205},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.354},
  url = {https://aclanthology.org/2021.findings-emnlp.354},
  urldate = {2023-02-18},
  abstract = {Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50\% to 96\% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2021WantReduceLabeling_Want To Reduce Labeling Cost.pdf}
}

@inproceedings{Wang2021WantReduceLabelinga,
  title = {Want {{To Reduce Labeling Cost}}? {{GPT-3 Can Help}}},
  shorttitle = {Want {{To Reduce Labeling Cost}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2021},
  author = {Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  year = {2021},
  pages = {4195--4205},
  publisher = {{Association for Computational Linguistics}},
  address = {{Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.findings-emnlp.354},
  url = {https://aclanthology.org/2021.findings-emnlp.354},
  urldate = {2023-10-23},
  abstract = {Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often taskspecific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 175 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that, to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50\% to 96\% less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance with limited labeling budget. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.},
  langid = {english},
  keywords = {data generation},
  file = {/Users/ma/Zotero/storage/G2PHKK5J/Wang et al. - 2021 - Want To Reduce Labeling Cost GPT-3 Can Help.pdf}
}

@misc{Wang2022Code4StructCodeGeneration,
  title = {{{Code4Struct}}: {{Code Generation}} for {{Few-Shot Structured Prediction}} from {{Natural Language}}},
  shorttitle = {{{Code4Struct}}},
  author = {Wang, Xingyao and Li, Sha and Ji, Heng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12810},
  eprint = {2210.12810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.12810},
  urldate = {2023-03-02},
  abstract = {Large Language Model (LLM) trained on the mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. In this work, we propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks in NLP. For example, Event Argument Extraction (EAE) aims to convert text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints with ease. We exploit the analogy between PL and NLP problems, and, as a case study, we use Code4Struct to tackle the EAE task using code generation. We ask a LLM to generate code to instantiate an event class with predicted arguments given a NL sentence. Despite only using 50 training instances for each event type, Code4Struct is comparable to fully-supervised models trained on 4,202 event instances and, when given the same 50-shot data, outperforms current state-of-the-art (SOTA) by 20.8\% absolute F1. When prompted with hierarchical event types implemented using inheritance, Code4Struct can predict arguments for low-resource event types using 10-shot training instances from its sibling event type and outperforms zero-shot baseline by 12\% absolute F1.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022Code4StructCodeGeneration_Code4Struct.pdf;/Users/ma/Zotero/storage/E32SFQDE/2210.html}
}

@inproceedings{Wang2022DeepStructPretrainingLanguage,
  title = {{{DeepStruct}}: {{Pretraining}} of {{Language Models}} for {{Structure Prediction}}},
  shorttitle = {{{DeepStruct}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  year = {2022},
  month = may,
  pages = {803--823},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.67},
  url = {https://aclanthology.org/2022.findings-acl.67},
  urldate = {2023-05-01},
  abstract = {We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022DeepStructPretrainingLanguage_DeepStruct.pdf}
}

@inproceedings{Wang2022DeepStructPretrainingLanguagea,
  title = {{{DeepStruct}}: {{Pretraining}} of {{Language Models}} for {{Structure Prediction}}},
  shorttitle = {{{DeepStruct}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Wang, Chenguang and Liu, Xiao and Chen, Zui and Hong, Haoyun and Tang, Jie and Song, Dawn},
  year = {2022},
  month = may,
  pages = {803--823},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.findings-acl.67},
  url = {https://aclanthology.org/2022.findings-acl.67},
  urldate = {2023-05-01},
  abstract = {We introduce a method for improving the structural understanding abilities of language models. Unlike previous approaches that finetune the models with task-specific augmentation, we pretrain language models to generate structures from the text on a collection of task-agnostic corpora. Our structure pretraining enables zero-shot transfer of the learned knowledge that models have about the structure tasks. We study the performance of this approach on 28 datasets, spanning 10 structure prediction tasks including open information extraction, joint entity and relation extraction, named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, factual probe, intent detection, and dialogue state tracking. We further enhance the pretraining with the task-specific training sets. We show that a 10B parameter language model transfers non-trivially to most tasks and obtains state-of-the-art performance on 21 of 28 datasets that we evaluate. Our code and datasets will be made publicly available.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022DeepStructPretrainingLanguagea_DeepStruct.pdf}
}

@inproceedings{Wang2022ScienceWorldYourAgenta,
  title = {{{ScienceWorld}}: {{Is}} Your {{Agent Smarter}} than a 5th {{Grader}}?},
  shorttitle = {{{ScienceWorld}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wang, Ruoyao and Jansen, Peter and C{\^o}t{\'e}, Marc-Alexandre and Ammanabrolu, Prithviraj},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = {2022},
  month = dec,
  pages = {11279--11298},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  doi = {10.18653/v1/2022.emnlp-main.775},
  url = {https://aclanthology.org/2022.emnlp-main.775},
  urldate = {2023-11-01},
  abstract = {We present ScienceWorld, a benchmark to test agents' scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that current models cannot reason about or explain learned science concepts in novel contexts. For instance, models can easily answer what the conductivity of a known material is but struggle when asked how they would conduct an experiment in a grounded environment to find the conductivity of an unknown material. This begs the question of whether current models are simply retrieving answers by way of seeing a large number of similar examples or if they have learned to reason about concepts in a reusable manner. We hypothesize that agents need to be grounded in interactive environments to achieve such reasoning capabilities. Our experiments provide empirical evidence supporting this hypothesis \textendash{} showing that a 1.5 million parameter agent trained interactively for 100k steps outperforms a 11 billion parameter model statically trained for scientific question-answering and reasoning from millions of expert demonstrations.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022ScienceWorldYourAgenta_ScienceWorld.pdf}
}

@misc{Wang2022SelfConsistencyImprovesChain,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2022},
  month = oct,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2203.11171},
  urldate = {2023-03-06},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022SelfConsistencyImprovesChain_Self-Consistency Improves Chain of Thought Reasoning in Language Models.pdf;/Users/ma/Zotero/storage/KCEUT8C7/2203.html}
}

@misc{Wang2022SelfInstructAligningLanguage,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2023-03-30},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022SelfInstructAligningLanguage_Self-Instruct.pdf;/Users/ma/Zotero/storage/HHFPCJCL/2212.html}
}

@misc{Wang2022SelfInstructAligningLanguagea,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10560},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2023-05-04},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022SelfInstructAligningLanguagea_Self-Instruct.pdf;/Users/ma/Zotero/storage/F9SRRU5W/2212.html}
}

@misc{Wang2022SuperNaturalInstructionsGeneralizationDeclarative,
  title = {Super-{{NaturalInstructions}}: {{Generalization}} via {{Declarative Instructions}} on 1600+ {{NLP Tasks}}},
  shorttitle = {Super-{{NaturalInstructions}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  year = {2022},
  month = oct,
  number = {arXiv:2204.07705},
  eprint = {2204.07705},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.07705},
  url = {http://arxiv.org/abs/2204.07705},
  urldate = {2023-03-05},
  abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9\% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022SuperNaturalInstructionsGeneralizationDeclarative_Super-NaturalInstructions.pdf;/Users/ma/Zotero/storage/PMCB4GMS/2204.html}
}

@inproceedings{Wang2022SuperNaturalInstructionsGeneralizationDeclarativea,
  title = {Super-{{NaturalInstructions}}: {{Generalization}} via {{Declarative Instructions}} on 1600+ {{NLP Tasks}}},
  shorttitle = {Super-{{NaturalInstructions}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Pal, Kuntal Kumar and Patel, Maitreya and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Doshi, Savan and Sampat, Shailaja Keyur and Mishra, Siddhartha and Reddy A, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong},
  year = {2022},
  month = dec,
  pages = {5085--5109},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.340},
  urldate = {2023-03-08},
  abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\textemdash training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9\% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.},
  keywords = {dataset\_NaturalInstructions,Instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2022SuperNaturalInstructionsGeneralizationDeclarativea_Super-NaturalInstructions.pdf}
}

@misc{wang2023BoostingEventExtraction,
  title = {Boosting {{Event Extraction}} with {{Denoised Structure-to-Text Augmentation}}},
  author = {{wang}, bo and Huang, Heyan and Wei, Xiaochi and Shi, Ge and Liu, Xiao and Feng, Chong and Zhou, Tong and Wang, Shuaiqiang and Yin, Dawei},
  year = {2023},
  month = may,
  number = {arXiv:2305.09598},
  eprint = {2305.09598},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.09598},
  urldate = {2023-10-17},
  abstract = {Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting, leading to unsatisfactory performances. In order to solve these problems, we propose a denoised structure-to-text augmentation framework for event extraction DAEE, which generates additional training data through the knowledge-based structure-to-text generation model and selects the effective subset from the generated data iteratively with a deep reinforcement learning agent. Experimental results on several datasets demonstrate that the proposed method generates more diverse text representations for event extraction and achieves comparable results with the state-of-the-art.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/wang2023BoostingEventExtraction_Boosting Event Extraction with Denoised Structure-to-Text Augmentation.pdf;/Users/ma/Zotero/storage/L9XCAZKS/2305.html}
}

@misc{Wang2023Code4StructCodeGeneration,
  title = {{{Code4Struct}}: {{Code Generation}} for {{Few-Shot Event Structure Prediction}}},
  shorttitle = {{{Code4Struct}}},
  author = {Wang, Xingyao and Li, Sha and Ji, Heng},
  year = {2023},
  month = may,
  number = {arXiv:2210.12810},
  eprint = {2210.12810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12810},
  url = {http://arxiv.org/abs/2210.12810},
  urldate = {2023-06-16},
  abstract = {Large Language Model (LLM) trained on a mixture of text and code has demonstrated impressive capability in translating natural language (NL) into structured code. We observe that semantic structures can be conveniently translated into code and propose Code4Struct to leverage such text-to-structure translation capability to tackle structured prediction tasks. As a case study, we formulate Event Argument Extraction (EAE) as converting text into event-argument structures that can be represented as a class object using code. This alignment between structures and code enables us to take advantage of Programming Language (PL) features such as inheritance and type annotation to introduce external knowledge or add constraints. We show that, with sufficient in-context examples, formulating EAE as a code generation problem is advantageous over using variants of text-based prompts. Despite only using 20 training event instances for each event type, Code4Struct is comparable to supervised models trained on 4,202 instances and outperforms current state-of-the-art (SOTA) trained on 20-shot data by 29.5\% absolute F1. Code4Struct can use 10-shot training data from a sibling event type to predict arguments for zero-resource event types and outperforms the zero-shot baseline by 12\% absolute F1.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2023Code4StructCodeGeneration_Code4Struct.pdf;/Users/ma/Zotero/storage/MKGIBG5A/2210.html}
}

@misc{Wang2023DRGLLaMATuningLLaMA,
  title = {{{DRG-LLaMA}} : {{Tuning LLaMA Model}} to {{Predict Diagnosis-related Group}} for {{Hospitalized Patients}}},
  shorttitle = {{{DRG-LLaMA}}},
  author = {Wang, Hanyin and Gao, Chufan and Dantona, Christopher and Hull, Bryan and Sun, Jimeng},
  year = {2023},
  month = sep,
  number = {arXiv:2309.12625},
  eprint = {2309.12625},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.12625},
  url = {http://arxiv.org/abs/2309.12625},
  urldate = {2023-10-17},
  abstract = {In the U.S. inpatient payment system, the Diagnosis-Related Group (DRG) is pivotal, but its assignment process is inefficient. The study introduces DRG-LLaMA, an advanced large language model (LLM) fine-tuned on clinical notes to enhance DRGs assignment. Utilizing LLaMA as the foundational model and optimizing it through Low-Rank Adaptation (LoRA) on 236,192 MIMIC-IV discharge summaries, our DRG-LLaMA-7B model exhibited a noteworthy macro-averaged F1 score of 0.327, a top-1 prediction accuracy of 52.0\%, and a macro-averaged Area Under the Curve (AUC) of 0.986, with a maximum input token length of 512. This model surpassed the performance of prior leading models in DRG prediction, showing a relative improvement of 40.3\% and 35.7\% in macro-averaged F1 score compared to ClinicalBERT and CAML, respectively. Applied to base DRG and complication or comorbidity (CC)/major complication or comorbidity (MCC) prediction, DRG-LLaMA achieved a top-1 prediction accuracy of 67.8\% and 67.5\%, respectively. Additionally, our findings indicate that DRG-LLaMA's performance correlates with increased model parameters and input context lengths.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2023DRGLLaMATuningLLaMA_DRG-LLaMA.pdf;/Users/ma/Zotero/storage/79Y3D8D2/2309.html}
}

@misc{Wang2023EnableLanguageModels,
  title = {Enable {{Language Models}} to {{Implicitly Learn Self-Improvement From Data}}},
  author = {Wang, Ziqi and Hou, Le and Lu, Tianjian and Wu, Yuexin and Li, Yunxuan and Yu, Hongkun and Ji, Heng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.00898},
  eprint = {2310.00898},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.00898},
  url = {http://arxiv.org/abs/2310.00898},
  urldate = {2023-10-25},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/9A7QN7VW/Wang et al. - 2023 - Enable Language Models to Implicitly Learn Self-Im.pdf}
}

@misc{Wang2023GuidingLanguageModel,
  title = {Guiding {{Language Model Reasoning}} with {{Planning Tokens}}},
  author = {Wang, Xinyi and Caccia, Lucas and Ostapenko, Oleksiy and Yuan, Xingdi and Sordoni, Alessandro},
  year = {2023},
  month = oct,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2310.05707v1},
  urldate = {2023-10-10},
  abstract = {Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce 'planning tokens' at the start of each reasoning step, serving as a guide for the model. These token embeddings are then fine-tuned along with the rest of the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001\%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. plain chain-of-thought fine-tuning baselines.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2023GuidingLanguageModel_Guiding Language Model Reasoning with Planning Tokens.pdf}
}

@misc{Wang2023LargeLanguageModels,
  title = {Large {{Language Models Are Implicitly Topic Models}}: {{Explaining}} and {{Finding Good Demonstrations}} for {{In-Context Learning}}},
  shorttitle = {Large {{Language Models Are Implicitly Topic Models}}},
  author = {Wang, Xinyi and Zhu, Wanrong and Saxon, Michael and Steyvers, Mark and Wang, William Yang},
  year = {2023},
  month = may,
  number = {arXiv:2301.11916},
  eprint = {2301.11916},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.11916},
  url = {http://arxiv.org/abs/2301.11916},
  urldate = {2023-05-30},
  abstract = {In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5\% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that large language models implicitly infer a latent concept variable.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2023LargeLanguageModels_Large Language Models Are Implicitly Topic Models.pdf;/Users/ma/Zotero/storage/BY8ZEBJ3/2301.html}
}

@misc{Wang2023OverwritingPretrainedBias,
  title = {Overwriting {{Pretrained Bias}} with {{Finetuning Data}}},
  author = {Wang, Angelina and Russakovsky, Olga},
  year = {2023},
  month = aug,
  number = {arXiv:2303.06167},
  eprint = {2303.06167},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.06167},
  url = {http://arxiv.org/abs/2303.06167},
  urldate = {2023-10-03},
  abstract = {Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2023OverwritingPretrainedBias_Overwriting Pretrained Bias with Finetuning Data.pdf;/Users/ma/Zotero/storage/TEXSJFTW/2303.html}
}

@misc{Wang2023SCOTTSelfConsistentChainofThought,
  title = {{{SCOTT}}: {{Self-Consistent Chain-of-Thought Distillation}}},
  shorttitle = {{{SCOTT}}},
  author = {Wang, Peifeng and Wang, Zhengyang and Li, Zheng and Gao, Yifan and Yin, Bing and Ren, Xiang},
  year = {2023},
  month = may,
  number = {arXiv:2305.01879},
  eprint = {2305.01879},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.01879},
  urldate = {2023-05-12},
  abstract = {Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that, while yielding comparable end-task performance, our method can generate CoT rationales that are more faithful than baselines do. Further analysis suggests that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wang2023SCOTTSelfConsistentChainofThought_SCOTT.pdf;/Users/ma/Zotero/storage/N53SHTYC/2305.html}
}

@misc{Wang2023SurveyLargeLanguage,
  title = {A {{Survey}} on {{Large Language Model}} Based {{Autonomous Agents}}},
  author = {Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and Zhao, Wayne Xin and Wei, Zhewei and Wen, Ji-Rong},
  year = {2023},
  month = sep,
  number = {arXiv:2308.11432},
  eprint = {2308.11432},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.11432},
  url = {http://arxiv.org/abs/2308.11432},
  urldate = {2023-10-25},
  abstract = {Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/7KAHC8XI/Wang et al. - 2023 - A Survey on Large Language Model based Autonomous .pdf;/Users/ma/Zotero/storage/N79UM3X8/2308.html}
}

@article{WangBoostingEventExtraction,
  title = {Boosting {{Event Extraction}} with {{Denoised Structure-to-Text Augmentation}}},
  author = {Wang, Bo and Huang, Heyan and Wei, Xiaochi and Shi, Ge and Liu, Xiao and Feng, Chong and Zhou, Tong and Wang, Shuaiqiang and Yin, Dawei},
  abstract = {Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting, leading to unsatisfactory performances. In order to solve these problems, we propose a denoised structure-to-text augmentation framework for event extraction (DAEE), which generates additional training data through the knowledgebased structure-to-text generation model and selects the effective subset from the generated data iteratively with a deep reinforcement learning agent. Experimental results on several datasets demonstrate that the proposed method generates more diverse text representations for event extraction and achieves comparable results with the state-of-the-art.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/9UZXCNZK/Wang et al. - Boosting Event Extraction with Denoised Structure-.pdf}
}

@article{WangSELFCONSISTENCYIMPROVESCHAIN,
  title = {{{SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed H and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  langid = {english},
  file = {/Users/ma/Zotero/storage/XSVVGL5E/Wang et al. - SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONI.pdf}
}

@misc{Webson2023AreLanguageModels,
  title = {Are {{Language Models Worse}} than {{Humans}} at {{Following Prompts}}? {{It}}'s {{Complicated}}},
  shorttitle = {Are {{Language Models Worse}} than {{Humans}} at {{Following Prompts}}?},
  author = {Webson, Albert and Loo, Alyssa Marie and Yu, Qinan and Pavlick, Ellie},
  year = {2023},
  month = jan,
  number = {arXiv:2301.07085},
  eprint = {2301.07085},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.07085},
  url = {http://arxiv.org/abs/2301.07085},
  urldate = {2023-01-31},
  abstract = {Prompts have been the center of progress in advancing language models' zero-shot and few-shot performance. However, recent work finds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior is not "human like". In this study, we challenge a central assumption in such work: that humans would perform badly when given pathological instructions. We find that humans are able to reliably ignore irrelevant instructions and thus, like models, perform well on the underlying task despite an apparent lack of signal regarding the task they are being asked to do. However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Thus, our conclusion is mixed with respect to prior work. We argue against the earlier claim that high performance with irrelevant prompts constitutes evidence against models' instruction understanding, but we reinforce the claim that models' failure to follow misleading instructions raises concerns. More broadly, we caution that future research should not idealize human behaviors as a monolith and should not train or evaluate models to mimic assumptions about these behaviors without first validating humans' behaviors empirically.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,LM instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Webson2023AreLanguageModels_Are Language Models Worse than Humans at Following Prompts.pdf;/Users/ma/Zotero/storage/Q62D7LL4/2301.html}
}

@article{Weeks2015EmotionsPartisanshipMisperceptions,
  title = {Emotions, {{Partisanship}}, and {{Misperceptions}}: {{How Anger}} and {{Anxiety Moderate}} the {{Effect}} of {{Partisan Bias}} on {{Susceptibility}} to {{Political Misinformation}}: {{Emotions}} and {{Misperceptions}}},
  shorttitle = {Emotions, {{Partisanship}}, and {{Misperceptions}}},
  author = {Weeks, Brian E.},
  year = {2015},
  month = aug,
  journal = {Journal of Communication},
  volume = {65},
  number = {4},
  pages = {699--719},
  issn = {00219916},
  doi = {10.1111/jcom.12164},
  url = {https://academic.oup.com/joc/article/65/4/699-719/4082338},
  urldate = {2023-10-12},
  abstract = {Citizens are frequently misinformed about political issues and candidates but the circumstances under which inaccurate beliefs emerge are not fully understood. This experimental study demonstrates that the independent experience of two emotions, anger and anxiety, in part determines whether citizens consider misinformation in a partisan or open-minded fashion. Anger encourages partisan, motivated evaluation of uncorrected misinformation that results in beliefs consistent with the supported political party, while anxiety at times promotes initial beliefs based less on partisanship and more on the information environment. However, exposure to corrections improves belief accuracy, regardless of emotion or partisanship. The results indicate that the unique experience of anger and anxiety can affect the accuracy of political beliefs by strengthening or attenuating the influence of partisanship.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/EQNTDAKR/Weeks - 2015 - Emotions, Partisanship, and Misperceptions How An.pdf}
}

@inproceedings{Wei2021TriggerNotSufficient,
  title = {Trigger Is {{Not Sufficient}}: {{Exploiting Frame-aware Knowledge}} for {{Implicit Event Argument Extraction}}},
  shorttitle = {Trigger Is {{Not Sufficient}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wei, Kaiwen and Sun, Xian and Zhang, Zequn and Zhang, Jingyuan and Zhi, Guo and Jin, Li},
  year = {2021},
  month = aug,
  pages = {4672--4682},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.360},
  url = {https://aclanthology.org/2021.acl-long.360},
  urldate = {2023-02-09},
  abstract = {Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event. However, most prior works focus on capturing direct relations between arguments and the event trigger. The lack of reasoning ability brings many challenges to the extraction of implicit arguments. In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope. The proposed method leverages related arguments of the expected one as clues to guide the reasoning process. To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model. Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wei2021TriggerNotSufficient_Trigger is Not Sufficient.pdf}
}

@misc{Wei2023LargerLanguageModels,
  title = {Larger Language Models Do In-Context Learning Differently},
  author = {Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03846},
  eprint = {2303.03846},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.03846},
  urldate = {2023-04-25},
  abstract = {We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wei2023LargerLanguageModels_Larger language models do in-context learning differently.pdf;/Users/ma/Zotero/storage/E5KEP88A/2303.html}
}

@misc{Wei2023LargerLanguageModelsa,
  title = {Larger Language Models Do In-Context Learning Differently},
  author = {Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and Ma, Tengyu},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2303.03846v2},
  urldate = {2023-10-23},
  abstract = {We study how in-context learning (ICL) in language models is affected by semantic priors versus input-label mappings. We investigate two setups-ICL with flipped labels and ICL with semantically-unrelated labels-across various model families (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments on ICL with flipped labels show that overriding semantic priors is an emergent ability of model scale. While small language models ignore flipped labels presented in-context and thus rely primarily on semantic priors from pretraining, large models can override semantic priors when presented with in-context exemplars that contradict priors, despite the stronger semantic priors that larger models may hold. We next study semantically-unrelated label ICL (SUL-ICL), in which labels are semantically unrelated to their inputs (e.g., foo/bar instead of negative/positive), thereby forcing language models to learn the input-label mappings shown in in-context exemplars in order to perform the task. The ability to do SUL-ICL also emerges primarily with scale, and large-enough language models can even perform linear classification in a SUL-ICL setting. Finally, we evaluate instruction-tuned models and find that instruction tuning strengthens both the use of semantic priors and the capacity to learn input-label mappings, but more of the former.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wei2023LargerLanguageModelsa_Larger language models do in-context learning differently.pdf}
}

@misc{Wei2023ZeroShotInformationExtraction,
  title = {Zero-{{Shot Information Extraction}} via {{Chatting}} with {{ChatGPT}}},
  author = {Wei, Xiang and Cui, Xingyu and Cheng, Ning and Wang, Xiaobin and Zhang, Xin and Huang, Shen and Xie, Pengjun and Xu, Jinan and Chen, Yufeng and Zhang, Meishan and Jiang, Yong and Han, Wenjuan},
  year = {2023},
  month = feb,
  number = {arXiv:2302.10205},
  eprint = {2302.10205},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.10205},
  url = {http://arxiv.org/abs/2302.10205},
  urldate = {2023-06-21},
  abstract = {Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wei2023ZeroShotInformationExtraction_Zero-Shot Information Extraction via Chatting with ChatGPT.pdf;/Users/ma/Zotero/storage/CEMMCE23/2302.html}
}

@article{Weidinger2023UsingVeilIgnorance,
  title = {Using the {{Veil}} of {{Ignorance}} to Align {{AI}} Systems with Principles of Justice},
  author = {Weidinger, Laura and McKee, Kevin R. and Everett, Richard and Huang, Saffron and Zhu, Tina O. and Chadwick, Martin J. and Summerfield, Christopher and Gabriel, Iason},
  year = {2023},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {18},
  pages = {e2213709120},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2213709120},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.2213709120},
  urldate = {2023-05-03},
  abstract = {The philosopher John Rawls proposed the Veil of Ignorance (VoI) as a thought experiment to identify fair principles for governing a society. Here, we apply the VoI to an important governance domain: artificial intelligence (AI). In five incentive-compatible studies (N\;=\;2,\hspace{0.166em}508), including two preregistered protocols, participants choose principles to govern an Artificial Intelligence (AI) assistant from behind the veil: that is, without knowledge of their own relative position in the group. Compared to participants who have this information, we find a consistent preference for a principle that instructs the AI assistant to prioritize the worst-off. Neither risk attitudes nor political preferences adequately explain these choices. Instead, they appear to be driven by elevated concerns about fairness: Without prompting, participants who reason behind the VoI more frequently explain their choice in terms of fairness, compared to those in the Control condition. Moreover, we find initial support for the ability of the VoI to elicit more robust preferences: In the studies presented here, the VoI increases the likelihood of participants continuing to endorse their initial choice in a subsequent round where they know how they will be affected by the AI intervention and have a self-interested motivation to change their mind. These results emerge in both a descriptive and an immersive game. Our findings suggest that the VoI may be a suitable mechanism for selecting distributive principles to govern AI.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Weidinger2023UsingVeilIgnorance_Using the Veil of Ignorance to align AI systems with principles of justice.pdf}
}

@inproceedings{Weller2020LearningTaskDescriptions,
  title = {Learning from {{Task Descriptions}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Weller, Orion and Lourie, Nicholas and Gardner, Matt and Peters, Matthew E.},
  year = {2020},
  month = nov,
  pages = {1361--1375},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.105},
  url = {https://aclanthology.org/2020.emnlp-main.105},
  urldate = {2023-03-08},
  abstract = {Typically, machine learning systems solve new tasks by training on thousands of examples. In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two. To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area. We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks. Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model's ability to solve each task. Moreover, the dataset's structure tests specific types of systematic generalization. We find that the state-of-the-art T5 model achieves a score of 12\% on ZEST, leaving a significant challenge for NLP researchers.},
  keywords = {Instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Weller2020LearningTaskDescriptions_Learning from Task Descriptions.pdf}
}

@inproceedings{Williams2018BroadCoverageChallengeCorpus,
  title = {A {{Broad-Coverage Challenge Corpus}} for {{Sentence Understanding}} through {{Inference}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Williams, Adina and Nangia, Nikita and Bowman, Samuel},
  year = {2018},
  month = jun,
  pages = {1112--1122},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1101},
  url = {https://aclanthology.org/N18-1101},
  urldate = {2023-06-24},
  abstract = {This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Williams2018BroadCoverageChallengeCorpus_A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.pdf}
}

@misc{Wu2022EfficientMemoryAugmentedTransformer,
  title = {An {{Efficient Memory-Augmented Transformer}} for {{Knowledge-Intensive NLP Tasks}}},
  author = {Wu, Yuxiang and Zhao, Yu and Hu, Baotian and Minervini, Pasquale and Stenetorp, Pontus and Riedel, Sebastian},
  year = {2022},
  month = oct,
  number = {arXiv:2210.16773},
  eprint = {2210.16773},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.16773},
  url = {http://arxiv.org/abs/2210.16773},
  urldate = {2023-10-30},
  abstract = {Access to external knowledge is essential for many natural language processing tasks, such as question answering and dialogue. Existing methods often rely on a parametric model that stores knowledge in its parameters, or use a retrieval-augmented model that has access to an external knowledge source. Parametric and retrieval-augmented models have complementary strengths in terms of computational efficiency and predictive accuracy. To combine the strength of both approaches, we propose the Efficient Memory-Augmented Transformer (EMAT) -- it encodes external knowledge into a key-value memory and exploits the fast maximum inner product search for memory querying. We also introduce pre-training tasks that allow EMAT to encode informative key-value representations, and to learn an implicit strategy to integrate multiple memory slots into the transformer. Experiments on various knowledge-intensive tasks such as question answering and dialogue datasets show that, simply augmenting parametric models (T5-base) using our method produces more accurate results (e.g., 25.8 -{$>$} 44.3 EM on NQ) while retaining a high throughput (e.g., 1000 queries/s on NQ). Compared to retrieval-augmented models, EMAT runs substantially faster across the board and produces more accurate results on WoW and ELI5. Our code and datasets are available at https://github. com/uclnlp/EMAT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wu2022EfficientMemoryAugmentedTransformer_An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks.pdf;/Users/ma/Zotero/storage/JNJ3CV7B/2210.html}
}

@inproceedings{Wu2022IncorporatingInstructionalPrompts,
  title = {Incorporating {{Instructional Prompts}} into a {{Unified Generative Framework}} for {{Joint Multiple Intent Detection}} and {{Slot Filling}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Wu, Yangjun and Wang, Han and Zhang, Dongxiang and Chen, Gang and Zhang, Hao},
  year = {2022},
  month = oct,
  pages = {7203--7208},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  url = {https://aclanthology.org/2022.coling-1.631},
  urldate = {2023-05-03},
  abstract = {The joint multiple Intent Detection (ID) and Slot Filling (SF) is a significant challenge in spoken language understanding. Because the slots in an utterance may relate to multi-intents, most existing approaches focus on utilizing task-specific components to capture the relations between intents and slots. The customized networks restrict models from modeling commonalities between tasks and generalization for broader applications. To address the above issue, we propose a Unified Generative framework (UGEN) based on a prompt-based paradigm, and formulate the task as a question-answering problem. Specifically, we design 5-type templates as instructional prompts, and each template includes a question that acts as the driver to teach UGEN to grasp the paradigm, options that list the candidate intents or slots to reduce the answer search space, and the context denotes original utterance. Through the instructional prompts, UGEN is guided to understand intents, slots, and their implicit correlations. On two popular multi-intent benchmark datasets, experimental results demonstrate that UGEN achieves new SOTA performances on full-data and surpasses the baselines by a large margin on 5-shot (28.1\%) and 10-shot (23\%) scenarios, which verify that UGEN is robust and effective.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wu2022IncorporatingInstructionalPrompts_Incorporating Instructional Prompts into a Unified Generative Framework for.pdf}
}

@misc{Wu2023BloombergGPTLargeLanguage,
  title = {{{BloombergGPT}}: {{A Large Language Model}} for {{Finance}}},
  shorttitle = {{{BloombergGPT}}},
  author = {Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  year = {2023},
  month = mar,
  number = {arXiv:2303.17564},
  eprint = {2303.17564},
  primaryclass = {cs, q-fin},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.17564},
  url = {http://arxiv.org/abs/2303.17564},
  urldate = {2023-04-20},
  abstract = {The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. As a next step, we plan to release training logs (Chronicles) detailing our experience in training BloombergGPT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Quantitative Finance - General Finance},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wu2023BloombergGPTLargeLanguage_BloombergGPT.pdf;/Users/ma/Zotero/storage/CM3Q2WJY/2303.html}
}

@article{Wu2023GraphNeuralNetworks,
  title = {Graph {{Neural Networks}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {Graph {{Neural Networks}} for {{Natural Language Processing}}},
  author = {Wu, Lingfei and Chen, Yu and Shen, Kai and Guo, Xiaojie and Gao, Hanning and Li, Shucheng and Pei, Jian and Long, Bo},
  year = {2023},
  month = jan,
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  volume = {16},
  number = {2},
  pages = {119--328},
  publisher = {{Now Publishers, Inc.}},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000096},
  url = {https://www.nowpublishers.com/article/Details/MAL-096},
  urldate = {2023-09-11},
  abstract = {Graph Neural Networks for Natural Language Processing: A Survey},
  langid = {english},
  file = {/Users/ma/Zotero/storage/4WJQRFN6/survey.pdf;/Users/ma/Zotero/storage/JK8MW66D/Wu et al. - 2023 - Graph Neural Networks for Natural Language Process.pdf}
}

@article{Wu2023MEGACareKnowledgeguidedMultiview,
  title = {{{MEGACare}}: {{Knowledge-guided}} Multi-View Hypergraph Predictive Framework for Healthcare},
  shorttitle = {{{MEGACare}}},
  author = {Wu, Jialun and He, Kai and Mao, Rui and Li, Chen and Cambria, Erik},
  year = {2023},
  month = dec,
  journal = {Information Fusion},
  volume = {100},
  pages = {101939},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.101939},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253523002555},
  urldate = {2023-10-31},
  abstract = {Predicting a patient's future health condition by analyzing their Electronic Health Records (EHRs) is a trending subject in the intelligent medical field, which can help clinicians prescribe safely and effectively, and also make more accurate diagnoses. Benefiting from powerful feature extraction capabilities, graph representation learning can capture complex relationships and achieve promising performance in many clinical prediction tasks. However, existing works either exclusively consider single domain knowledge with an independent task or do not fully capitalize on domain knowledge that can provide more predictive signals in the code encoding stage. Moreover, the heterogeneous and high-dimensional nature of EHR data leads to a deficiency of hardly encoding implicit high-order correlations. To address these limitations, we proposed a knowledge-guided Multi-viEw hyperGrAph predictive framework (MEGACare) for diagnosis prediction and medication recommendation. Our MEGACare leveraged multi-faceted medical knowledge, including ontology structure, code description, and molecular information to enhance medical code presentations. Furthermore, we constructed an EHR hypergraph and a multi-view learning framework to capture the high-order correlation between patient visits and medical codes. Specifically, we propose three perspectives around the pairwise relationship between patient visits and medical codes to comprehensively learn patient representation and enhance the robustness of our framework. We evaluated our MEGACare framework against a set of state-of-the-art methods for two clinical outcome prediction tasks in the public MIMIC-III dataset, and the results showed that our proposed framework was superior to the baseline methods.11Our code and data are released at https://github.com/senticnet/MEGACare.},
  keywords = {Electronic health record,Healthcare,Hypergraph,Information bottleneck,Multi-view learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Wu2023MEGACareKnowledgeguidedMultiview_MEGACare.pdf}
}

@misc{Xi2023RisePotentialLarge,
  title = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}: {{A Survey}}},
  shorttitle = {The {{Rise}} and {{Potential}} of {{Large Language Model Based Agents}}},
  author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and Zheng, Rui and Fan, Xiaoran and Wang, Xiao and Xiong, Limao and Zhou, Yuhao and Wang, Weiran and Jiang, Changhao and Zou, Yicheng and Liu, Xiangyang and Yin, Zhangyue and Dou, Shihan and Weng, Rongxiang and Cheng, Wensen and Zhang, Qi and Qin, Wenjuan and Zheng, Yongyan and Qiu, Xipeng and Huang, Xuanjing and Gui, Tao},
  year = {2023},
  month = sep,
  number = {arXiv:2309.07864},
  eprint = {2309.07864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.07864},
  url = {http://arxiv.org/abs/2309.07864},
  urldate = {2023-11-01},
  abstract = {For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xi2023RisePotentialLarge_The Rise and Potential of Large Language Model Based Agents.pdf;/Users/ma/Zotero/storage/Q3X4M2QJ/2309.html}
}

@inproceedings{Xia2023FindParentThen,
  title = {Find {{Parent}} Then {{Label Children}}: {{A Two-stage Taxonomy Completion Method}} with {{Pre-trained Language Model}}},
  shorttitle = {Find {{Parent}} Then {{Label Children}}},
  booktitle = {Proceedings of the 17th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Xia, Fei and Weng, Yixuan and He, Shizhu and Liu, Kang and Zhao, Jun},
  year = {2023},
  month = may,
  pages = {1032--1042},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dubrovnik, Croatia}},
  url = {https://aclanthology.org/2023.eacl-main.73},
  urldate = {2023-05-04},
  abstract = {Taxonomies, which organize domain concepts into hierarchical structures, are crucial for building knowledge systems and downstream applications. As domain knowledge evolves, taxonomies need to be continuously updated to include new concepts.Previous approaches have mainly focused on adding concepts to the leaf nodes of the existing hierarchical tree, which does not fully utilize the taxonomy's knowledge and is unable to update the original taxonomy structure (usually involving non-leaf nodes). In this paper, we propose a two-stage method called ATTEMPT for taxonomy completion. Our method inserts new concepts into the correct position by finding a parent node and labeling child nodes. Specifically, by combining local nodes with prompts to generate natural sentences, we take advantage of pre-trained language models for hypernym/hyponymy recognition. Experimental results on two public datasets (including six domains) show that ATTEMPT performs best on both taxonomy completion and extension tasks, surpassing existing methods.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xia2023FindParentThen_Find Parent then Label Children.pdf}
}

@inproceedings{Xie2023GraphAwareLanguageModel,
  title = {Graph-{{Aware Language Model Pre-Training}} on a {{Large Graph Corpus Can Help Multiple Graph Applications}}},
  booktitle = {Proceedings of the 29th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Xie, Han and Zheng, Da and Ma, Jun and Zhang, Houyu and Ioannidis, Vassilis N. and Song, Xiang and Ping, Qing and Wang, Sheng and Yang, Carl and Xu, Yi and Zeng, Belinda and Chilimbi, Trishul},
  year = {2023},
  month = aug,
  series = {{{KDD}} '23},
  pages = {5270--5281},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3580305.3599833},
  url = {https://dl.acm.org/doi/10.1145/3580305.3599833},
  urldate = {2023-09-18},
  abstract = {Model pre-training on large text corpora has been demonstrated effective for various downstream applications in the NLP domain. In the graph mining domain, a similar analogy can be drawn for pre-training graph models on large graphs in the hope of benefiting downstream graph applications, which has also been explored by several recent studies. However, no existing study has ever investigated the pre-training of text plus graph models on large heterogeneous graphs with abundant textual information (a.k.a. large graph corpora) and then fine-tuning the model on different related downstream applications with different graph schemas. To address this problem, we propose a framework of graph-aware language model pre-training (GaLM) on a large graph corpus, which incorporates large language models and graph neural networks, and a variety of fine-tuning methods on downstream applications. We conduct extensive experiments on Amazon's real internal datasets and large public datasets. Comprehensive empirical results and in-depth analysis demonstrate the effectiveness of our proposed methods along with lessons learned.},
  isbn = {9798400701030},
  keywords = {graph neural network,heterogeneous graph,large language model,pre-training and fine-tuning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xie2023GraphAwareLanguageModel_Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help.pdf}
}

@misc{Xie2023LargeLanguageModels,
  title = {Large {{Language Models}} as {{Master Key}}: {{Unlocking}} the {{Secrets}} of {{Materials Science}} with {{GPT}}},
  shorttitle = {Large {{Language Models}} as {{Master Key}}},
  author = {Xie, Tong and Wan, Yuwei and Huang, Wei and Zhou, Yufei and Liu, Yixuan and Linghu, Qingyuan and Wang, Shaozhou and Kit, Chunyu and Grazian, Clara and Zhang, Wenjie and Hoex, Bram},
  year = {2023},
  month = apr,
  number = {arXiv:2304.02213},
  eprint = {2304.02213},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.02213},
  url = {http://arxiv.org/abs/2304.02213},
  urldate = {2023-10-26},
  abstract = {The amount of data has growing significance in exploring cutting-edge materials and a number of datasets have been generated either by hand or automated approaches. However, the materials science field struggles to effectively utilize the abundance of data, especially in applied disciplines where materials are evaluated based on device performance rather than their properties. This article presents a new natural language processing (NLP) task called structured information inference (SII) to address the complexities of information extraction at the device level in materials science. We accomplished this task by tuning GPT-3 on an existing perovskite solar cell FAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8\% F1-score and extended the dataset with data published since its release. The produced data is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature empowers materials scientists to develop models by selecting high-quality review articles within their domain. Additionally, we designed experiments to predict the electrical performance of solar cells and design materials or devices with targeted parameters using large language models (LLMs). Our results demonstrate comparable performance to traditional machine learning methods without feature selection, highlighting the potential of LLMs to acquire scientific knowledge and design new materials akin to materials scientists.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xie2023LargeLanguageModels_Large Language Models as Master Key.pdf;/Users/ma/Zotero/storage/TC6NC5DR/2304.html}
}

@inproceedings{Xin2021ArtAbstentionSelective,
  title = {The {{Art}} of {{Abstention}}: {{Selective Prediction}} and {{Error Regularization}} for {{Natural Language Processing}}},
  shorttitle = {The {{Art}} of {{Abstention}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Xin, Ji and Tang, Raphael and Yu, Yaoliang and Lin, Jimmy},
  year = {2021},
  month = aug,
  pages = {1040--1051},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.84},
  url = {https://aclanthology.org/2021.acl-long.84},
  urldate = {2023-07-18},
  abstract = {In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples. Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks. To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators. We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget. We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness. We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy\textendash efficiency trade-offs. Source code for this paper can be found at https://github.com/castorini/transformers-selective.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xin2021ArtAbstentionSelective_The Art of Abstention.pdf}
}

@inproceedings{Xu2021DocumentlevelEventExtraction,
  title = {Document-Level {{Event Extraction}} via {{Heterogeneous Graph-based Interaction Model}} with a {{Tracker}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Xu, Runxin and Liu, Tianyu and Li, Lei and Chang, Baobao},
  year = {2021},
  month = aug,
  pages = {3533--3546},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.274},
  url = {https://aclanthology.org/2021.acl-long.274},
  urldate = {2023-02-09},
  abstract = {Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.},
  keywords = {dataset\_Doc2EDAG,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2021DocumentlevelEventExtraction_Document-level Event Extraction via Heterogeneous Graph-based Interaction Model.pdf}
}

@inproceedings{Xu2022ImprovingEventCoreference,
  title = {Improving {{Event Coreference Resolution Using Document-level}} and {{Topic-level Information}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Xu, Sheng and Li, Peifeng and Zhu, Qiaoming},
  year = {2022},
  month = dec,
  pages = {6765--6775},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.454},
  urldate = {2023-03-03},
  abstract = {Event coreference resolution (ECR) aims to cluster event mentions that refer to the same real-world events. Deep learning methods have achieved SOTA results on the ECR task. However, due to the encoding length limitation, previous methods either adopt classical pairwise models based on sentence-level context or split each document into multiple chunks and encode them separately. They failed to capture the interactions and contextual cues among those long-distance event mentions. Besides, high-level information, such as event topics, is rarely considered to enhance representation learning for ECR. To address the above two issues, we first apply a Longformer-based encoder to obtain the document-level embeddings and an encoder with a trigger-mask mechanism to learn sentence-level embeddings based on local context. In addition, we propose an event topic generator to infer the latent topic-level representations. Finally, using the above event embeddings, we employ a multiple tensor matching method to capture their interactions at the document, sentence, and topic levels. Experimental results on the KBP 2017 dataset show that our model outperforms the SOTA baselines.},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2022ImprovingEventCoreference_Improving Event Coreference Resolution Using Document-level and Topic-level.pdf}
}

@misc{Xu2022RealisticLowresourceRelation,
  title = {Towards {{Realistic Low-resource Relation Extraction}}: {{A Benchmark}} with {{Empirical Baseline Study}}},
  shorttitle = {Towards {{Realistic Low-resource Relation Extraction}}},
  author = {Xu, Xin and Chen, Xiang and Zhang, Ningyu and Xie, Xin and Chen, Xi and Chen, Huajun},
  year = {2022},
  month = nov,
  number = {arXiv:2210.10678},
  eprint = {2210.10678},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.10678},
  urldate = {2023-06-28},
  abstract = {This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distribution; (iii) Data augmentation complements existing baselines and can bring much performance gain, while self-training may not consistently achieve advancement to low-resource RE. Code and datasets are in https://github.com/zjunlp/LREBench.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2022RealisticLowresourceRelation_Towards Realistic Low-resource Relation Extraction.pdf;/Users/ma/Zotero/storage/59QKEJIM/2210.html}
}

@inproceedings{Xu2022TwoStreamAMRenhancedModel,
  title = {A {{Two-Stream AMR-enhanced Model}} for {{Document-level Event Argument Extraction}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Xu, Runxin and Wang, Peiyi and Liu, Tianyu and Zeng, Shuang and Chang, Baobao and Sui, Zhifang},
  year = {2022},
  month = jul,
  pages = {5025--5036},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.370},
  url = {https://aclanthology.org/2022.naacl-main.370},
  urldate = {2023-02-09},
  abstract = {Most previous studies aim at extracting events from a single sentence, while document-level event extraction still remains under-explored. In this paper, we focus on extracting event arguments from an entire document, which mainly faces two critical problems: a) the long-distance dependency between trigger and arguments over sentences; b) the distracting context towards an event in the document. To address these issues, we propose a Two-Stream Abstract meaning Representation enhanced extraction model (TSAR). TSAR encodes the document from different perspectives by a two-stream encoding module, to utilize local and global information and lower the impact of distracting context. Besides, TSAR introduces an AMR-guided interaction module to capture both intra-sentential and inter-sentential features, based on the locally and globally constructed AMR semantic graphs. An auxiliary boundary loss is introduced to enhance the boundary information for text spans explicitly. Extensive experiments illustrate that TSAR outperforms previous state-of-the-art by a large margin, with 2.54 F1 and 5.13 F1 performance gain on the public RAMS and WikiEvents datasets respectively, showing the superiority in the cross-sentence arguments extraction. We release our code in https://github.com/ PKUnlp-icler/TSAR.},
  keywords = {dataset\_MUC4,dataset\_RAMS,dataset\_WikiEvents,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2022TwoStreamAMRenhancedModel_A Two-Stream AMR-enhanced Model for Document-level Event Argument Extraction.pdf}
}

@misc{Xu2022ZeroPromptScalingPromptBased,
  title = {{{ZeroPrompt}}: {{Scaling Prompt-Based Pretraining}} to 1,000 {{Tasks Improves Zero-Shot Generalization}}},
  shorttitle = {{{ZeroPrompt}}},
  author = {Xu, Hanwei and Chen, Yujun and Du, Yulun and Shao, Nan and Wang, Yanggang and Li, Haiyu and Yang, Zhilin},
  year = {2022},
  month = oct,
  number = {arXiv:2201.06910},
  eprint = {2201.06910},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.06910},
  urldate = {2023-03-08},
  abstract = {We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting. While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that task scaling can be an efficient alternative to model scaling; i.e., the model size has little impact on performance with an extremely large number of tasks. Our results show that task scaling can substantially improve training efficiency by 30 times in FLOPs. Moreover, we present a prompting method that incorporates a genetic algorithm to automatically search for the best prompt for unseen tasks, along with a few other improvements. Empirically, ZeroPrompt substantially improves both the efficiency and the performance of zero-shot learning across a variety of academic and production datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Instruction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2022ZeroPromptScalingPromptBased_ZeroPrompt.pdf;/Users/ma/Zotero/storage/JVWW7RZC/2201.html}
}

@misc{Xu2023CanNLIProvide,
  title = {Can {{NLI Provide Proper Indirect Supervision}} for {{Low-resource Biomedical Relation Extraction}}?},
  author = {Xu, Jiashu and Ma, Mingyu Derek and Chen, Muhao},
  year = {2023},
  month = may,
  number = {arXiv:2212.10784},
  eprint = {2212.10784},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10784},
  url = {http://arxiv.org/abs/2212.10784},
  urldate = {2023-05-24},
  abstract = {Two key obstacles in biomedical relation extraction (RE) are the scarcity of annotations and the prevalence of instances without explicitly pre-defined labels due to low annotation coverage. Existing approaches, which treat biomedical RE as a multi-class classification task, often result in poor generalization in low-resource settings and do not have the ability to make selective prediction on unknown cases but give a guess from seen relations, hindering the applicability of those approaches. We present NBR, which converts biomedical RE as natural language inference formulation through indirect supervision. By converting relations to natural language hypotheses, NBR is capable of exploiting semantic cues to alleviate annotation scarcity. By incorporating a ranking-based loss that implicitly calibrates abstinent instances, NBR learns a clearer decision boundary and is instructed to abstain on uncertain instances. Extensive experiments on three widely-used biomedical RE benchmarks, namely ChemProt, DDI and GAD, verify the effectiveness of NBR in both full-set and low-resource regimes. Our analysis demonstrates that indirect supervision benefits biomedical RE even when a domain gap exists, and combining NLI knowledge with biomedical knowledge leads to the best performance gains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2023CanNLIProvide_Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical.pdf;/Users/ma/Zotero/storage/XK6IPDPA/2212.html}
}

@misc{Xu2023LemurHarmonizingNatural,
  title = {Lemur: {{Harmonizing Natural Language}} and {{Code}} for {{Language Agents}}},
  shorttitle = {Lemur},
  author = {Xu, Yiheng and Su, Hongjin and Xing, Chen and Mi, Boyu and Liu, Qian and Shi, Weijia and Hui, Binyuan and Zhou, Fan and Liu, Yitao and Xie, Tianbao and Cheng, Zhoujun and Zhao, Siheng and Kong, Lingpeng and Wang, Bailin and Xiong, Caiming and Yu, Tao},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06830},
  eprint = {2310.06830},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.06830},
  url = {http://arxiv.org/abs/2310.06830},
  urldate = {2023-10-25},
  abstract = {We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks among open-source models. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. https://github.com/OpenLemur/Lemur},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/6SUCRVWE/Xu et al. - 2023 - Lemur Harmonizing Natural Language and Code for L.pdf;/Users/ma/Zotero/storage/A3FBPWW3/2310.html}
}

@inproceedings{Xu2023VecoCareVisitSequencesClinical,
  title = {{{VecoCare}}: {{Visit Sequences-Clinical Notes Joint Learning}} for {{Diagnosis Prediction}} in {{Healthcare Data}}},
  shorttitle = {{{VecoCare}}},
  booktitle = {Thirty-{{Second International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Xu, Yongxin and Yang, Kai and Zhang, Chaohe and Zou, Peinie and Wang, Zhiyuan and Ding, Hongxin and Zhao, Junfeng and Wang, Yasha and Xie, Bing},
  year = {2023},
  month = aug,
  volume = {5},
  pages = {4921--4929},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2023/547},
  url = {https://www.ijcai.org/proceedings/2023/547},
  urldate = {2023-10-31},
  abstract = {Electronic proceedings of IJCAI 2023},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2023VecoCareVisitSequencesClinical_VecoCare.pdf}
}

@misc{Xu2023WizardLMEmpoweringLarge,
  title = {{{WizardLM}}: {{Empowering Large Language Models}} to {{Follow Complex Instructions}}},
  shorttitle = {{{WizardLM}}},
  author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12244},
  eprint = {2304.12244},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12244},
  url = {http://arxiv.org/abs/2304.12244},
  urldate = {2023-05-30},
  abstract = {Training large language models (LLM) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM model are preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing large language models. Our codes and generated data are public at https://github.com/nlpxucan/WizardLM},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Xu2023WizardLMEmpoweringLarge_WizardLM.pdf;/Users/ma/Zotero/storage/RGT6NQVF/2304.html}
}

@inproceedings{Yamada2020LUKEDeepContextualized,
  title = {{{LUKE}}: {{Deep Contextualized Entity Representations}} with {{Entity-aware Self-attention}}},
  shorttitle = {{{LUKE}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Yamada, Ikuya and Asai, Akari and Shindo, Hiroyuki and Takeda, Hideaki and Matsumoto, Yuji},
  year = {2020},
  month = nov,
  pages = {6442--6454},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.523},
  url = {https://aclanthology.org/2020.emnlp-main.523},
  urldate = {2023-06-24},
  abstract = {Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yamada2020LUKEDeepContextualized_LUKE.pdf}
}

@inproceedings{Yan2021UnifiedGenerativeFramework,
  title = {A {{Unified Generative Framework}} for {{Various NER Subtasks}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yan, Hang and Gui, Tao and Dai, Junqi and Guo, Qipeng and Zhang, Zheng and Qiu, Xipeng},
  year = {2021},
  month = aug,
  pages = {5808--5822},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.451},
  url = {https://aclanthology.org/2021.acl-long.451},
  urldate = {2023-05-01},
  abstract = {Named Entity Recognition (NER) is the task of identifying spans that represent entities in sentences. Whether the entity spans are nested or discontinuous, the NER task can be categorized into the flat NER, nested NER, and discontinuous NER subtasks. These subtasks have been mainly solved by the token-level sequence labelling or span-level classification. However, these solutions can hardly tackle the three kinds of NER subtasks concurrently. To that end, we propose to formulate the NER subtasks as an entity span sequence generation task, which can be solved by a unified sequence-to-sequence (Seq2Seq) framework. Based on our unified framework, we can leverage the pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the special design of the tagging schema or ways to enumerate spans. We exploit three types of entity representations to linearize entities into a sequence. Our proposed framework is easy-to-implement and achieves state-of-the-art (SoTA) or near SoTA performance on eight English NER datasets, including two flat NER datasets, three nested NER datasets, and three discontinuous NER datasets.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yan2021UnifiedGenerativeFramework_A Unified Generative Framework for Various NER Subtasks.pdf}
}

@inproceedings{Yang2018DCFEEDocumentlevelChinese,
  title = {{{DCFEE}}: {{A Document-level Chinese Financial Event Extraction System}} Based on {{Automatically Labeled Training Data}}},
  shorttitle = {{{DCFEE}}},
  booktitle = {Proceedings of {{ACL}} 2018, {{System Demonstrations}}},
  author = {Yang, Hang and Chen, Yubo and Liu, Kang and Xiao, Yang and Zhao, Jun},
  year = {2018},
  month = jul,
  pages = {50--55},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-4009},
  url = {https://aclanthology.org/P18-4009},
  urldate = {2023-03-03},
  abstract = {We present an event extraction framework to detect event mentions and extract events from the document-level financial news. Up to now, methods based on supervised learning paradigm gain the highest performance in public datasets (such as ACE2005, KBP2015). These methods heavily depend on the manually labeled training data. However, in particular areas, such as financial, medical and judicial domains, there is no enough labeled data due to the high cost of data labeling process. Moreover, most of the current methods focus on extracting events from one sentence, but an event is usually expressed by multiple sentences in one document. To solve these problems, we propose a Document-level Chinese Financial Event Extraction (DCFEE) system which can automatically generate a large scaled labeled data and extract events from the whole document. Experimental results demonstrate the effectiveness of it},
  keywords = {document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2018DCFEEDocumentlevelChinese_DCFEE.pdf}
}

@inproceedings{Yang2019ChannelMattersSelfdisclosure,
  title = {The {{Channel Matters}}: {{Self-disclosure}}, {{Reciprocity}} and {{Social Support}} in {{Online Cancer Support Groups}}},
  shorttitle = {The {{Channel Matters}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Yang, Diyi and Yao, Zheng and Seering, Joseph and Kraut, Robert},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300261},
  url = {https://doi.org/10.1145/3290605.3300261},
  urldate = {2023-11-04},
  abstract = {People with health concerns go to online health support groups to obtain help and advice. To do so, they frequently disclose personal details, many times in public. Although research in non-health settings suggests that people self-disclose less in public than in private, this pattern may not apply to health support groups where people want to get relevant help. Our work examines how the use of private and public channels influences members' self-disclosure in an online cancer support group, and how channels moderate the influence of self-disclosure on reciprocity and receiving support. By automatically measuring people's self-disclosure at scale, we found that members of cancer support groups revealed more negative self-disclosure in the public channels compared to the private channels. Although one's self-disclosure leads others to self-disclose and to provide support, these effects were generally stronger in the private channel. These channel effects probably occur because the public channels are the primary venue for support exchange, while the private channels are mainly used for follow-up conversations. We discuss theoretical and practical implications of our work.},
  isbn = {978-1-4503-5970-2},
  keywords = {channel difference,online communities,online health support groups,self-disclosure,social support},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2019ChannelMattersSelfdisclosure_The Channel Matters.pdf}
}

@article{Yang2021COVID19InfodemicTwitter,
  title = {The {{COVID-19 Infodemic}}: {{Twitter}} versus {{Facebook}}},
  shorttitle = {The {{COVID-19 Infodemic}}},
  author = {Yang, Kai-Cheng and Pierri, Francesco and Hui, Pik-Mai and Axelrod, David and {Torres-Lugo}, Christopher and Bryden, John and Menczer, Filippo},
  year = {2021},
  month = jan,
  journal = {Big Data \& Society},
  volume = {8},
  number = {1},
  pages = {20539517211013861},
  publisher = {{SAGE Publications Ltd}},
  issn = {2053-9517},
  doi = {10.1177/20539517211013861},
  url = {https://doi.org/10.1177/20539517211013861},
  urldate = {2023-10-15},
  abstract = {The global spread of the novel coronavirus is affected by the spread of related misinformation?the so-called COVID-19 Infodemic?that makes populations more vulnerable to the disease through resistance to mitigation efforts. Here, we analyze the prevalence and diffusion of links to low-credibility content about the pandemic across two major social media platforms, Twitter and Facebook. We characterize cross-platform similarities and differences in popular sources, diffusion patterns, influencers, coordination, and automation. Comparing the two platforms, we find divergence among the prevalence of popular low-credibility sources and suspicious videos. A minority of accounts and pages exert a strong influence on each platform. These misinformation ?superspreaders? are often associated with the low-credibility sources and tend to be verified by the platforms. On both platforms, there is evidence of coordinated sharing of Infodemic content. The overt nature of this manipulation points to the need for societal-level solutions in addition to mitigation strategies within the platforms. However, we highlight limits imposed by inconsistent data-access policies on our capability to study harmful manipulations of information ecosystems.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2021COVID19InfodemicTwitter_The COVID-19 Infodemic.pdf}
}

@inproceedings{Yang2021DocumentlevelEventExtraction,
  title = {Document-Level {{Event Extraction}} via {{Parallel Prediction Networks}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yang, Hang and Sui, Dianbo and Chen, Yubo and Liu, Kang and Zhao, Jun and Wang, Taifeng},
  year = {2021},
  month = aug,
  pages = {6298--6308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.492},
  url = {https://aclanthology.org/2021.acl-long.492},
  urldate = {2023-02-09},
  abstract = {Document-level event extraction (DEE) is indispensable when events are described throughout a document. We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document. It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences. In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner. Specifically, we first introduce a document-level encoder to obtain the document-aware representations. Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel. Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization. The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task. Code will be available at https://github.com/HangYang-NLP/DE-PPN.},
  keywords = {dataset\_Doc2EDAG,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2021DocumentlevelEventExtraction_Document-level Event Extraction via Parallel Prediction Networks.pdf}
}

@incollection{Yang2022MultiDomainLongTailedRecognition,
  title = {On {{Multi-Domain Long-Tailed Recognition}}, {{Imbalanced Domain Generalization}} and {{Beyond}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2022},
  author = {Yang, Yuzhe and Wang, Hao and Katabi, Dina},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  volume = {13680},
  pages = {57--75},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20044-1_4},
  url = {https://link.springer.com/10.1007/978-3-031-20044-1\_4},
  urldate = {2023-10-27},
  abstract = {Real-world data often exhibit imbalanced label distributions. Existing studies on data imbalance focus on single-domain settings, i.e., samples are from the same data distribution. However, natural data can originate from distinct domains, where a minority class in one domain could have abundant instances from other domains. We formalize the task of Multi-Domain Long-Tailed Recognition (MDLT), which learns from multi-domain imbalanced data, addresses label imbalance, domain shift, and divergent label distributions across domains, and generalizes to all domain-class pairs. We first develop the domain-class transferability graph, and show that such transferability governs the success of learning in MDLT. We then propose BoDA, a theoretically grounded learning strategy that tracks the upper bound of transferability statistics, and ensures balanced alignment and calibration across imbalanced domain-class distributions. We curate five MDLT benchmarks based on widely-used multi-domain datasets, and compare BoDA to twenty algorithms that span different learning strategies. Extensive and rigorous experiments verify the superior performance of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on Domain Generalization benchmarks, highlighting the importance of addressing data imbalance across domains, which can be crucial for improving generalization to unseen domains. Code and data are available at: https://github.com/YyzHarry/multi-domain-imbalance.},
  isbn = {978-3-031-20043-4 978-3-031-20044-1},
  langid = {english},
  file = {/Users/ma/Zotero/storage/2GNKTEG9/Yang et al. - 2022 - On Multi-Domain Long-Tailed Recognition, Imbalance.pdf}
}

@misc{Yang2023HarnessingPowerLLMs,
  title = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}: {{A Survey}} on {{ChatGPT}} and {{Beyond}}},
  shorttitle = {Harnessing the {{Power}} of {{LLMs}} in {{Practice}}},
  author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Yin, Bing and Hu, Xia},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13712},
  eprint = {2304.13712},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.13712},
  url = {http://arxiv.org/abs/2304.13712},
  urldate = {2023-05-30},
  abstract = {This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \textbackslash url\{https://github.com/Mooler0410/LLMsPracticalGuide\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2023HarnessingPowerLLMsa_Harnessing the Power of LLMs in Practice.pdf;/Users/ma/Zotero/storage/GVVNCMPK/2304.html}
}

@article{Yang2023InterpretableDiseasePrediction,
  title = {Interpretable {{Disease Prediction}} via {{Path Reasoning}} over Medical Knowledge Graphs and Admission History},
  author = {Yang, Zongbao and Lin, Yuchen and Xu, Yinxin and Hu, Jinlong and Dong, Shoubin},
  year = {2023},
  month = dec,
  journal = {Knowledge-Based Systems},
  volume = {281},
  pages = {111082},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2023.111082},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705123008328},
  urldate = {2023-10-31},
  abstract = {Disease prediction based on patients' historical admission records is an essential task in the medical field, but current predictive models often lack interpretability, which is a critical aspect in clinical practice. In this paper, we propose a Knowledge Guided Interpretable Disease Prediction method (KGxDP) via Path Reasoning over Medical Knowledge Graphs and Admission History. In KGxDP, the representation of a patient is formulated via a personalized medical knowledge graph, which is then combined with the patient's admission sequence embedding to form an inclusive subgraph. This admission sequence embedding is modeled by a Transformer based on the patient's admission history, capturing the time-based variations of each diagnosis. Furthermore, the subgraph is updated via graph reasoning by using a node-type and edge-type specified Graph Attention Network (GAT) and subsequently combined with admission sequence embedding for disease prediction. This process also facilitates interpretability by extracting critical paths within the subgraphs. Empirical evaluations on public MIMIC-III, MIMIC-IV and eICU datasets demonstrate that KGxDP outperforms state-of-arts models in predicting patients' future diseases while also providing convincing explanations. The extracted paths are used as prompts for ChatGPT to generates user friendly, understandable Natural Language Explanations (NLE) for the prediction results, which also shows that the extracted paths by KGxDP have strong interpretability. This augmentation in predictive accuracy and explanation reliability holds significant potential to positively impact clinical decision-making.},
  keywords = {Admission history,Disease prediction,Graph reasoning,Medical knowledge graph,Natural Language Explanation},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2023InterpretableDiseasePrediction_Interpretable Disease Prediction via Path Reasoning over medical knowledge.pdf}
}

@article{Yang2023KerPrintLocalGlobalKnowledge,
  title = {{{KerPrint}}: {{Local-Global Knowledge Graph Enhanced Diagnosis Prediction}} for {{Retrospective}} and {{Prospective Interpretations}}},
  shorttitle = {{{KerPrint}}},
  author = {Yang, Kai and Xu, Yongxin and Zou, Peinie and Ding, Hongxin and Zhao, Junfeng and Wang, Yasha and Xie, Bing},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {4},
  pages = {5357--5365},
  issn = {2374-3468},
  doi = {10.1609/aaai.v37i4.25667},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/25667},
  urldate = {2023-10-31},
  abstract = {While recent developments of deep learning models have led to record-breaking achievements in many areas, the lack of sufficient interpretation remains a problem for many specific applications, such as the diagnosis prediction task in healthcare. The previous knowledge graph(KG) enhanced approaches mainly focus on learning clinically meaningful representations, the importance of medical concepts, and even the knowledge paths from inputs to labels. However, it is infeasible to interpret the diagnosis prediction, which needs to consider different medical concepts, various medical relationships, and the time-effectiveness of knowledge triples in different patient contexts. More importantly, the retrospective and prospective interpretations of disease processes are valuable to clinicians for the patients' confounding diseases. We propose KerPrint, a novel KG enhanced approach for retrospective and prospective interpretations to tackle these problems. Specifically, we propose a time-aware KG attention method to solve the problem of knowledge decay over time for trustworthy retrospective interpretation. We also propose a novel element-wise attention method to select candidate global knowledge using comprehensive representations from the local KG for prospective interpretation. We validate the effectiveness of our KerPrint through an extensive experimental study on a real-world dataset and a public dataset. The results show that our proposed approach not only achieves significant improvement over knowledge-enhanced methods but also gives the interpretability of diagnosis prediction in both retrospective and prospective views.},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {PEAI: Interpretability and Explainability},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2023KerPrintLocalGlobalKnowledge_KerPrint.pdf}
}

@misc{Yang2023LargeLanguageModels,
  title = {Large {{Language Models}} as {{Optimizers}}},
  author = {Yang, Chengrun and Wang, Xuezhi and Lu, Yifeng and Liu, Hanxiao and Le, Quoc V. and Zhou, Denny and Chen, Xinyun},
  year = {2023},
  month = sep,
  number = {arXiv:2309.03409},
  eprint = {2309.03409},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.03409},
  url = {http://arxiv.org/abs/2309.03409},
  urldate = {2023-10-11},
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8\% on GSM8K, and by up to 50\% on Big-Bench Hard tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2023LargeLanguageModels_Large Language Models as Optimizers.pdf;/Users/ma/Zotero/storage/6EA8NM2R/2309.html}
}

@misc{Yang2023LeveragingLargeLanguage,
  title = {Leveraging {{Large Language Model}} for {{Automatic Evolving}} of {{Industrial Data-Centric R}}\&{{D Cycle}}},
  author = {Yang, Xu and Yang, Xiao and Liu, Weiqing and Li, Jinhui and Yu, Peng and Ye, Zeqi and Bian, Jiang},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11249},
  eprint = {2310.11249},
  primaryclass = {cs, q-fin},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.11249},
  url = {http://arxiv.org/abs/2310.11249},
  urldate = {2023-11-02},
  abstract = {In the wake of relentless digital transformation, data-driven solutions are emerging as powerful tools to address multifarious industrial tasks such as forecasting, anomaly detection, planning, and even complex decision-making. Although data-centric R\&D has been pivotal in harnessing these solutions, it often comes with significant costs in terms of human, computational, and time resources. This paper delves into the potential of large language models (LLMs) to expedite the evolution cycle of data-centric R\&D. Assessing the foundational elements of data-centric R\&D, including heterogeneous task-related data, multi-facet domain knowledge, and diverse computing-functional tools, we explore how well LLMs can understand domain-specific requirements, generate professional ideas, utilize domain-specific tools to conduct experiments, interpret results, and incorporate knowledge from past endeavors to tackle new challenges. We take quantitative investment research as a typical example of industrial data-centric R\&D scenario and verified our proposed framework upon our full-stack open-sourced quantitative research platform Qlib and obtained promising results which shed light on our vision of automatic evolving of industrial data-centric R\&D cycle.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Finance - General Finance},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yang2023LeveragingLargeLanguage_Leveraging Large Language Model for Automatic Evolving of Industrial.pdf;/Users/ma/Zotero/storage/8KXQSJYN/2310.html}
}

@misc{Yao2022SchemaawareReferencePrompt,
  title = {Schema-Aware {{Reference}} as {{Prompt Improves Data-Efficient Relational Triple}} and {{Event Extraction}}},
  author = {Yao, Yunzhi and Mao, Shengyu and Chen, Xiang and Zhang, Ningyu and Deng, Shumin and Chen, Huajun},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10709},
  eprint = {2210.10709},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.10709},
  url = {http://arxiv.org/abs/2210.10709},
  urldate = {2023-02-24},
  abstract = {Information Extraction, which aims to extract structural relational triple or event from unstructured texts, often suffers from data scarcity issues. With the development of pre-trained language models, many prompt-based approaches to data-efficient information extraction have been proposed and achieved impressive performance. However, existing prompt learning methods for information extraction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structure knowledge with pre-defined schema; (ii) representation learning with locally individual instances limits the performance given the insufficient features. In this paper, we propose a novel approach of schema-aware Reference As Prompt (RAP), which dynamically leverage schema and knowledge inherited from global (few-shot) training data for each sample. Specifically, we propose a schema-aware reference store, which unifies symbolic schema and relevant textual instances. Then, we employ a dynamic reference integration module to retrieve pertinent knowledge from the datastore as prompts during training and inference. Experimental results demonstrate that RAP can be plugged into various existing models and outperforms baselines in low-resource settings on four datasets of relational triple extraction and event extraction. In addition, we provide comprehensive empirical ablations and case analysis regarding different types and scales of knowledge in order to better understand the mechanisms of RAP. Code is available in https://github.com/zjunlp/RAP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yao2022SchemaawareReferencePrompt_Schema-aware Reference as Prompt Improves Data-Efficient Relational Triple and.pdf;/Users/ma/Zotero/storage/585TS35U/2210.html}
}

@misc{Yao2023ReActSynergizingReasoning,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03629},
  url = {http://arxiv.org/abs/2210.03629},
  urldate = {2023-10-07},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yao2023ReActSynergizingReasoning_ReAct.pdf;/Users/ma/Zotero/storage/VCDTHSRF/2210.html}
}

@misc{Yao2023TreeThoughtsDeliberate,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = {2023},
  month = may,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.10601},
  url = {http://arxiv.org/abs/2305.10601},
  urldate = {2023-07-17},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/ysymyth/tree-of-thought-llm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yao2023TreeThoughtsDeliberate_Tree of Thoughts.pdf;/Users/ma/Zotero/storage/DBII8HZK/2305.html}
}

@misc{Yao2023WebShopScalableRealWorld,
  title = {{{WebShop}}: {{Towards Scalable Real-World Web Interaction}} with {{Grounded Language Agents}}},
  shorttitle = {{{WebShop}}},
  author = {Yao, Shunyu and Chen, Howard and Yang, John and Narasimhan, Karthik},
  year = {2023},
  month = feb,
  number = {arXiv:2207.01206},
  eprint = {2207.01206},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.01206},
  url = {http://arxiv.org/abs/2207.01206},
  urldate = {2023-10-26},
  abstract = {Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop WebShop -- a simulated e-commerce website environment with \$1.18\$ million real-world products and \$12,087\$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over \$1,600\$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of \$29\textbackslash\%\$, which outperforms rule-based heuristics (\$9.6\textbackslash\%\$) but is far lower than human expert performance (\$59\textbackslash\%\$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yao2023WebShopScalableRealWorld_WebShop.pdf;/Users/ma/Zotero/storage/VQD9PVCQ/2207.html}
}

@article{YaoOntologyawarePrescriptionRecommendation,
  title = {Ontology-Aware {{Prescription Recommendation}} in {{Treatment Pathways Using Multi-evidence Healthcare Data}}},
  author = {Yao, Zijun and Liu, Bin and Wang, Fei and Sow, Daby and Li, Ying},
  journal = {ACM Transactions on Information Systems},
  volume = {41},
  number = {4},
  langid = {english},
  file = {/Users/ma/Zotero/storage/Y6MKSLM6/Yao et al. - Ontology-aware Prescription Recommendation in Trea.pdf}
}

@inproceedings{Ye2022GenerativeKnowledgeGraph,
  title = {Generative {{Knowledge Graph Construction}}: {{A Review}}},
  shorttitle = {Generative {{Knowledge Graph Construction}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ye, Hongbin and Zhang, Ningyu and Chen, Hui and Chen, Huajun},
  year = {2022},
  month = dec,
  pages = {1--17},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.1},
  urldate = {2023-05-01},
  abstract = {Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ye2022GenerativeKnowledgeGraph_Generative Knowledge Graph Construction.pdf}
}

@misc{Ye2022GuessInstructionFlipped,
  title = {Guess the {{Instruction}}! {{Flipped Learning Makes Language Models Stronger Zero-Shot Learners}}},
  author = {Ye, Seonghyeon and Kim, Doyoung and Jang, Joel and Shin, Joongbo and Seo, Minjoon},
  year = {2022},
  month = dec,
  number = {arXiv:2210.02969},
  eprint = {2210.02969},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2210.02969},
  urldate = {2023-03-05},
  abstract = {Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4\% and 9.7\% points, respectively. Flipped gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20\% average F1 score. This indicates that the strong task generalization of Flipped comes from improved generalization to novel labels. We release our code at https://github.com/seonghyeonye/Flipped-Learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ye2022GuessInstructionFlipped_Guess the Instruction.pdf;/Users/ma/Zotero/storage/5VIJ3QZB/2210.html}
}

@misc{Ye2022ZeroGenEfficientZeroshot,
  title = {{{ZeroGen}}: {{Efficient Zero-shot Learning}} via {{Dataset Generation}}},
  shorttitle = {{{ZeroGen}}},
  author = {Ye, Jiacheng and Gao, Jiahui and Li, Qintong and Xu, Hang and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  year = {2022},
  month = oct,
  number = {arXiv:2202.07922},
  eprint = {2202.07922},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2202.07922},
  urldate = {2023-05-12},
  abstract = {There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, \textbackslash textsc\{ZeroGen\}. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotation-free and efficient, we argue that \textbackslash textsc\{ZeroGen\} can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of \textbackslash textsc\{ZeroGen\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Ye2022ZeroGenEfficientZeroshot_ZeroGen.pdf;/Users/ma/Zotero/storage/QC9SRWCY/2202.html}
}

@article{YeGUESSINSTRUCTIONFLIPPED,
  title = {{{GUESS THE INSTRUCTION}}! {{FLIPPED LEARNING MAKES LANGUAGE MODELS STRONGER ZERO-SHOT LEARNERS}}},
  author = {Ye, Seonghyeon and Kim, Doyoung and Jang, Joel and Shin, Joongbo and Seo, Minjoon},
  abstract = {Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose FLIPPED LEARNING, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with FLIPPED LEARNING, referred to as FLIPPED, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized FLIPPED outperforms zero-shot T011B (Sanh et al., 2021) and even a 16 times larger 3-shot GPT-3 (175B) (Brown et al., 2020) on average by 8.4\% and 9.7\% points, respectively. FLIPPED gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20\% average F1 score. This indicates that the strong task generalization of FLIPPED comes from improved generalization to novel labels. We release our code at github.com/seonghyeonye/Flipped-Learning.},
  langid = {english},
  keywords = {i2},
  file = {/Users/ma/Zotero/storage/Z4E9Q88E/Ye et al. - GUESS THE INSTRUCTION! FLIPPED LEARNING MAKES LANG.pdf}
}

@misc{Yin2023LargeLanguageModels,
  title = {Do {{Large Language Models Know What They Don}}'t {{Know}}?},
  author = {Yin, Zhangyue and Sun, Qiushi and Guo, Qipeng and Wu, Jiawen and Qiu, Xipeng and Huang, Xuanjing},
  year = {2023},
  month = may,
  number = {arXiv:2305.18153},
  eprint = {2305.18153},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.18153},
  urldate = {2023-05-31},
  abstract = {Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yin2023LargeLanguageModels_Do Large Language Models Know What They Don't Know.pdf;/Users/ma/Zotero/storage/U22H8AYM/2305.html}
}

@misc{You2021DesignSpaceGraph,
  title = {Design {{Space}} for {{Graph Neural Networks}}},
  author = {You, Jiaxuan and Ying, Rex and Leskovec, Jure},
  year = {2021},
  month = jul,
  number = {arXiv:2011.08843},
  eprint = {2011.08843},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.08843},
  url = {http://arxiv.org/abs/2011.08843},
  urldate = {2023-02-02},
  abstract = {The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/ma/Drive_ma/Papers_Zotero/You2021DesignSpaceGraph_Design Space for Graph Neural Networks.pdf;/Users/ma/Zotero/storage/M5RHPZJW/2011.html}
}

@misc{Youn2023KGLMIntegratingKnowledge,
  title = {{{KGLM}}: {{Integrating Knowledge Graph Structure}} in {{Language Models}} for {{Link Prediction}}},
  shorttitle = {{{KGLM}}},
  author = {Youn, Jason and Tagkopoulos, Ilias},
  year = {2023},
  month = may,
  number = {arXiv:2211.02744},
  eprint = {2211.02744},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.02744},
  url = {http://arxiv.org/abs/2211.02744},
  urldate = {2023-10-30},
  abstract = {The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Youn2023KGLMIntegratingKnowledge_KGLM.pdf;/Users/ma/Zotero/storage/SBM2E65F/2211.html}
}

@inproceedings{Yu2020BridgingTextKnowledge,
  title = {Bridging {{Text}} and {{Knowledge}} with {{Multi-Prototype Embedding}} for {{Few-Shot Relational Triple Extraction}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Yu, Haiyang and Zhang, Ningyu and Deng, Shumin and Ye, Hongbin and Zhang, Wei and Chen, Huajun},
  year = {2020},
  month = dec,
  pages = {6399--6410},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.563},
  url = {https://aclanthology.org/2020.coling-main.563},
  urldate = {2023-05-01},
  abstract = {Current supervised relational triple extraction approaches require huge amounts of labeled data and thus suffer from poor performance in few-shot settings. However, people can grasp new knowledge by learning a few instances. To this end, we take the first step to study the few-shot relational triple extraction, which has not been well understood. Unlike previous single-task few-shot problems, relational triple extraction is more challenging as the entities and relations have implicit correlations. In this paper, We propose a novel multi-prototype embedding network model to jointly extract the composition of relational triples, namely, entity pairs and corresponding relations. To be specific, we design a hybrid prototypical learning mechanism that bridges text and knowledge concerning both entities and relations. Thus, implicit correlations between entities and relations are injected. Additionally, we propose a prototype-aware regularization to learn more representative prototypes. Experimental results demonstrate that the proposed method can improve the performance of the few-shot triple extraction.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yu2020BridgingTextKnowledge_Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot.pdf}
}

@inproceedings{Yu2021LanguageEmbeddingsTypology,
  title = {Language {{Embeddings}} for {{Typology}} and {{Cross-lingual Transfer Learning}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Yu, Dian and He, Taiqi and Sagae, Kenji},
  year = {2021},
  month = aug,
  pages = {7210--7225},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.560},
  url = {https://aclanthology.org/2021.acl-long.560},
  urldate = {2023-03-13},
  abstract = {Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yu2021LanguageEmbeddingsTypology_Language Embeddings for Typology and Cross-lingual Transfer Learning.pdf}
}

@inproceedings{Yu2022UnsupervisedDiscoveryObject,
  title = {Unsupervised {{Discovery}} of {{Object Radiance Fields}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yu, Hong-Xing and Guibas, Leonidas and Wu, Jiajun},
  year = {2022},
  month = jan,
  url = {https://openreview.net/forum?id=rwE8SshAlxw},
  urldate = {2023-05-18},
  abstract = {We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that explains the image formation process, captures the scene's 3D nature, and is learned without supervision. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating the complex 3D-to-2D image formation process into powerful inference schemes like deep networks. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on multi-view RGB images without annotations, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF enables novel tasks, such as scene segmentation and editing in 3D, and it performs well on these tasks and on novel view synthesis on three datasets.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yu2022UnsupervisedDiscoveryObject_Unsupervised Discovery of Object Radiance Fields.pdf}
}

@misc{Yu2023EventLinkingGrounding,
  title = {Event {{Linking}}: {{Grounding Event Mentions}} to {{Wikipedia}}},
  shorttitle = {Event {{Linking}}},
  author = {Yu, Xiaodong and Yin, Wenpeng and Gupta, Nitish and Roth, Dan},
  year = {2023},
  month = feb,
  number = {arXiv:2112.07888},
  eprint = {2112.07888},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.07888},
  url = {http://arxiv.org/abs/2112.07888},
  urldate = {2023-03-02},
  abstract = {Comprehending an article requires understanding its constituent events. However, the context where an event is mentioned often lacks the details of this event. A question arises: how can the reader obtain more knowledge about this particular event in addition to what is provided by the local context in the article? This work defines Event Linking, a new natural language understanding task at the event level. Event linking tries to link an event mention appearing in an article to the most appropriate Wikipedia page. This page is expected to provide rich knowledge about what the event mention refers to. To standardize the research in this new direction, we contribute in four-fold. First, this is the first work in the community that formally defines Event Linking task. Second, we collect a dataset for this new task. Specifically, we automatically gather training set from Wikipedia, and then create two evaluation sets: one from the Wikipedia domain, reporting the in-domain performance, and a second from the real-world news domain, to evaluate out-of-domain performance. Third, we retrain and evaluate two state-of-the-art (SOTA) entity linking models, showing the challenges of event linking, and we propose an event-specific linking system EVELINK to set a competitive result for the new task. Fourth, we conduct a detailed and insightful analysis to help understand the task and the limitation of the current model. Overall, as our analysis shows, Event Linking is a considerably challenging and essential task requiring more effort from the community. Data and code are available here: https://github.com/CogComp/event-linking.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yu2023EventLinkingGrounding_Event Linking.pdf;/Users/ma/Zotero/storage/YA2LNHMQ/2112.html}
}

@misc{Yue2023ZeroFewShotEvent,
  title = {Zero- and {{Few-Shot Event Detection}} via {{Prompt-Based Meta Learning}}},
  author = {Yue, Zhenrui and Zeng, Huimin and Lan, Mengfei and Ji, Heng and Wang, Dong},
  year = {2023},
  month = may,
  number = {arXiv:2305.17373},
  eprint = {2305.17373},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.17373},
  url = {http://arxiv.org/abs/2305.17373},
  urldate = {2023-06-12},
  abstract = {With emerging online topics as a source for numerous new events, detecting unseen / rare event types presents an elusive challenge for existing event detection methods, where only limited data access is provided for training. To address the data scarcity problem in event detection, we propose MetaEvent, a meta learning-based framework for zero- and few-shot event detection. Specifically, we sample training tasks from existing event types and perform meta training to search for optimal parameters that quickly adapt to unseen tasks. In our framework, we propose to use the cloze-based prompt and a trigger-aware soft verbalizer to efficiently project output to unseen event types. Moreover, we design a contrastive meta objective based on maximum mean discrepancy (MMD) to learn class-separating features. As such, the proposed MetaEvent can perform zero-shot event detection by mapping features to event types without any prior knowledge. In our experiments, we demonstrate the effectiveness of MetaEvent in both zero-shot and few-shot scenarios, where the proposed method achieves state-of-the-art performance in extensive experiments on benchmark datasets FewEvent and MAVEN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Yue2023ZeroFewShotEvent_Zero- and Few-Shot Event Detection via Prompt-Based Meta Learning.pdf;/Users/ma/Zotero/storage/V9F3QYKQ/2305.html}
}

@misc{Zayed2022DeepLearningHealthy,
  title = {Deep {{Learning}} on a {{Healthy Data Diet}}: {{Finding Important Examples}} for {{Fairness}}},
  shorttitle = {Deep {{Learning}} on a {{Healthy Data Diet}}},
  author = {Zayed, Abdelrahman and Parthasarathi, Prasanna and Mordido, Goncalo and Palangi, Hamid and Shabanian, Samira and Chandar, Sarath},
  year = {2022},
  month = nov,
  number = {arXiv:2211.11109},
  eprint = {2211.11109},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2211.11109},
  urldate = {2023-02-02},
  abstract = {Data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful for fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model's fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zayed2022DeepLearningHealthy_Deep Learning on a Healthy Data Diet.pdf;/Users/ma/Zotero/storage/VUB5ZBVB/2211.html}
}

@misc{Zeng2022AreTransformersEffective,
  title = {Are {{Transformers Effective}} for {{Time Series Forecasting}}?},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  year = {2022},
  month = aug,
  number = {arXiv:2205.13504},
  eprint = {2205.13504},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.13504},
  url = {http://arxiv.org/abs/2205.13504},
  urldate = {2023-11-01},
  abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \textbackslash emph\{permutation-invariant\} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \textbackslash url\{https://github.com/cure-lab/LTSF-Linear\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ma/Zotero/storage/7LRZVLGI/Zeng et al. - 2022 - Are Transformers Effective for Time Series Forecas.pdf;/Users/ma/Zotero/storage/VQ7UAQRC/2205.html}
}

@article{Zeng2022DeeplearningSystemBridging,
  title = {A Deep-Learning System Bridging Molecule Structure and Biomedical Text with Comprehension Comparable to Human Professionals},
  author = {Zeng, Zheni and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong},
  year = {2022},
  month = feb,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {862},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-28494-3},
  url = {https://www.nature.com/articles/s41467-022-28494-3},
  urldate = {2023-10-26},
  abstract = {To accelerate biomedical research process, deep-learning systems are developed to automatically acquire knowledge about molecule entities by reading large-scale biomedical data. Inspired by humans that learn deep molecule knowledge from versatile reading on both molecule structure and biomedical text information, we propose a knowledgeable machine reading system that bridges both types of information in a unified deep-learning framework for comprehensive biomedical research assistance. We solve the problem that existing machine reading models can only process different types of data separately, and thus achieve a comprehensive and thorough understanding of molecule entities. By grasping meta-knowledge in an~unsupervised~fashion within and across different information sources, our system can facilitate various real-world biomedical applications, including molecular property prediction, biomedical relation extraction and so on. Experimental results show that our system even surpasses human professionals in the capability of molecular property comprehension, and also reveal its promising potential in facilitating automatic drug discovery and documentation in the future.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Drug discovery,Machine learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zeng2022DeeplearningSystemBridging_A deep-learning system bridging molecule structure and biomedical text with.pdf}
}

@inproceedings{Zeng2022EAImprovingConsistency,
  title = {{{EA}}\$\^2\${{E}}: {{Improving Consistency}} with {{Event Awareness}} for {{Document-Level Argument Extraction}}},
  shorttitle = {{{EA}}\$\^2\${{E}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Zeng, Qi and Zhan, Qiusi and Ji, Heng},
  year = {2022},
  month = jul,
  pages = {2649--2655},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.202},
  url = {https://aclanthology.org/2022.findings-naacl.202},
  urldate = {2023-02-09},
  abstract = {Events are inter-related in documents. Motivated by the one-sense-per-discourse theory, we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events, which will further cause discrepancy for downstream applications such as event knowledge base population, question answering, and hypothesis generation. In this work, we formulate event argument consistency as the constraints from event-event relations under the document-level setting. To improve consistency we introduce the Event-Aware Argument Extraction (EA\$\^2\$E) model with augmented context for training and inference. Experiment results on WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA\$\^2\$E compared to baseline methods.},
  keywords = {dataset\_WikiEvents,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zeng2022EAImprovingConsistency_EA$^2$E2.pdf}
}

@misc{Zhan2023GLENGeneralPurposeEvent,
  title = {{{GLEN}}: {{General-Purpose Event Detection}} for {{Thousands}} of {{Types}}},
  shorttitle = {{{GLEN}}},
  author = {Zhan, Qiusi and Li, Sha and Conger, Kathryn and Palmer, Martha and Ji, Heng and Han, Jiawei},
  year = {2023},
  month = mar,
  number = {arXiv:2303.09093},
  eprint = {2303.09093},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.09093},
  url = {http://arxiv.org/abs/2303.09093},
  urldate = {2023-10-17},
  abstract = {The development of event extraction systems has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 3,465 different event types, making it over 20x larger in ontology than any current dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model specifically designed to handle the large ontology size and partial labels in GLEN. We show that our model exhibits superior performance (\textasciitilde 10\% F1 gain) compared to both conventional classification baselines and newer definition-based models. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhan2023GLENGeneralPurposeEvent_GLEN.pdf;/Users/ma/Zotero/storage/SZ9CRKQR/2303.html}
}

@inproceedings{Zhang2015ModelingIdeologyPredicting,
  title = {Modeling {{Ideology}} and {{Predicting Policy Change}} with {{Social Media}}: {{Case}} of {{Same-Sex Marriage}}},
  shorttitle = {Modeling {{Ideology}} and {{Predicting Policy Change}} with {{Social Media}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Amy X. and Counts, Scott},
  year = {2015},
  month = apr,
  pages = {2603--2612},
  publisher = {{ACM}},
  address = {{Seoul Republic of Korea}},
  doi = {10.1145/2702123.2702193},
  url = {https://dl.acm.org/doi/10.1145/2702123.2702193},
  urldate = {2023-05-22},
  abstract = {Social media has emerged as a prominent platform where people can express their feelings about social and political issues of our time. We study the many voices discussing an issue within a constituency and how they reflect ideology and may signal the outcome of important policy decisions. Focusing on the issue of same-sex marriage legalization, we examine almost 2 million public Twitter posts related to same-sex marriage in the U.S. states over the course of 4 years starting from 2011. Among other findings, we find evidence of moral culture wars between ideologies and show that constituencies that express higher levels of emotion and have fewer actively engaged participants often precede legalization efforts that fail. From our measures, we build statistical models to predict the outcome of potential policy changes, with our best model achieving 87\% accuracy. We also achieve accuracies of 70\%, comparable to public opinion surveys, many months before a policy decision. We discuss how these analyses can augment traditional political science techniques as well as assist activists and policy analysts in understanding discussions on important issues at a population scale.},
  isbn = {978-1-4503-3145-6},
  langid = {english},
  file = {/Users/ma/Zotero/storage/657T8TJ6/Zhang and Counts - 2015 - Modeling Ideology and Predicting Policy Change wit.pdf}
}

@inproceedings{Zhang2015ModelingIdeologyPredictinga,
  title = {Modeling {{Ideology}} and {{Predicting Policy Change}} with {{Social Media}}: {{Case}} of {{Same-Sex Marriage}}},
  shorttitle = {Modeling {{Ideology}} and {{Predicting Policy Change}} with {{Social Media}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Amy X. and Counts, Scott},
  year = {2015},
  month = apr,
  pages = {2603--2612},
  publisher = {{ACM}},
  address = {{Seoul Republic of Korea}},
  doi = {10.1145/2702123.2702193},
  url = {https://dl.acm.org/doi/10.1145/2702123.2702193},
  urldate = {2023-10-17},
  isbn = {978-1-4503-3145-6},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2015ModelingIdeologyPredictinga_Modeling Ideology and Predicting Policy Change with Social Media.pdf}
}

@inproceedings{Zhang2017PositionawareAttentionSupervised,
  title = {Position-Aware {{Attention}} and {{Supervised Data Improve Slot Filling}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
  year = {2017},
  month = sep,
  pages = {35--45},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1004},
  url = {https://aclanthology.org/D17-1004},
  urldate = {2023-08-15},
  abstract = {Organized relational knowledge in the form of ``knowledge graphs'' is important for many applications. However, the ability to populate knowledge bases with facts automatically extracted from documents has improved frustratingly slowly. This paper simultaneously addresses two issues that have held back prior work. We first propose an effective new model, which combines an LSTM sequence model with a form of entity position-aware attention that is better suited to relation extraction. Then we build TACRED, a large (119,474 examples) supervised relation extraction dataset obtained via crowdsourcing and targeted towards TAC KBP relations. The combination of better supervised data and a more appropriate high-capacity model enables much better relation extraction performance. When the model trained on this new dataset replaces the previous relation extraction component of the best TAC KBP 2015 slot filling system, its F1 score increases markedly from 22.2\% to 26.7\%.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2017PositionawareAttentionSupervised_Position-aware Attention and Supervised Data Improve Slot Filling.pdf}
}

@inproceedings{Zhang2019ExtractingEntitiesEvents,
  title = {Extracting {{Entities}} and {{Events}} as a {{Single Task Using}} a {{Transition-Based Neural Model}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Junchi and Qin, Yanxia and Zhang, Yue and Liu, Mengchi and Ji, Donghong},
  year = {2019},
  month = aug,
  pages = {5422--5428},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/753},
  url = {https://www.ijcai.org/proceedings/2019/753},
  urldate = {2023-02-17},
  abstract = {The task of event extraction contains subtasks including detections for entity mentions, event triggers and argument roles. Traditional methods solve them as a pipeline, which does not make use of task correlation for their mutual benefits. There have been recent efforts towards building a joint model for all tasks. However, due to technical challenges, there has not been work predicting the joint output structure as a single task. We build a first model to this end using a neural transition-based framework, incrementally predicting complex joint structures in a state-transition process. Results on standard benchmarks show the benefits of the joint model, which gives the best result in the literature.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/Users/ma/Zotero/storage/WWYYIUF4/Zhang et al. - 2019 - Extracting Entities and Events as a Single Task Us.pdf}
}

@article{Zhang2020DiagnosticPredictionSequenceofsets,
  title = {Diagnostic {{Prediction}} with {{Sequence-of-sets Representation Learning}} for {{Clinical Events}}},
  author = {Zhang, Tianran and Chen, Muhao and Bui, Alex A. T.},
  year = {2020},
  month = aug,
  journal = {Artificial intelligence in medicine. Conference on Artificial Intelligence in Medicine (2005-)},
  volume = {12299},
  pages = {348--358},
  doi = {10.1007/978-3-030-59137-3_31},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8143801/},
  urldate = {2023-11-02},
  abstract = {Electronic health records (EHRs) contain both ordered and unordered chronologies of clinical events that occur during a patient encounter. However, during data preprocessing steps, many predictive models impose a predefined order on unordered clinical events sets (e.g., alphabetical, natural order from the chart, etc.), which is potentially incompatible with the temporal nature of the sequence and predictive task. To address this issue, we propose DPSS, which seeks to capture each patient's clinical event records as sequences of event sets. For each clinical event set, we assume that the predictive model should be invariant to the order of concurrent events and thus employ a novel permutation sampling mechanism. This paper evaluates the use of this permuted sampling method given different data-driven models for predicting a heart failure (HF) diagnosis in subsequent patient visits. Experimental results using the MIMIC-III dataset show that the permutation sampling mechanism offers improved discriminative power based on the area under the receiver operating curve (AUROC) and precision-recall curve (pr-AUC) metrics as HF diagnosis prediction becomes more robust to different data ordering schemes.},
  pmcid = {PMC8143801},
  pmid = {34036298},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2020DiagnosticPredictionSequenceofsets_Diagnostic Prediction with Sequence-of-sets Representation Learning for.pdf}
}

@inproceedings{Zhang2020HierarchicalAttentionPropagation,
  title = {Hierarchical {{Attention Propagation}} for {{Healthcare Representation Learning}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zhang, Muhan and King, Christopher R. and Avidan, Michael and Chen, Yixin},
  year = {2020},
  month = aug,
  pages = {249--256},
  publisher = {{ACM}},
  address = {{Virtual Event CA USA}},
  doi = {10.1145/3394486.3403067},
  url = {https://dl.acm.org/doi/10.1145/3394486.3403067},
  urldate = {2023-11-02},
  isbn = {978-1-4503-7998-4},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2020HierarchicalAttentionPropagation_Hierarchical Attention Propagation for Healthcare Representation Learning.pdf}
}

@inproceedings{Zhang2020TwoStepApproachImplicit,
  title = {A {{Two-Step Approach}} for {{Implicit Event Argument Detection}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zhang, Zhisong and Kong, Xiang and Liu, Zhengzhong and Ma, Xuezhe and Hovy, Eduard},
  year = {2020},
  month = jul,
  pages = {7479--7485},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.667},
  url = {https://aclanthology.org/2020.acl-main.667},
  urldate = {2023-03-03},
  abstract = {In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries. The addition of cross-sentence argument candidates imposes great challenges for modeling. To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion. Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline. We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements. It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.},
  keywords = {dataset\_RAMS,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2020TwoStepApproachImplicit_A Two-Step Approach for Implicit Event Argument Detection.pdf}
}

@article{Zhang2021TaxonomyCompletionTriplet,
  title = {Taxonomy {{Completion}} via {{Triplet Matching Network}}},
  author = {Zhang, Jieyu and Song, Xiangchen and Zeng, Ying and Chen, Jiaze and Shen, Jiaming and Mao, Yuning and Li, Lei},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {5},
  pages = {4662--4670},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i5.16596},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16596},
  urldate = {2023-04-29},
  abstract = {Automatically constructing taxonomy finds many applications in e-commerce and web search. One critical challenge is as data and business scope grow in real applications, new concepts are emerging and needed to be added to the existing taxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an appropriate hypernym concept from the taxonomy for a new query concept. In this paper, we formulate a new task, ``taxonomy completion'', by discovering both the hypernym and hyponym concepts for a query. We propose Triplet Matching Network (TMN), to find the appropriate  pairs for a given query concept. TMN consists of one primal scorer and multiple auxiliary scorers. These auxiliary scorers capture various fine-grained signals (e.g., query to hypernym or query to hyponym semantics), and the primal scorer makes a holistic prediction on  triplet based on the internal feature representations of all auxiliary scorers. Also, an innovative channel-wise gating mechanism that retains task-specific information in concept representations is introduced to further boost model performance. Experiments on four real-world large-scale datasets show that TMN achieves the best performance on both taxonomy completion task and the previous taxonomy expansion task, outperforming existing methods.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Information Extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2021TaxonomyCompletionTriplet_Taxonomy Completion via Triplet Matching Network.pdf}
}

@inproceedings{Zhang2021ZeroshotLabelAwareEvent,
  title = {Zero-Shot {{Label-Aware Event Trigger}} and {{Argument Classification}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Zhang, Hongming and Wang, Haoyu and Roth, Dan},
  year = {2021},
  month = aug,
  pages = {1331--1340},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.114},
  url = {https://aclanthology.org/2021.findings-acl.114},
  urldate = {2023-05-23},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2021ZeroshotLabelAwareEvent_Zero-shot Label-Aware Event Trigger and Argument Classification.pdf}
}

@inproceedings{Zhang2022DeBiasGenerativeExtraction,
  title = {De-{{Bias}} for {{Generative Extraction}} in {{Unified NER Task}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhang, Shuai and Shen, Yongliang and Tan, Zeqi and Wu, Yiquan and Lu, Weiming},
  year = {2022},
  month = may,
  pages = {808--818},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.59},
  url = {https://aclanthology.org/2022.acl-long.59},
  urldate = {2023-05-01},
  abstract = {Named entity recognition (NER) is a fundamental task to recognize specific types of entities from a given sentence. Depending on how the entities appear in the sentence, it can be divided into three subtasks, namely, Flat NER, Nested NER, and Discontinuous NER. Among the existing approaches, only the generative model can be uniformly adapted to these three subtasks. However, when the generative model is applied to NER, its optimization objective is not consistent with the task, which makes the model vulnerable to the incorrect biases. In this paper, we analyze the incorrect biases in the generation process from a causality perspective and attribute them to two confounders: pre-context confounder and entity-order confounder. Furthermore, we design Intra- and Inter-entity Deconfounding Data Augmentation methods to eliminate the above confounders according to the theory of backdoor adjustment. Experiments show that our method can improve the performance of the generative NER model in various datasets.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2022DeBiasGenerativeExtraction_De-Bias for Generative Extraction in Unified NER Task.pdf}
}

@inproceedings{Zhang2022TransferLearningSemantic,
  title = {Transfer {{Learning}} from {{Semantic Role Labeling}} to {{Event Argument Extraction}} with {{Template-based Slot Querying}}},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Zhisong and Strubell, Emma and Hovy, Eduard},
  year = {2022},
  month = dec,
  pages = {2627--2647},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.169},
  urldate = {2023-02-09},
  abstract = {In this work, we investigate transfer learning from semantic role labeling (SRL) to event argument extraction (EAE), considering their similar argument structures. We view the extraction task as a role querying problem, unifying various methods into a single framework. There are key discrepancies on role labels and distant arguments between semantic role and event argument annotations. To mitigate these discrepancies, we specify natural language-like queries to tackle the label mismatch problem and devise argument augmentation to recover distant arguments. We show that SRL annotations can serve as a valuable resource for EAE, and a template-based slot querying strategy is especially effective for facilitating the transfer. In extensive evaluations on two English EAE benchmarks, our proposed model obtains impressive zero-shot results by leveraging SRL annotations, reaching nearly 80\% of the fullysupervised scores. It further provides benefits in low-resource cases, where few EAE annotations are available. Moreover, we show that our approach generalizes to cross-domain and multilingual scenarios.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2022TransferLearningSemantic_Transfer Learning from Semantic Role Labeling to Event Argument Extraction with.pdf}
}

@misc{Zhang2023CausalReasoningEntities,
  title = {Causal {{Reasoning}} of {{Entities}} and {{Events}} in {{Procedural Texts}}},
  author = {Zhang, Li and Xu, Hainiu and Yang, Yue and Zhou, Shuyan and You, Weiqiu and Arora, Manni and {Callison-Burch}, Chris},
  year = {2023},
  month = feb,
  number = {arXiv:2301.10896},
  eprint = {2301.10896},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2301.10896},
  urldate = {2023-10-17},
  abstract = {Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhang2023CausalReasoningEntities_Causal Reasoning of Entities and Events in Procedural Texts.pdf;/Users/ma/Zotero/storage/B884GS4C/2301.html}
}

@misc{Zhang2023HowLanguageModel,
  title = {How {{Language Model Hallucinations Can Snowball}}},
  author = {Zhang, Muru and Press, Ofir and Merrill, William and Liu, Alisa and Smith, Noah A.},
  year = {2023},
  month = may,
  number = {arXiv:2305.13534},
  eprint = {2305.13534},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2305.13534},
  urldate = {2023-05-24},
  abstract = {A major risk of using language models in practical applications is their tendency to hallucinate incorrect statements. Hallucinations are often attributed to knowledge gaps in LMs, but we hypothesize that in some cases, when justifying previously generated hallucinations, LMs output false claims that they can separately recognize as incorrect. We construct three question-answering datasets where ChatGPT and GPT-4 often state an incorrect answer and offer an explanation with at least one incorrect claim. Crucially, we find that ChatGPT and GPT-4 can identify 67\% and 87\% of their own mistakes, respectively. We refer to this phenomenon as hallucination snowballing: an LM over-commits to early mistakes, leading to more mistakes that it otherwise would not make.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/A9DM5ZHP/Zhang et al. - 2023 - How Language Model Hallucinations Can Snowball.pdf}
}

@inproceedings{Zhao2021CalibrateUseImproving,
  title = {Calibrate {{Before Use}}: {{Improving Few-shot Performance}} of {{Language Models}}},
  shorttitle = {Calibrate {{Before Use}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  year = {2021},
  month = jul,
  pages = {12697--12706},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/zhao21c.html},
  urldate = {2023-03-16},
  abstract = {GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given a training prompt and a content-free test input such as "N/A". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's accuracy (up to 30.0\% absolute) across different choices of the prompt, while also making learning considerably more stable.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhao2021CalibrateUseImproving_Calibrate Before Use.pdf;/Users/ma/Zotero/storage/3U3HET8X/Zhao et al. - 2021 - Calibrate Before Use Improving Few-shot Performan.pdf}
}

@misc{Zhao2022DistillationResistantWatermarkingModel,
  title = {Distillation-{{Resistant Watermarking}} for {{Model Protection}} in {{NLP}}},
  author = {Zhao, Xuandong and Li, Lei and Wang, Yu-Xiang},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03312},
  eprint = {2210.03312},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03312},
  url = {http://arxiv.org/abs/2210.03312},
  urldate = {2023-08-25},
  abstract = {How can we protect the intellectual property of trained NLP models? Modern NLP models are prone to stealing by querying and distilling from their publicly exposed APIs. However, existing protection methods such as watermarking only work for images but are not applicable to text. We propose Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP models from being stolen via distillation. DRW protects a model by injecting watermarks into the victim's prediction probability corresponding to a secret key and is able to detect such a key by probing a suspect model. We prove that a protected model still retains the original accuracy within a certain bound. We evaluate DRW on a diverse set of NLP tasks including text classification, part-of-speech tagging, and named entity recognition. Experiments show that DRW protects the original model and detects stealing suspects at 100\% mean average precision for all four tasks while the prior method fails on two.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhao2022DistillationResistantWatermarkingModel_Distillation-Resistant Watermarking for Model Protection in NLP.pdf;/Users/ma/Zotero/storage/SFNLYC7H/2210.html}
}

@misc{Zhao2023GraphTextGraphReasoning,
  title = {{{GraphText}}: {{Graph Reasoning}} in {{Text Space}}},
  shorttitle = {{{GraphText}}},
  author = {Zhao, Jianan and Zhuo, Le and Shen, Yikang and Qu, Meng and Liu, Kai and Bronstein, Michael and Zhu, Zhaocheng and Tang, Jian},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01089},
  eprint = {2310.01089},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.01089},
  url = {http://arxiv.org/abs/2310.01089},
  urldate = {2023-10-25},
  abstract = {Large Language Models (LLMs) have gained the ability to assimilate human knowledge and facilitate natural language interactions with both humans and other LLMs. However, despite their impressive achievements, LLMs have not made significant advancements in the realm of graph machine learning. This limitation arises because graphs encapsulate distinct relational data, making it challenging to transform them into natural language that LLMs understand. In this paper, we bridge this gap with a novel framework, GraphText, that translates graphs into natural language. GraphText derives a graph-syntax tree for each graph that encapsulates both the node attributes and inter-node relationships. Traversal of the tree yields a graph text sequence, which is then processed by an LLM to treat graph tasks as text generation tasks. Notably, GraphText offers multiple advantages. It introduces training-free graph reasoning: even without training on graph data, GraphText with ChatGPT can achieve on par with, or even surpassing, the performance of supervised-trained graph neural networks through in-context learning (ICL). Furthermore, GraphText paves the way for interactive graph reasoning, allowing both humans and LLMs to communicate with the model seamlessly using natural language. These capabilities underscore the vast, yet-to-be-explored potential of LLMs in the domain of graph machine learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ma/Zotero/storage/Z6XEIDXQ/Zhao et al. - 2023 - GraphText Graph Reasoning in Text Space.pdf;/Users/ma/Zotero/storage/Y35XSSB6/2310.html}
}

@misc{Zhao2023ProtectingLanguageGeneration,
  title = {Protecting {{Language Generation Models}} via {{Invisible Watermarking}}},
  author = {Zhao, Xuandong and Wang, Yu-Xiang and Li, Lei},
  year = {2023},
  month = aug,
  number = {arXiv:2302.03162},
  eprint = {2302.03162},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.03162},
  url = {http://arxiv.org/abs/2302.03162},
  urldate = {2023-09-15},
  abstract = {Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhao2023ProtectingLanguageGeneration_Protecting Language Generation Models via Invisible Watermarking.pdf;/Users/ma/Zotero/storage/3ILT2P52/2302.html}
}

@article{ZhaoPROMPTTRIGGERSBACKDOOR,
  title = {{{PROMPT AS TRIGGERS FOR BACKDOOR ATTACK}}: {{EXAMINING THE VULNERABILITY IN LANGUAGE MODELS}}},
  author = {Zhao, Shuai and Wen, Jinming and Tuan, Luu Anh and Zhao, Junbo and Fu, Jie},
  abstract = {The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose ProAttack, a novel and efficient method for performing cleanlabel backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text classification tasks, we empirically validate ProAttack's competitive performance in textual backdoor attacks. Notably, in the rich-resource setting, ProAttack achieves state-of-the-art attack success rates in the clean-label backdoor attack benchmark without external triggers. All data and code used in our models are publically available2.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/7LTS8VHS/Zhao et al. - PROMPT AS TRIGGERS FOR BACKDOOR ATTACK EXAMINING .pdf}
}

@article{ZhaoProvableRobustWatermarking,
  title = {Provable {{Robust Watermarking}} for {{AI-Generated Text}}},
  author = {Zhao, Xuandong and Ananth, Prabhanjan and Li, Lei and Wang, Yu-Xiang},
  abstract = {As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at https://github. com/XuandongZhao/GPTWatermark.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/AEZ2G4TK/Zhao et al. - Provable Robust Watermarking for AI-Generated Text.pdf}
}

@inproceedings{Zheng2017JointExtractionEntities,
  title = {Joint {{Extraction}} of {{Entities}} and {{Relations Based}} on a {{Novel Tagging Scheme}}},
  booktitle = {Proceedings of the 55th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zheng, Suncong and Wang, Feng and Bao, Hongyun and Hao, Yuexing and Zhou, Peng and Xu, Bo},
  year = {2017},
  month = jul,
  pages = {1227--1236},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1113},
  url = {https://aclanthology.org/P17-1113},
  urldate = {2023-05-01},
  abstract = {Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What's more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zheng2017JointExtractionEntities_Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme.pdf}
}

@inproceedings{Zheng2019Doc2EDAGEndtoEndDocumentlevel,
  title = {{{Doc2EDAG}}: {{An End-to-End Document-level Framework}} for {{Chinese Financial Event Extraction}}},
  shorttitle = {{{Doc2EDAG}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Zheng, Shun and Cao, Wei and Xu, Wei and Bian, Jiang},
  year = {2019},
  month = nov,
  pages = {337--346},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1032},
  url = {https://aclanthology.org/D19-1032},
  urldate = {2023-03-03},
  abstract = {Most existing event extraction (EE) methods merely extract event arguments within the sentence scope. However, such sentence-level EE methods struggle to handle soaring amounts of documents from emerging applications, such as finance, legislation, health, etc., where event arguments always scatter across different sentences, and even multiple such event mentions frequently co-exist in the same document. To address these challenges, we propose a novel end-to-end model, Doc2EDAG, which can generate an entity-based directed acyclic graph to fulfill the document-level EE (DEE) effectively. Moreover, we reformalize a DEE task with the no-trigger-words design to ease the document-level event labeling. To demonstrate the effectiveness of Doc2EDAG, we build a large-scale real-world dataset consisting of Chinese financial announcements with the challenges mentioned above. Extensive experiments with comprehensive analyses illustrate the superiority of Doc2EDAG over state-of-the-art methods. Data and codes can be found at https://github.com/dolphin-zs/Doc2EDAG.},
  keywords = {dataset\_Doc2EDAG,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zheng2019Doc2EDAGEndtoEndDocumentlevel_Doc2EDAG.pdf}
}

@misc{Zhong2023MedDiffusionBoostingHealth,
  title = {{{MedDiffusion}}: {{Boosting Health Risk Prediction}} via {{Diffusion-based Data Augmentation}}},
  shorttitle = {{{MedDiffusion}}},
  author = {Zhong, Yuan and Cui, Suhan and Wang, Jiaqi and Wang, Xiaochen and Yin, Ziyi and Wang, Yaqing and Xiao, Houping and Huai, Mengdi and Wang, Ting and Ma, Fenglong},
  year = {2023},
  month = oct,
  number = {arXiv:2310.02520},
  eprint = {2310.02520},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2310.02520},
  urldate = {2023-10-31},
  abstract = {Health risk prediction is one of the fundamental tasks under predictive modeling in the medical domain, which aims to forecast the potential health risks that patients may face in the future using their historical Electronic Health Records (EHR). Researchers have developed several risk prediction models to handle the unique challenges of EHR data, such as its sequential nature, high dimensionality, and inherent noise. These models have yielded impressive results. Nonetheless, a key issue undermining their effectiveness is data insufficiency. A variety of data generation and augmentation methods have been introduced to mitigate this issue by expanding the size of the training data set through the learning of underlying data distributions. However, the performance of these methods is often limited due to their task-unrelated design. To address these shortcomings, this paper introduces a novel, end-to-end diffusion-based risk prediction model, named MedDiffusion. It enhances risk prediction performance by creating synthetic patient data during training to enlarge sample space. Furthermore, MedDiffusion discerns hidden relationships between patient visits using a step-wise attention mechanism, enabling the model to automatically retain the most vital information for generating high-quality data. Experimental evaluation on four real-world medical datasets demonstrates that MedDiffusion outperforms 14 cutting-edge baselines in terms of PR-AUC, F1, and Cohen's Kappa. We also conduct ablation studies and benchmark our model against GAN-based alternatives to further validate the rationality and adaptability of our model design. Additionally, we analyze generated data to offer fresh insights into the model's interpretability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhong2023MedDiffusionBoostingHealth_MedDiffusion.pdf;/Users/ma/Zotero/storage/8CKN26GX/2310.html}
}

@article{Zhou2020MutationEffectEstimation,
  title = {Mutation Effect Estimation on Protein\textendash Protein Interactions Using Deep Contextualized Representation Learning},
  author = {Zhou, Guangyu and Chen, Muhao and Ju, Chelsea J T and Wang, Zheng and Jiang, Jyun-Yu and Wang, Wei},
  year = {2020},
  month = jun,
  journal = {NAR Genomics and Bioinformatics},
  volume = {2},
  number = {2},
  pages = {lqaa015},
  issn = {2631-9268},
  doi = {10.1093/nargab/lqaa015},
  url = {https://doi.org/10.1093/nargab/lqaa015},
  urldate = {2023-08-04},
  abstract = {The functional impact of protein mutations is reflected on the alteration of conformation and thermodynamics of protein\textendash protein interactions (PPIs). Quantifying the changes of two interacting proteins upon mutations is commonly carried out by computational approaches. Hence, extensive research efforts have been put to the extraction of energetic or structural features on proteins, followed by statistical learning methods to estimate the effects of mutations on PPI properties. Nonetheless, such features require extensive human labors and expert knowledge to obtain, and have limited abilities to reflect point mutations. We present an end-to-end deep learning framework, MuPIPR~(Mutation Effects in Protein\textendash protein Interaction PRediction Using Contextualized Representations), to estimate the effects of mutations on PPIs. MuPIPR incorporates a contextualized representation mechanism of amino acids to propagate the effects of a point mutation to surrounding amino acid representations, therefore amplifying the subtle change in a long protein sequence. On top of that, MuPIPR leverages a Siamese residual recurrent convolutional neural encoder to encode a wild-type protein pair and its mutation pair. Multi-layer perceptron regressors are applied to the protein pair representations to predict the quantifiable changes of PPI properties upon mutations. Experimental evaluations show that, with only sequence information, MuPIPR outperforms various state-of-the-art systems on estimating the changes of binding affinity for SKEMPI v1, and offers comparable performance on SKEMPI v2. Meanwhile, MuPIPR also demonstrates state-of-the-art performance on estimating the changes of buried surface areas. The software implementation is available at https://github.com/guangyu-zhou/MuPIPR.},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhou2020MutationEffectEstimation_Mutation effect estimation on protein–protein interactions using deep.pdf;/Users/ma/Zotero/storage/PDWM7WS3/5781175.html}
}

@article{Zhou2021InformerEfficientTransformer,
  title = {Informer: {{Beyond Efficient Transformer}} for {{Long Sequence Time-Series Forecasting}}},
  shorttitle = {Informer},
  author = {Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {12},
  pages = {11106--11115},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i12.17325},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17325},
  urldate = {2023-11-01},
  abstract = {Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Energy,Environment \& Sustainability},
  file = {/Users/ma/Zotero/storage/I92G53UB/Zhou et al. - 2021 - Informer Beyond Efficient Transformer for Long Se.pdf}
}

@inproceedings{Zhou2022DocumentLevelEventArgument,
  title = {Document-{{Level Event Argument Extraction}} by {{Leveraging Redundant Information}} and {{Closed Boundary Loss}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zhou, Hanzhang and Mao, Kezhi},
  year = {2022},
  month = jul,
  pages = {3041--3052},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.naacl-main.222},
  url = {https://aclanthology.org/2022.naacl-main.222},
  urldate = {2023-02-09},
  abstract = {In document-level event argument extraction, an argument is likely to appear multiple times in different expressions in the document. The redundancy of arguments underlying multiple sentences is beneficial but is often overlooked. In addition, in event argument extraction, most entities are regarded as class ``others'', i.e. Universum class, which is defined as a collection of samples that do not belong to any class of interest. Universum class is composed of heterogeneous entities without typical common features. Classifiers trained by cross entropy loss could easily misclassify the Universum class because of their open decision boundary. In this paper, to make use of redundant event information underlying a document, we build an entity coreference graph with the graph2token module to produce a comprehensive and coreference-aware representation for every entity and then build an entity summary graph to merge the multiple extraction results. To better classify Universum class, we propose a new loss function to build classifiers with closed boundaries. Experimental results show that our model outperforms the previous state-of-the-art models by 3.35\% in F1-score.},
  keywords = {dataset\_MUC4,document-level event extraction},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhou2022DocumentLevelEventArgument_Document-Level Event Argument Extraction by Leveraging Redundant Information.pdf}
}

@misc{Zhou2023LIMALessMore,
  title = {{{LIMA}}: {{Less Is More}} for {{Alignment}}},
  shorttitle = {{{LIMA}}},
  author = {Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer},
  year = {2023},
  month = may,
  journal = {arXiv.org},
  url = {https://arxiv.org/abs/2305.11206v1},
  urldate = {2023-05-24},
  abstract = {Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43\% of cases; this statistic is as high as 58\% when compared to Bard and 65\% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.},
  langid = {english},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhou2023LIMALessMore_LIMA.pdf}
}

@misc{Zhou2023UniversalNERTargetedDistillation,
  title = {{{UniversalNER}}: {{Targeted Distillation}} from {{Large Language Models}} for {{Open Named Entity Recognition}}},
  shorttitle = {{{UniversalNER}}},
  author = {Zhou, Wenxuan and Zhang, Sheng and Gu, Yu and Chen, Muhao and Poon, Hoifung},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03279},
  eprint = {2308.03279},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2308.03279},
  urldate = {2023-10-20},
  abstract = {Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We will release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhou2023UniversalNERTargetedDistillation_UniversalNER.pdf;/Users/ma/Zotero/storage/AF6HLRS9/2308.html}
}

@inproceedings{Zhu2022EfficientDocumentlevelEvent,
  title = {Efficient {{Document-level Event Extraction}} via {{Pseudo-Trigger-aware Pruned Complete Graph}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Zhu, Tong and Qu, Xiaoye and Chen, Wenliang and Wang, Zhefeng and Huai, Baoxing and Yuan, Nicholas and Zhang, Min},
  year = {2022},
  month = jul,
  pages = {4552--4558},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Vienna, Austria}},
  doi = {10.24963/ijcai.2022/632},
  url = {https://www.ijcai.org/proceedings/2022/632},
  urldate = {2023-02-20},
  abstract = {Most previous studies of document-level event extraction mainly focus on building argument chains in an autoregressive way, which achieves a certain success but is inefficient in both training and inference. In contrast to the previous studies, we propose a fast and lightweight model named as PTPCG. In our model, we design a novel strategy for event argument combination together with a non-autoregressive decoding algorithm via pruned complete graphs, which are constructed under the guidance of the automatically selected pseudo triggers. Compared to the previous systems, our system achieves competitive results with 19.8\% of parameters and much lower resource consumption, taking only 3.8\% GPU hours for training and up to 8.5 times faster for inference. Besides, our model shows superior compatibility for the datasets with (or without) triggers and the pseudo triggers can be the supplements for annotated triggers to make further improvements. Codes are available at https: //github.com/Spico197/DocEE .},
  isbn = {978-1-956792-00-3},
  langid = {english},
  keywords = {dataset\_Doc2EDAG,document-level event extraction},
  file = {/Users/ma/Zotero/storage/MNV9XYSJ/Zhu et al. - 2022 - Efficient Document-level Event Extraction via Pseu.pdf}
}

@misc{Zhu2023DescriptiveKnowledgeGraph,
  title = {Descriptive {{Knowledge Graph}} in {{Biomedical Domain}}},
  author = {Zhu, Kerui and Huang, Jie and Chang, Kevin Chen-Chuan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.11681},
  eprint = {2310.11681},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.11681},
  url = {http://arxiv.org/abs/2310.11681},
  urldate = {2023-10-25},
  abstract = {We present a novel system that automatically extracts and generates informative and descriptive sentences from the biomedical corpus and facilitates the efficient search for relational knowledge. Unlike previous search engines or exploration systems that retrieve unconnected passages, our system organizes descriptive sentences as a relational graph, enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease). Our system also uses ChatGPT and a fine-tuned relation synthesis model to generate concise and reliable descriptive sentences from retrieved information, reducing the need for extensive human reading effort. With our system, researchers can easily obtain both high-level knowledge and detailed references and interactively steer to the information of interest. We spotlight the application of our system in COVID-19 research, illustrating its utility in areas such as drug repurposing and literature curation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Zotero/storage/F5VX694H/Zhu et al. - 2023 - Descriptive Knowledge Graph in Biomedical Domain.pdf;/Users/ma/Zotero/storage/47IVCYDG/2310.html}
}

@misc{Zhu2023LargeLanguageModels,
  title = {Large {{Language Models}} Can {{Learn Rules}}},
  author = {Zhu, Zhaocheng and Xue, Yuan and Chen, Xinyun and Zhou, Denny and Tang, Jian and Schuurmans, Dale and Dai, Hanjun},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07064},
  eprint = {2310.07064},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07064},
  url = {http://arxiv.org/abs/2310.07064},
  urldate = {2023-11-02},
  abstract = {When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an absolute gain of 11-27\% in accuracy. The learned rules are also transferable to different models and to different forms of the same problem.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ma/Drive_ma/Papers_Zotero/Zhu2023LargeLanguageModels_Large Language Models can Learn Rules.pdf;/Users/ma/Zotero/storage/WPYHVCFM/2310.html}
}

@article{ZiemsCanLargeLanguage,
  title = {Can {{Large Language Models Transform Computational Social Science}}?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Zhang, Zhehao and Yang, Diyi and Chen, Jiaao},
  abstract = {Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zeroshot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the hidden meaning behind text). In summary, LLMs can significantly reduce costs and increase efficiency of social science analysis in partnership with humans.},
  langid = {english},
  file = {/Users/ma/Zotero/storage/QHJPP9KW/Ziems et al. - Can Large Language Models Transform Computational .pdf}
}
