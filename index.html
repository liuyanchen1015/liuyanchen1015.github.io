<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Yanchen Liu</title>
    <meta name="author" content="Yanchen  Liu" />
    <meta name="description" content="Yanchen Liu Homepage. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
    <meta name="keywords" content="刘延晨, Yanchen Liu, yanchen Liu, Yanchen liu, yanchen liu, Yanchen, 慕尼黑工业大学，TUM, Technical University of Munich, Technische Universität München, 慕尼黑大学, LMU, Ludwig Maximilian University of Munich, 清华大学, 清华, THU, Tsinghua University, 麻省理工学院, 麻省理工, MIT, Massachusetts Institute of Technology, 哈佛大学, Harvard, Harvard University, 斯坦福大学, Stanford, Stanford University, 洛阳，北京, 慕尼黑, 波士顿, 剑桥, Luoyang, Peking, Munich, Boston, Cambridge" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/ico.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://liuyanchen1015.github.io/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/YanchenLiu_CV.pdf">cv</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Yanchen Liu
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-left">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/me-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/me-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/me-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/me.png" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="me.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              <p align="left"><font size="2">Cambridge, MA, USA 02139<br>Schwarzman College of Computing, MIT</font></p>
            </div>
          </div>

          <div class="clearfix">
            <p>Hi all! I am a 1st year PhD student at <a href="https://www.mit.edu/" target="_blank" rel="noopener noreferrer">MIT</a> <img src="assets/img/MIT.png" alt="mit" height="14px">.</p>

<p>Previously, I obtained my Master’s degree from <a href="https://www.harvard.edu/" target="_blank" rel="noopener noreferrer">Harvard University</a> <img src="assets/img/h.png" alt="h" height="20px">, working with Prof. 
	<a href="https://himalakkaraju.github.io/" target="_blank" rel="noopener noreferrer">Hima Lakkaraju</a> and my Bachelor’s degree in Computer Science from 
	<a href="https://www.tum.de/en/" target="_blank" rel="noopener noreferrer">TUM</a> <img src="assets/img/TUM.png" alt="tum" height="13px"> , with a minor in Computational Linguistics at 
	<a href="https://www.lmu.de/en/" target="_blank" rel="noopener noreferrer">LMU</a> <img src="assets/img/LMU.jpeg" alt="lmu" height="18px"> , supervised by Prof. 
	<a href="https://scholar.google.com/citations?user=qIL9dWUAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Hinrich Schütze</a>. Also, I had the chance to visit the 
	<a href="https://nlp.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford NLP Group</a> <img src="assets/img/Stanford.png" alt="s" height="19px"> , collaborating with Prof.
	<a href="https://cs.stanford.edu/~diyiy/index.html" target="_blank" rel="noopener noreferrer">Diyi Yang</a> for one year.</p>

<!-- My research interests mainly lie in `LLM safety`, including robustness <a href="https://arxiv.org/abs/2305.13406">[1]</a><a href="https://arxiv.org/abs/2311.00915">[2]</a>, fairness <a href="https://arxiv.org/abs/2310.14607">[3]</a>, privacy <a href="https://arxiv.org/abs/2409.00138">[4]</a>, misinformation <a href="https://arxiv.org/abs/2311.09630">[5]</a>, alignment, jailbreaking <a href="">[6]</a>, and, moreover, `safety of LM agents` <a href="https://arxiv.org/abs/2409.00138">[4]</a>. Technically, I have worked on `parameter-efficient adaptation/composition`<a href="https://arxiv.org/abs/2305.13406">[1]</a><a href="https://arxiv.org/abs/2311.00915">[2]</a><a href="https://arxiv.org/abs/2302.14413">[7]</a>  and `ICL`<a href="https://arxiv.org/abs/2310.14607">[3]</a><a href="https://arxiv.org/abs/2202.06133">[8]</a>. -->

<p>My current research interests mainly lie in <code class="language-plaintext highlighter-rouge">LLM safety</code> (including <code class="language-plaintext highlighter-rouge">safety of LM agents</code> <a href="https://arxiv.org/abs/2409.00138" target="_blank" rel="noopener noreferrer">[1]</a>), <code class="language-plaintext highlighter-rouge">alignment</code>, and <code class="language-plaintext highlighter-rouge">scalable oversight</code>.</p>

          </div>
      
          <!-- News -->
          <br>          
          <div class="news">
            <h2>news</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless">
               
                <tr>
                  <th scope="row">Sep 23, 2025</th>
                  <td>
                    I have started my PhD at MIT! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">  <img src="/assets/img/blob_excited.gif" alt="drawing" width="19">

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Sep 20, 2024</th>
                  <td>
                    One paper is accepted to EMNLP 2024! See you in Miami! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> 🎉

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Nov 16, 2023</th>
                  <td>
                    Greatly honored to give a talk on my work, <a href="assets/pdf/DADA.pdf">Dynamic Aggregation &amp; Auto-Discovery of Linguistic Features</a>, within the <a href="https://nlp.stanford.edu/" target="_blank" rel="noopener noreferrer">Stanford NLP Group</a>. <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">  <img src="/assets/img/pikachu_wave.gif" alt="pikachu waving" width="19">
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>


          <div style="margin-top: -100px;">
            <p></p>
          </div>   
                         
<!--           <div class="logo">
          
           <p> <center>
                  <a href="https://www.harvard.edu/" target="_blank">
                        <img src="./assets/img/h.png" height="90px" style="margin-right:18px">
                  </a>
                  <a href="https://www.mit.edu/" target="_blank">
                        <img src="./assets/img/MIT.png" height="85px" style="margin-right:18px">
                  </a>
                  <a href="https://www.stanford.edu/" target="_blank">
                        <img src="./assets/img/S.png" height="90px" style="margin-right:18px">
                  </a>
                  <a href="https://www.lmu.de/en/" target="_blank">
                        <img src="./assets/img/LMU.jpeg" height="90px" style="margin-right:18px">
                  </a>
                  <a href="https://www.tum.de/en/" target="_blank">
                        <img src="./assets/img/TUM.png" height="85px" style="margin-right:18px">
                  </a>
                  <a href="https://www.tsinghua.edu.cn/en/" target="_blank">
                        <img src="./assets/img/tsinghua.png" height="90px">
                  </a>
                 <a href="https://ir.baidu.com/" target="_blank">
                        <img src="./assets/img/baidu.png" height="90px">
                  </a>
           </center> </p>
          </div> -->
              
              
              
              
              
              
              
          
          <!-- Selected papers -->
            <br><br><br>
          <div class="publications">
            <h2>selected publications</h2>
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/privacy_lens.png"></div>

        <!-- Entry bib key -->
        <div id="Shao2024PrivacyLens" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action</div>
          <!-- Author -->
          <div class="author">
          

          Yijia Shao, Tianshi Li, Weiyan Shi, <em style="color: #990000; opacity: 0.9;">Yanchen Liu</em>, and <a href="https://cs.stanford.edu/~diyiy/index.html" target="_blank" rel="noopener noreferrer">Diyi Yang</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 38th Annual Conference on Neural Information Processing Systems</em>
          </div>
          <!-- My custom -->
          <div class="small-break"></div>
          
              <span class="venue">NeurIPS 2024</span>
          
          
          <!-- 
            <span class="tags-container"> 
                
                
                      <span class="tag"># LLM Agent</span>
                
                      <span class="tag"># Safety</span>
                
                      <span class="tag"># Privacy</span>
                
            </span>
           -->
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2409.00138" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents’ actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Shao2024PrivacyLens</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shao, Yijia and Li, Tianshi and Shi, Weiyan and Liu, Yanchen and Yang, Diyi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Under Review}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 38th Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/suscep.png"></div>

        <!-- Entry bib key -->
        <div id="Liu2023Suscep" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach</div>
          <!-- Author -->
          <div class="author">
          

          <em style="color: #990000; opacity: 0.9;">Yanchen Liu</em>, Mingyu Derek Ma, Wenna Qin, Azure Zhou, Jiaao Chen, Weiyan Shi, <a href="https://web.cs.ucla.edu/~weiwang/" target="_blank" rel="noopener noreferrer">Wei Wang</a>, and <a href="https://cs.stanford.edu/~diyiy/index.html" target="_blank" rel="noopener noreferrer">Diyi Yang</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>
          </div>
          <!-- My custom -->
          <div class="small-break"></div>
          
              <span class="venue">EMNLP 2024</span>
          
          
          <!-- 
            <span class="tags-container"> 
                
                
                      <span class="tag"># Social Computing</span>
                
                      <span class="tag"># CSS</span>
                
            </span>
           -->
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2311.09630" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://liuyanchen1015.github.io/blog/2023/suscep/" class="btn btn-sm z-depth-0" role="button">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Susceptibility to misinformation describes the degree of belief in unverifiable claims, a latent aspect of individuals’ mental processes that is not observable. Existing susceptibility studies heavily rely on self-reported beliefs, which can be subject to bias, expensive to collect, and challenging to scale for downstream applications. To address these limitations, in this work, we propose a computational approach to model users’ latent susceptibility levels. As shown in previous research, susceptibility is influenced by various factors (e.g., demographic factors, political ideology), and directly influences people’s reposting behavior on social media. To represent the underlying mental process, our susceptibility modeling incorporates these factors as inputs, guided by the supervision of people’s sharing behavior. Using COVID-19 as a testbed domain, our experiments demonstrate a significant alignment between the susceptibility scores estimated by our computational modeling and human judgments, confirming the effectiveness of this latent modeling approach. Furthermore, we apply our model to annotate susceptibility scores on a large-scale dataset and analyze the relationships between susceptibility with various factors. Our analysis reveals that political leanings and psychological factors exhibit varying degrees of association with susceptibility to COVID-19 misinformation.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Liu2023Suscep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Decoding Susceptibility: Modeling Misbelief to Misinformation Through a Computational Approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yanchen and Ma, Mingyu Derek and Qin, Wenna and Zhou, Azure and Chen, Jiaao and Shi, Weiyan and Wang, Wei and Yang, Diyi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{EMNLP 2024}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/tabular_biases.png"></div>

        <!-- Entry bib key -->
        <div id="Liu2023TabularBiases" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classification</div>
          <!-- Author -->
          <div class="author">
          

          <em style="color: #990000; opacity: 0.9;">Yanchen Liu</em>, Srishti Gautam, <a href="https://jiaqima.github.io/" target="_blank" rel="noopener noreferrer">Jiaqi Ma</a>, and <a href="https://himalakkaraju.github.io/" target="_blank" rel="noopener noreferrer">Himabindu Lakkaraju</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics</em>
          </div>
          <!-- My custom -->
          <div class="small-break"></div>
          
              <span class="venue">NAACL 2024</span>
          
          
          <!-- 
            <span class="tags-container"> 
                
                
                      <span class="tag"># Fairness</span>
                
                      <span class="tag"># Tabular</span>
                
            </span>
           -->
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2310.14607" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent literature has suggested the potential of using large language models (LLMs) to make predictions for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in the society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is imperative to explore the following questions: what sources of information do LLMs draw upon when making predictions for tabular tasks; whether and to what extent are LLM predictions for tabular tasks influenced by social biases and stereotypes; and what are the consequential implications for fairness? Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular prediction tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and fine-tuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pre-training corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Liu2023TabularBiases</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yanchen and Gautam, Srishti and Ma, Jiaqi and Lakkaraju, Himabindu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{EMNLP 2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/DADA.png"></div>

        <!-- Entry bib key -->
        <div id="Liu2023DADA" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules</div>
          <!-- Author -->
          <div class="author">
          

          <em style="color: #990000; opacity: 0.9;">Yanchen Liu</em>, <a href="https://williamheld.com/" target="_blank" rel="noopener noreferrer">William Held</a>, and <a href="https://cs.stanford.edu/~diyiy/index.html" target="_blank" rel="noopener noreferrer">Diyi Yang</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>
          </div>
          <!-- My custom -->
          <div class="small-break"></div>
          
              <span class="venue">EMNLP 2023</span>
          
          
          <!-- 
            <span class="tags-container"> 
                
                
                      <span class="tag"># Dialect-Inclusive</span>
                
                      <span class="tag"># Robustness</span>
                
                      <span class="tag"># Fairness</span>
                
                      <span class="tag"># Linguistics</span>
                
            </span>
           -->
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2305.13406" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
            <a href="https://liuyanchen1015.github.io/DADA/" class="btn btn-sm z-depth-0" role="button">HTML</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting existing LLMs to different English dialects.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Liu2023DADA</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yanchen and Held, William and Yang, Diyi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://liuyanchen1015.github.io/DADA/}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{EMNLP 2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 preview"><img class="preview z-depth-1 rounded" src="/assets/img/publication_preview/SOUP.jpg"></div>

        <!-- Entry bib key -->
        <div id="Liu2022SemanticPriming" class="col-sm-8">
        
          <!-- Title -->
          <div class="title">Semantic-Oriented Unlabeled Priming for Large-Scale Language Models</div>
          <!-- Author -->
          <div class="author">
          

          <em style="color: #990000; opacity: 0.9;">Yanchen Liu</em>, <a href="https://scholar.google.de/citations?user=k8CKy5UAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Timo Schick</a>, and <a href="https://scholar.google.com/citations?user=qIL9dWUAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Hinrich Schütze</a>
</div>

          <!-- Journal/Book title and date -->
          
          <div class="periodical">
            <em>In ACL Workshop on Simple and Efficient Natural Language Processing, 2022</em>
          </div>
          <!-- My custom -->
          <div class="small-break"></div>
          
              <span class="venue">ACLW 2023</span>
          
          
          <!-- 
            <span class="tags-container"> 
                
                
                      <span class="tag"># Low-Resource</span>
                
                      <span class="tag"># Data-Efficiency</span>
                
            </span>
           -->
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2202.06133" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Due to the high costs associated with finetuning large language models, various recent works propose to adapt them to specific tasks without any parameter updates through in-context learning. Unfortunately, for in-context learning there is currently no way to leverage unlabeled data, which is often much easier to obtain in large quantities than labeled examples. In this work, we therefore investigate ways to make use of unlabeled examples to improve the zero-shot performance of pretrained language models without any finetuning: We introduce Semantic-Oriented Unlabeled Priming (SOUP), a method that classifies examples by retrieving semantically similar unlabeled examples, assigning labels to them in a zero-shot fashion, and then using them for in-context learning. We also propose bag-of-contexts priming, a new priming strategy that is more suitable for our setting and enables the usage of more examples than fit into the context window.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Liu2022SemanticPriming</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantic-Oriented Unlabeled Priming for Large-Scale Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Yanchen and Schick, Timo and Schütze, Hinrich}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACL 2023}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACL Workshop on Simple and Efficient Natural Language Processing, 2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


<!-- 
        <br><br>
        <center> <a href="https://www.revolvermaps.com/livestats/5rgtpvc7y8h/"><img src="//rf.revolvermaps.com/h/m/a/0/ff0000/199/35/5rgtpvc7y8h.png" width="350" height="175" alt="Map" style="border:0;"></a> </center>
        <br><br>
-->
      <br><br>
      <center> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=300&amp;t=tt&amp;d=jPJuOPmE06Ht6of7kxbjncpuRsdnbjWuISpPJBuXZ5c"></script> </center>
      <br><br>
          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%79%61%6E%63%68%65%6E%6C%69%75@%66%61%73.%68%61%72%76%61%72%64.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=Ypagfq8AAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.semanticscholar.org/author/2108082280" title="Semantic Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i></a>
            <a href="https://github.com/liuyanchen1015" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/yanchen-liu-35a80617b" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/_yanchenliu" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            

            </div>  
                
            <div class="contact-note">
            Please feel free to email me if you are interested in academic collaborations.

            </div>
                
          </div>              
              
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Yanchen  Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.
Last updated: September 26, 2025.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
  </body>
</html>
