---
layout: about
title: about
permalink: /
subtitle:
profile:
  align: left
  image: me.png
  image_circular: false # crops the image to make it circular
  address: <p align="left"><font size="2">Cambridge, MA, USA 02139<br>Schwarzman College of Computing, MIT</font></p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
--- 

Hi all! I am a 1st-year PhD student at [MIT](https://www.mit.edu/) <img src="assets/img/MIT.png" alt="mit" height="14px">.

Previously, I obtained my Master's degree from [Harvard University](https://www.harvard.edu/) <img src="assets/img/h.png" alt="h" height="20px">, where I worked with Prof. [Hima Lakkaraju](https://himalakkaraju.github.io/) and my Bachelor's degree in Computer Science from 
[TUM](https://www.tum.de/en/) <img src="assets/img/TUM.png" alt="tum" height="13px"> , with a minor in Computational Linguistics at 
[LMU](https://www.lmu.de/en/) <img src="assets/img/LMU.jpeg" alt="lmu" height="18px"> , supervised by Prof. 
[Hinrich Schütze](https://scholar.google.com/citations?user=qIL9dWUAAAAJ&hl=en). Also, I had the opportunity to visit the 
[Stanford NLP Group](https://nlp.stanford.edu/) <img src="assets/img/Stanford.png" alt="s" height="19px"> , collaborating with Prof.
[Diyi Yang](https://cs.stanford.edu/~diyiy/index.html) for one year.

<!-- My research interests mainly lie in `LLM safety`, including robustness <a href="https://arxiv.org/abs/2305.13406">[1]</a><a href="https://arxiv.org/abs/2311.00915">[2]</a>, fairness <a href="https://arxiv.org/abs/2310.14607">[3]</a>, privacy <a href="https://arxiv.org/abs/2409.00138">[4]</a>, misinformation <a href="https://arxiv.org/abs/2311.09630">[5]</a>, alignment, jailbreaking <a href="">[6]</a>, and, moreover, `safety of LM agents` <a href="https://arxiv.org/abs/2409.00138">[4]</a>. Technically, I have worked on `parameter-efficient adaptation/composition`<a href="https://arxiv.org/abs/2305.13406">[1]</a><a href="https://arxiv.org/abs/2311.00915">[2]</a><a href="https://arxiv.org/abs/2302.14413">[7]</a>  and `ICL`<a href="https://arxiv.org/abs/2310.14607">[3]</a><a href="https://arxiv.org/abs/2202.06133">[8]</a>. -->


My current research interests mainly lie in`LLM safety`<a href="https://arxiv.org/abs/2305.13406">[1]</a><a href="https://arxiv.org/abs/2311.00915">[2]</a><a href="https://arxiv.org/abs/2310.14607">[3]</a><a href="https://arxiv.org/abs/2409.00138">[4]</a><a href="https://arxiv.org/abs/2311.09630">[5]</a> (including the `safety of LM agents` <a href="https://arxiv.org/abs/2409.00138">[4]</a>), `alignment`, and `scalable oversight`, as well as other issues related to `Human–AI Symbioses` that we may face in the future.
